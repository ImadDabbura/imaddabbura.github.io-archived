<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.4.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Imad Dabbura">

  
  
  
    
  
  <meta name="description" content="Generalization (test) error is the most important metric in Machine/Deep Learning. It gives us an estimate on the performance of the model on unseen data. Test error is decomposed into 3 parts (see above figure): Variance, Squared-Bias, and Irreducible Error. Models with high bias are not complex enough (too simple) for the data and tend to underfit. The simplest model is taking the average (mode) of target variable and assign it to all predictions.">

  
  <link rel="alternate" hreflang="en-us" href="https://imaddabbura.github.io/post/coding-nn-regularization/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.2a1836334f0eacc81815993d549766bf.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-127825273-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="https://www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://imaddabbura.github.io/post/coding-nn-regularization/">

  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@ImadPhd">
  <meta property="twitter:creator" content="@ImadPhd">
  
  <meta property="og:site_name" content="Imad Dabbura">
  <meta property="og:url" content="https://imaddabbura.github.io/post/coding-nn-regularization/">
  <meta property="og:title" content="Coding Neural Network - Regularization | Imad Dabbura">
  <meta property="og:description" content="Generalization (test) error is the most important metric in Machine/Deep Learning. It gives us an estimate on the performance of the model on unseen data. Test error is decomposed into 3 parts (see above figure): Variance, Squared-Bias, and Irreducible Error. Models with high bias are not complex enough (too simple) for the data and tend to underfit. The simplest model is taking the average (mode) of target variable and assign it to all predictions."><meta property="og:image" content="https://imaddabbura.github.io/post/coding-nn-regularization/featured.png">
  <meta property="twitter:image" content="https://imaddabbura.github.io/post/coding-nn-regularization/featured.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2018-05-08T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2018-05-08T00:00:00&#43;00:00">
  

  

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.js"></script>
<script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#2962ff",
          "text": "#fff"
        },
        "button": {
          "background": "#fff",
          "text": "#2962ff"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "https://cookies.insites.com"
      }
    })});
</script>



  





  <title>Coding Neural Network - Regularization | Imad Dabbura</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Imad Dabbura</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/til/"><span>Today I Learned</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/resume.pdf"><span>Resume</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  




















  
  


<div class="article-container pt-3">
  <h1 itemprop="name">Coding Neural Network - Regularization</h1>

  

  



<meta content="2018-05-08 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2018-05-08 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    <time>May 8, 2018</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    13 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="/post/coding-nn-regularization/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/machine-learning/">Machine Learning</a>, <a href="/categories/deep-learning/">Deep Learning</a></span>
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://imaddabbura.github.io/post/coding-nn-regularization/&amp;text=Coding%20Neural%20Network%20-%20Regularization" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://imaddabbura.github.io/post/coding-nn-regularization/&amp;t=Coding%20Neural%20Network%20-%20Regularization" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Coding%20Neural%20Network%20-%20Regularization&amp;body=https://imaddabbura.github.io/post/coding-nn-regularization/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://imaddabbura.github.io/post/coding-nn-regularization/&amp;title=Coding%20Neural%20Network%20-%20Regularization" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Coding%20Neural%20Network%20-%20Regularization%20https://imaddabbura.github.io/post/coding-nn-regularization/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://imaddabbura.github.io/post/coding-nn-regularization/&amp;title=Coding%20Neural%20Network%20-%20Regularization" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

  














</div>


<div class="article-header container-fluid featured-image-wrapper mt-4 mb-4" style="max-width: 492px; max-height: 309px;">
  <div style="position: relative">
    <img src="/post/coding-nn-regularization/featured.png" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <!-- raw HTML omitted -->
<p>Generalization (test) error is the most important metric in Machine/Deep Learning. It gives us an estimate on the performance of the model on unseen data. Test error is decomposed into 3 parts (see above figure): <strong>Variance, Squared-Bias, and Irreducible Error</strong>. Models with high bias are not complex enough (too simple) for the data and tend to underfit. The simplest model is taking the average (mode) of target variable and assign it to all predictions. On the contrary, models with high variance overfit the training data by closely follow (mimick) the training data where the learning algorithm will follow the signal and the noise. Note that as the complexity (flexibility) of the model increases → the model will become less interpretable such as Neural Networks. Below is the bias-variance decomposition:</p>
<p>$$MSE = E(y - \widehat{y})^2$$
$$ = E(y - f + f - \widehat{y})^2$$
$$ = E((y - f)^2 + 2(y - f)(f - \widehat{y}) + (f - \widehat{y})^2); \quad substitute\ y = f + \epsilon$$
$$ = E((\epsilon + f - f)^2 + 2(\epsilon + f - f)(f - \widehat{y}) + (f - \widehat{y})^2)$$
$$ = E(\epsilon)^2 + E(\epsilon)E(f - \widehat{y}) + E(f - \widehat{y})^2; \quad where\ E(\epsilon) = 0$$
$$ = E(\epsilon)^2 + E(f - \widehat{y})^2;\quad add\ and\ subtract\ E(\widehat{y})$$
$$ = E(\epsilon)^2 + E(f - E(\widehat{y}) + E(\widehat{y}) - \widehat{y})^2$$
$$ = E(\epsilon)^2 + E(f - E(\widehat{y}))^2 + E(\widehat{y} - E(\widehat{y}))^2$$
$$\Rightarrow MSE = var(\widehat{y}) + (Bias(\widehat{y}))^2 + var(\epsilon)$$
Where:</p>
<ul>
<li>$var(\epsilon)$: Irreducible error that resulted from omitted features and unmeasured variation with each example.</li>
<li>$Bias(\widehat{y})$: Error that is introduced by approximating a real-life problem with a simple model.</li>
<li>$var(\widehat{y})$: amount by which $\widehat{y}$ would change if we estimated it using different data set.</li>
</ul>
<p>Therefore, we can control only the variance and the bias of the $\widehat{y}$ <strong>BUT NOT</strong> irreducible error. As a result, our job is to try to estimate the right level of complexity to achieve the lowest test error.</p>
<!-- raw HTML omitted -->
<p>Regularization adds stability to the learning algorithm by making it less sensitive to the training data and processes. Since we don&rsquo;t know and have no access to the true function that we can use to compare our estimated function with it, the best strategy would be to build a very complex model that fits the training data really well (overfitting) and regularize it so that it would have a good generalization (test) error. When using regularization, we try to reduce the generalization error and that may lead to increase the training error in the process which is okay because what we care about is how well the model generalizes. With regularization, we try to bring back the very complex model that suffers from overfitting to a good model by increasing bias and reducing variance. This builds on the assumption that complex model has large parameters and simple model has small parameters.</p>
<p>Below are some methods used for regularization:</p>
<ul>
<li><strong>L2 Parameter Regularization</strong> It&rsquo;s also known as <strong>weight decay</strong>. This method adds L2 norm penalty to the objective function to drive the weights towards the origin. Even though this method shrinks all weights by the same proportion towards zero; however, it will never make any weight to be exactly zero.</li>
<li><strong>L1 Parameter Regularization (Lasso)</strong> It can be seen as a feature selection method because; in contrast to L2 regularization, some weights will be actually zero. It shrinks all weights by the same amount by adding L1 norm penalty to the objective function.</li>
<li><strong>Dropout</strong> Dropout can be seen as an approximation to bagging techniques. On each iteration, we randomly shut down some neurons on each layer and don&rsquo;t use those neurons in both forward propagation and back-propagation. This will force the neural network to spread out weights and not focus on specific neurons because it will never know which neurons will show up on each iteration. Therefore, it can be seen as training different model on each iteration. Also, since we drop some neurons on each iteration, this will lead to smaller network which in turns means simpler network.</li>
<li><strong>Augmentation</strong> Add fake data by using the training examples and adding distortions to them such as rescaling and rotating the images in the case of image recognition. The idea here is that it&rsquo;s always better to train the model on more data to achieve better performance. Note that augmented examples don&rsquo;t add much information to the model as much as independent examples do but still it&rsquo;s a valid alternative when collecting more data is not feasible.</li>
<li><strong>Early Stopping</strong> This method tries to optimize the cost function and regularize it so that it would have lower generalization error. The way it works is that on each iteration we record the validation error. If the validation error improves, we store a copy of the parameters and will continue until the optimization algorithm terminates. It&rsquo;s a good method if computational time and resources is an issue for us.</li>
</ul>
<p>In this post, we&rsquo;ll cover L2 parameter regularization.</p>
<!-- raw HTML omitted -->
<p>We normally don&rsquo;t regularize bias and regularize weights only. We can use hessian matrix and it&rsquo;s eigenvalues and eigenvectors to see the sensitivity of the weights to the weight decay. The weight $w_i$ will be rescaled using $\frac{\lambda_i}{\lambda_i + \alpha}$ where $\lambda_i$ (eigenvalue) measures the sensitivity of hessian matrix in that direction (eigenvector) and $\alpha$ is the regularized hyperparameter. Therefore,</p>
<ul>
<li>If $\lambda_i &raquo; \alpha$, the cost function is very sensitive in that direction and the corresponding weight reduces the cost significantly $\Rightarrow$ don&rsquo;t decay (shrink) much.</li>
<li>If $\lambda_i &laquo; \alpha$, the cost function is not sensitive in that direction and the corresponding weight doesn&rsquo;t reduce the cost significantly $\Rightarrow$ decay (shrink) away towards zero.</li>
</ul>
<p>The objective function (binary cross-entropy) would then change from:</p>
<p>$$J = -\frac{1}{m} \sum\limits_{i = 1}^{m}  \large(y^{(i)}\log(a^{<a href="i">L</a>}) + (1-y^{(i)})\log(1- a^{<a href="i">L</a>}) \large)\tag{1}$$</p>
<p>To:</p>
<p>$$J_{regularized} = \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m}  \large(y^{(i)}\log(a^{<a href="i">L</a>}) + (1-y^{(i)})\log(1- a^{<a href="i">L</a>}) \large)}_\text{cross-entropy cost}  + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_{l=1}^L \sum\limits_{i=1}^{n^l} \sum\limits_{j=1}^{n^{l-1}} W_{j,i}^{[l]2} }_\text{L2 regularization cost} \tag{2}$$</p>
<p>Also, the new gradients and the update equation would be:</p>
<p>$$\nabla_w J_{regularized} = \nabla_w J + \frac{\lambda}{m}w$$
$$w = w - \alpha\nabla_w J - \alpha\frac{\lambda}{m}w$$
$$\Rightarrow w = w\underbrace{(1 - \alpha\frac{\lambda}{m})}_\text{weight decay} - \nabla J$$</p>
<p>Note that here $\alpha$ is the learning rate and $\lambda$ is the regularized hyperparameter. As $\lambda$ increases, the bias increases (and the model becomes less flexible) with the following extreme cases (see figure 2):</p>
<ul>
<li>$\lambda = 0$, no regularization.</li>
<li>$\lambda \rightarrow \infty$, model becomes very simple where all weights are essentially zero. In the case of regression, we would end-up with the intercept only which is equal to the average of the target variable.</li>
</ul>



  




<figure>

  <a data-fancybox="" href="/img/coding-nn-from-scratch/bias_variance_lambda.png" >

<img src="/img/coding-nn-from-scratch/bias_variance_lambda.png" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Model complexity (underfitting/overfitting) as a function of regularization parameter $\lambda$.</h4>
  
</figcaption>

</figure>

<p>It sometimes maybe helpful to see how L2 parameter regularization works using normal equation. The normal quation is:
$$W = (X^TX + \lambda I)^{-1}X^TY\tag{3}$$
This means that:</p>
<ul>
<li>Adding $\lambda$ to the variance would decrease the weight since $w_i = \frac{cov_{x, y}}{\sigma^2_x}$.</li>
<li>Even if $X^TX$ is not invertible, adding $\lambda$ to each feature will make it full rank matrix $\Rightarrow$ invertible.</li>
</ul>
<p>To illustrate how regularization helps us reduce generalization error, we&rsquo;ll use the cats_vs_dogs dataset. The dataset has images for cats and dogs. We&rsquo;ll try to build a neural network to classify if the image has a cat or a dog. Each image is 64 x 64 pixels on RGB scale.</p>
<p>We&rsquo;ll be using functions we wrote in <a href="https://nbviewer.jupyter.org/github/ImadDabbura/blog-posts/blob/master/notebooks/Coding-Neural-Network-Forwad-Back-Propagation.ipynb"><em>&ldquo;Coding Neural Network - Forward Propagation and Backpropagation&rdquo;</em></a> post to initialize parameters, compute forward propagation, cross-entropy cost, gradients, etc.</p>
<p>Let&rsquo;s import the data and take a look at the shape as well as a sample of a cat image from the training set.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Loading packages</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> sys
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> h5py
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#34;../scripts/&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> coding_neural_network_from_scratch <span style="color:#f92672">import</span> (initialize_parameters,
</span></span><span style="display:flex;"><span>                                                L_model_forward,
</span></span><span style="display:flex;"><span>                                                compute_cost,
</span></span><span style="display:flex;"><span>                                                relu_gradient,
</span></span><span style="display:flex;"><span>                                                sigmoid_gradient,
</span></span><span style="display:flex;"><span>                                                tanh_gradient,
</span></span><span style="display:flex;"><span>                                                update_parameters,
</span></span><span style="display:flex;"><span>                                                accuracy)
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> gradient_checking <span style="color:#f92672">import</span> dictionary_to_vector
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> load_dataset <span style="color:#f92672">import</span> load_dataset_catvsdog
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">%</span>matplotlib inline
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>set_context(<span style="color:#e6db74">&#34;notebook&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#34;fivethirtyeight&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;figure.figsize&#39;</span>] <span style="color:#f92672">=</span> (<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">6</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Import training data</span>
</span></span><span style="display:flex;"><span>train_dataset <span style="color:#f92672">=</span> h5py<span style="color:#f92672">.</span>File(<span style="color:#e6db74">&#34;../data/train_catvnoncat.h5&#34;</span>)
</span></span><span style="display:flex;"><span>X_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(train_dataset[<span style="color:#e6db74">&#34;train_set_x&#34;</span>])
</span></span><span style="display:flex;"><span>Y_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(train_dataset[<span style="color:#e6db74">&#34;train_set_y&#34;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot a sample image</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>imshow(X_train[<span style="color:#ae81ff">50</span>])
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#34;off&#34;</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Import test data</span>
</span></span><span style="display:flex;"><span>test_dataset <span style="color:#f92672">=</span> h5py<span style="color:#f92672">.</span>File(<span style="color:#e6db74">&#34;../data/test_catvnoncat.h5&#34;</span>)
</span></span><span style="display:flex;"><span>X_test <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(test_dataset[<span style="color:#e6db74">&#34;test_set_x&#34;</span>])
</span></span><span style="display:flex;"><span>Y_test <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(test_dataset[<span style="color:#e6db74">&#34;test_set_y&#34;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Transform data</span>
</span></span><span style="display:flex;"><span>X_train <span style="color:#f92672">=</span> X_train<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">209</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>T
</span></span><span style="display:flex;"><span>X_train <span style="color:#f92672">=</span> X_train <span style="color:#f92672">/</span> <span style="color:#ae81ff">255</span>
</span></span><span style="display:flex;"><span>Y_train <span style="color:#f92672">=</span> Y_train<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">209</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X_test <span style="color:#f92672">=</span> X_test<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">50</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>T
</span></span><span style="display:flex;"><span>X_test <span style="color:#f92672">=</span> X_test <span style="color:#f92672">/</span> <span style="color:#ae81ff">255</span>
</span></span><span style="display:flex;"><span>Y_test <span style="color:#f92672">=</span> Y_test<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># print the new shape of both training and test datasets</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Training data dimensions:&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;X&#39;s dimension: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">, Y&#39;s dimension: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(X_train<span style="color:#f92672">.</span>shape, Y_train<span style="color:#f92672">.</span>shape))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Test data dimensions:&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;X&#39;s dimension: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">, Y&#39;s dimension: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(X_test<span style="color:#f92672">.</span>shape, Y_test<span style="color:#f92672">.</span>shape))
</span></span></code></pre></div><pre><code>Training data dimensions:
X's dimension: (12288, 209), Y's dimension: (1, 209)
Test data dimensions:
X's dimension: (12288, 50), Y's dimension: (1, 50)
</code></pre>



  




<figure>

  <a data-fancybox="" href="/img/coding-nn-from-scratch/cat_sample.png" >

<img src="/img/coding-nn-from-scratch/cat_sample.png" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Sample image.</h4>
  
</figcaption>

</figure>

<p>The training set has 209 examples and the test set has 50 examples. Let&rsquo;s first write all the helper functions that would help us write the multi-layer neural network.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_cost_reg</span>(AL, y, parameters, lambd<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># number of examples</span>
</span></span><span style="display:flex;"><span>    m <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># compute traditional cross entropy cost</span>
</span></span><span style="display:flex;"><span>    cross_entropy_cost <span style="color:#f92672">=</span> compute_cost(AL, y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># convert parameters dictionary to vector</span>
</span></span><span style="display:flex;"><span>    parameters_vector <span style="color:#f92672">=</span> dictionary_to_vector(parameters)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># compute the regularization penalty</span>
</span></span><span style="display:flex;"><span>    L2_regularization_penalty <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>        lambd <span style="color:#f92672">/</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> m)) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>sum(np<span style="color:#f92672">.</span>square(parameters_vector))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># compute the total cost</span>
</span></span><span style="display:flex;"><span>    cost <span style="color:#f92672">=</span> cross_entropy_cost <span style="color:#f92672">+</span> L2_regularization_penalty
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> cost
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">linear_backword_reg</span>(dZ, cache, lambd<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
</span></span><span style="display:flex;"><span>    A_prev, W, b <span style="color:#f92672">=</span> cache
</span></span><span style="display:flex;"><span>    m <span style="color:#f92672">=</span> A_prev<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    dW <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> m) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>dot(dZ, A_prev<span style="color:#f92672">.</span>T) <span style="color:#f92672">+</span> (lambd <span style="color:#f92672">/</span> m) <span style="color:#f92672">*</span> W
</span></span><span style="display:flex;"><span>    db <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> m) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>sum(dZ, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    dA_prev <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(W<span style="color:#f92672">.</span>T, dZ)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> (dA_prev<span style="color:#f92672">.</span>shape <span style="color:#f92672">==</span> A_prev<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> (dW<span style="color:#f92672">.</span>shape <span style="color:#f92672">==</span> W<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> (db<span style="color:#f92672">.</span>shape <span style="color:#f92672">==</span> b<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> dA_prev, dW, db
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">linear_activation_backward_reg</span>(dA, cache, activation_fn<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;relu&#34;</span>, lambd<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
</span></span><span style="display:flex;"><span>    linear_cache, activation_cache <span style="color:#f92672">=</span> cache
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> activation_fn <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;sigmoid&#34;</span>:
</span></span><span style="display:flex;"><span>        dZ <span style="color:#f92672">=</span> sigmoid_gradient(dA, activation_cache)
</span></span><span style="display:flex;"><span>        dA_prev, dW, db <span style="color:#f92672">=</span> linear_backword_reg(dZ, linear_cache, lambd)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> activation_fn <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;tanh&#34;</span>:
</span></span><span style="display:flex;"><span>        dZ <span style="color:#f92672">=</span> tanh_gradient(dA, activation_cache)
</span></span><span style="display:flex;"><span>        dA_prev, dW, db <span style="color:#f92672">=</span> linear_backword_reg(dZ, linear_cache, lambd)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> activation_fn <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;relu&#34;</span>:
</span></span><span style="display:flex;"><span>        dZ <span style="color:#f92672">=</span> relu_gradient(dA, activation_cache)
</span></span><span style="display:flex;"><span>        dA_prev, dW, db <span style="color:#f92672">=</span> linear_backword_reg(dZ, linear_cache, lambd)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> dA_prev, dW, db
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">L_model_backward_reg</span>(AL, y, caches, hidden_layers_activation_fn<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;relu&#34;</span>,
</span></span><span style="display:flex;"><span>                         lambd<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>reshape(AL<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>    L <span style="color:#f92672">=</span> len(caches)
</span></span><span style="display:flex;"><span>    grads <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    dAL <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>divide(AL <span style="color:#f92672">-</span> y, np<span style="color:#f92672">.</span>multiply(AL, <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> AL))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    grads[<span style="color:#e6db74">&#34;dA&#34;</span> <span style="color:#f92672">+</span> str(L <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)], grads[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(L)], grads[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(L)] <span style="color:#f92672">=</span>\
</span></span><span style="display:flex;"><span>        linear_activation_backward_reg(dAL, caches[L <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>], <span style="color:#e6db74">&#34;sigmoid&#34;</span>, lambd)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> l <span style="color:#f92672">in</span> range(L <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        current_cache <span style="color:#f92672">=</span> caches[l <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        grads[<span style="color:#e6db74">&#34;dA&#34;</span> <span style="color:#f92672">+</span> str(l <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)], grads[<span style="color:#e6db74">&#34;dW&#34;</span> <span style="color:#f92672">+</span> str(l)], grads[<span style="color:#e6db74">&#34;db&#34;</span> <span style="color:#f92672">+</span> str(l)] <span style="color:#f92672">=</span>\
</span></span><span style="display:flex;"><span>            linear_activation_backward_reg(
</span></span><span style="display:flex;"><span>                grads[<span style="color:#e6db74">&#34;dA&#34;</span> <span style="color:#f92672">+</span> str(l)], current_cache,
</span></span><span style="display:flex;"><span>                hidden_layers_activation_fn, lambd)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> grads
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">model_with_regularization</span>(
</span></span><span style="display:flex;"><span>        X, y, layers_dims, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>,  num_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">3000</span>,
</span></span><span style="display:flex;"><span>        print_cost<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, hidden_layers_activation_fn<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;relu&#34;</span>, lambd<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># get number of examples</span>
</span></span><span style="display:flex;"><span>    m <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># to get consistents output</span>
</span></span><span style="display:flex;"><span>    np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># initialize parameters</span>
</span></span><span style="display:flex;"><span>    parameters <span style="color:#f92672">=</span> initialize_parameters(layers_dims)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># intialize cost list</span>
</span></span><span style="display:flex;"><span>    cost_list <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># implement gradient descent</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_epochs):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># compute forward propagation</span>
</span></span><span style="display:flex;"><span>        AL, caches <span style="color:#f92672">=</span> L_model_forward(
</span></span><span style="display:flex;"><span>            X, parameters, hidden_layers_activation_fn)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># compute regularized cost</span>
</span></span><span style="display:flex;"><span>        reg_cost <span style="color:#f92672">=</span> compute_cost_reg(AL, y, parameters, lambd)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># compute gradients</span>
</span></span><span style="display:flex;"><span>        grads <span style="color:#f92672">=</span> L_model_backward_reg(
</span></span><span style="display:flex;"><span>            AL, y, caches, hidden_layers_activation_fn, lambd)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># update parameters</span>
</span></span><span style="display:flex;"><span>        parameters <span style="color:#f92672">=</span> update_parameters(parameters, grads, learning_rate)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># print cost</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">and</span> print_cost:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">&#34;The cost after </span><span style="color:#e6db74">{}</span><span style="color:#e6db74"> iterations: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(
</span></span><span style="display:flex;"><span>                (i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>), reg_cost))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># append cost</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> i <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            cost_list<span style="color:#f92672">.</span>append(reg_cost)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># plot the cost curve</span>
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(cost_list)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;Iterations (per hundreds)&#34;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Cost&#34;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Cost curve for the learning rate = </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(learning_rate))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> parameters
</span></span></code></pre></div><p>Now we&rsquo;re ready to train the neural network. We&rsquo;ll first build a neural network with no regularization and then one with regularization to see which one has lower generalization error. Note that $\lambda$ should be tuned to get the best results but we&rsquo;ll here choose an arbitrary value to illustrate the concept. Both neural netwotks would have 2 hidden layers where each hidden layer has 5 units.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># set up layers dimensions</span>
</span></span><span style="display:flex;"><span>layers_dims <span style="color:#f92672">=</span> [X_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># train NN</span>
</span></span><span style="display:flex;"><span>parameters <span style="color:#f92672">=</span> model_with_regularization(X_train, Y_train, layers_dims,
</span></span><span style="display:flex;"><span>                                       learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.03</span>, num_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">2500</span>, print_cost<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>                                       hidden_layers_activation_fn<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tanh&#34;</span>,
</span></span><span style="display:flex;"><span>                                       lambd<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># print the test accuracy</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;The training accuracy rate: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(accuracy(X_train, parameters, Y_train, <span style="color:#e6db74">&#34;tanh&#34;</span>)[<span style="color:#f92672">-</span><span style="color:#ae81ff">7</span>:]))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;The test accuracy rate: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(accuracy(X_test, parameters, Y_test, <span style="color:#e6db74">&#34;tanh&#34;</span>)[<span style="color:#f92672">-</span><span style="color:#ae81ff">7</span>:]))
</span></span></code></pre></div><pre><code>The cost after 100 iterations: 0.6555634398145331
The cost after 200 iterations: 0.6467746423961933
The cost after 300 iterations: 0.6446638811282552
The cost after 400 iterations: 0.6441400737542232
The cost after 500 iterations: 0.6440063101787575
The cost after 600 iterations: 0.6439697872317176
The cost after 700 iterations: 0.6439570623358253
The cost after 800 iterations: 0.6439491872993496
The cost after 900 iterations: 0.6439407592837082
The cost after 1000 iterations: 0.6439294591543208
The cost after 1100 iterations: 0.6439131091764411
The cost after 1200 iterations: 0.6438883396380859
The cost after 1300 iterations: 0.6438489715870495
The cost after 1400 iterations: 0.6437825798034876
The cost after 1500 iterations: 0.6436617691190204
The cost after 1600 iterations: 0.6434191397054715
The cost after 1700 iterations: 0.642864008138056
The cost after 1800 iterations: 0.6413476000796884
The cost after 1900 iterations: 0.6360827945885947
The cost after 2000 iterations: 0.6124050450908987
The cost after 2100 iterations: 0.511236045905345
The cost after 2200 iterations: 0.5287658028657057
The cost after 2300 iterations: 0.43124104856359174
The cost after 2400 iterations: 0.38213869447364884
The cost after 2500 iterations: 0.3386708692392079
The training accuracy rate: 82.30%.
The test accuracy rate: 78.00%.
</code></pre>



  




<figure>

  <a data-fancybox="" href="/img/coding-nn-from-scratch/loss_no_reg.png" >

<img src="/img/coding-nn-from-scratch/loss_no_reg.png" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Cost curve with no regularization.</h4>
  
</figcaption>

</figure>

<p>The training accuracy is 82.30% but the test accuracy is 78%. The difference between training and test accuracy is not that much, i.e. we don&rsquo;t have a lot of overfitting. Therefore, a little bit of regularization may help such as $\lambda = 0.02$. Values of $\lambda$s that practitioners recommend are: 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># train NN with regularization</span>
</span></span><span style="display:flex;"><span>parameters <span style="color:#f92672">=</span> model_with_regularization(X_train, Y_train, layers_dims,
</span></span><span style="display:flex;"><span>                                       learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.03</span>, num_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">2500</span>, print_cost<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>                                       hidden_layers_activation_fn<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tanh&#34;</span>,
</span></span><span style="display:flex;"><span>                                       lambd<span style="color:#f92672">=</span><span style="color:#ae81ff">0.02</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># print the test accuracy</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;The training accuracy rate: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(accuracy(X_train, parameters, Y_train, <span style="color:#e6db74">&#34;tanh&#34;</span>)[<span style="color:#f92672">-</span><span style="color:#ae81ff">7</span>:]))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;The test accuracy rate: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(accuracy(X_test, parameters, Y_test, <span style="color:#e6db74">&#34;tanh&#34;</span>)[<span style="color:#f92672">-</span><span style="color:#ae81ff">7</span>:]))
</span></span></code></pre></div><pre><code>The cost after 100 iterations: 0.6558634554205135
The cost after 200 iterations: 0.6470807090618383
The cost after 300 iterations: 0.6449737235917311
The cost after 400 iterations: 0.6444519406797673
The cost after 500 iterations: 0.6443191828114609
The cost after 600 iterations: 0.6442831256251426
The cost after 700 iterations: 0.6442705985766486
The cost after 800 iterations: 0.6442628048800636
The cost after 900 iterations: 0.6442544325786784
The cost after 1000 iterations: 0.6442432311807257
The cost after 1100 iterations: 0.6442270988055475
The cost after 1200 iterations: 0.6442027847231018
The cost after 1300 iterations: 0.6441643410411311
The cost after 1400 iterations: 0.6440998547029029
The cost after 1500 iterations: 0.6439832000181198
The cost after 1600 iterations: 0.6437505375793907
The cost after 1700 iterations: 0.6432228625403317
The cost after 1800 iterations: 0.6417982979158361
The cost after 1900 iterations: 0.6369273437378263
The cost after 2000 iterations: 0.6152774362019153
The cost after 2100 iterations: 0.5207828651496548
The cost after 2200 iterations: 0.5145012356446598
The cost after 2300 iterations: 0.40757220705507585
The cost after 2400 iterations: 0.517757346098386
The cost after 2500 iterations: 0.4574831239241244
The training accuracy rate: 65.55%.
The test accuracy rate: 80.00%.
</code></pre>



  




<figure>

  <a data-fancybox="" href="/img/coding-nn-from-scratch/loss_reg.png" >

<img src="/img/coding-nn-from-scratch/loss_reg.png" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Cost curve with regularization.</h4>
  
</figcaption>

</figure>

<p>As the results above show, we improved the generalization error by increasing the test accuracy from 78% to 80%. On the other hand, training accuracy decreased from 82.30% to 65.55%.</p>
<!-- raw HTML omitted -->
<p>Regularization is an effective technique to resolve overfitting. Since we don&rsquo;t know true distribution of the data, empirical risk, which is based of empirical distribution, is prone to overfitting. Therefore, the best strategy is to fit training data really well and then use a regularization technique so that the model generalizes well. L2 parameter regularization along with Dropout are two of the most widely used regularization technique in machine learning.</p>
<ul>
<li>One of the implicit assumptions of regularization techniques such as L2 and L1 parameter regularization is that the value of the parameters should be zero and try to shrink all parameters towards zero. It&rsquo;s meant to avoid following the training data very well which makes the learning algorithm picks some noise that is not helpful when applied on unseen data.</li>
<li>The value of $\lambda$ should be tuned to get the best generalization error. We typically use validation set when comparing models with values for $\lambda$s and pick the one with the lowest validation error.</li>
<li>Only use regularization if the model suffers from overfitting, i.e training error &laquo; validation error.</li>
<li>If after using regularization the validation error is still high, then we&rsquo;re most likely in the underfitting region. In other words, our model is still too simple and already has high bias. Therefore, add complexity to the model and then use regularization.</li>
<li>Since the majority of tasks we try to solve don&rsquo;t have enough data (or expensive to collect more data), overfitting will be more prevalent in Deep Learning than underfitting given the complexity of neural networks.</li>
</ul>
<p>The source code that created this post can be found <a href="https://github.com/ImadDabbura/blog-posts/blob/master/notebooks/Coding-Neural-Network-Regularization.ipynb">here</a>.
The post is inspired by deeplearning.ai courses.</p>

    </div>

    


    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/deep-learning/">Deep Learning</a>
  
  <a class="badge badge-light" href="/tags/ai/">AI</a>
  
</div>



    
      








  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hu057a6ce318cedc9fff5e5d4bcb0cb87d_16356_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="https://imaddabbura.github.io">Imad Dabbura</a></h5>
      <h6 class="card-subtitle">Senior Data Scientist</h6>
      <p class="card-text" itemprop="description">My interests include data science, machine learning and artificial intelligence.</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/#contact" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://twitter.com/imadphd" target="_blank" rel="noopener">
              <i class="fab fa-twitter"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="//www.linkedin.com/in/imaddabbura" >
              <i class="fab fa-linkedin"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/imaddabbura" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="//medium.com/@ImadPhd" >
              <i class="fab fa-medium"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/post/coding-nn-params-init/">Coding Neural Network - Parameters&#39; Initialization</a></li>
          
          <li><a href="/post/coding-nn-gradient-checking/">Coding Neural Network - Gradient Checking</a></li>
          
          <li><a href="/post/coding-nn-fwd-bckwd-prop/">Coding Neural Network - Forward Propagation and Backpropagtion</a></li>
          
          <li><a href="/post/character-level-language-model/">Character-level Language Model</a></li>
          
          <li><a href="/post/gradient-descent-algorithm/">Gradient Descent Algorithm and Its Variants</a></li>
          
        </ul>
      </div>
      
    

    
    <div class="article-widget">
      
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/post/coding-nn-dropout/" rel="next">Coding Neural Network - Dropout</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/coding-nn-params-init/" rel="prev">Coding Neural Network - Parameters&#39; Initialization</a>
  </div>
  
</div>

    </div>
    

    
<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "ImadDabbura" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>



  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js" integrity="sha256-0w92bcB21IY5+rGI84MGj52jNfHNbXVeQLrZ0CGdjNY=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/python.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    <script id="dsq-count-scr" src="https://ImadDabbura.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.842f599ee533ad7f6dbd4e00e95d3d79.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    © Imad Dabbura 2018 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
