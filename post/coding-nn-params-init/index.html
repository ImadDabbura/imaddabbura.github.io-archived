<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.4.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Imad Dabbura">

  
  
  
    
  
  <meta name="description" content="Optimization, in Machine Learning/Deep Learning contexts, is the process of changing the model&rsquo;s parameters to improve its performance. In other words, it&rsquo;s the process of finding the best parameters in the predefined hypothesis space to get the best possible performance. There are three kinds of optimization algorithms:
Optimization algorithm that is not iterative and simply solves for one point. Optimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression.">

  
  <link rel="alternate" hreflang="en-us" href="https://imaddabbura.github.io/post/coding-nn-params-init/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.2a1836334f0eacc81815993d549766bf.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-127825273-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="https://www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://imaddabbura.github.io/post/coding-nn-params-init/">

  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@ImadPhd">
  <meta property="twitter:creator" content="@ImadPhd">
  
  <meta property="og:site_name" content="Imad Dabbura">
  <meta property="og:url" content="https://imaddabbura.github.io/post/coding-nn-params-init/">
  <meta property="og:title" content="Coding Neural Network - Parameters&#39; Initialization | Imad Dabbura">
  <meta property="og:description" content="Optimization, in Machine Learning/Deep Learning contexts, is the process of changing the model&rsquo;s parameters to improve its performance. In other words, it&rsquo;s the process of finding the best parameters in the predefined hypothesis space to get the best possible performance. There are three kinds of optimization algorithms:
Optimization algorithm that is not iterative and simply solves for one point. Optimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression."><meta property="og:image" content="https://imaddabbura.github.io/post/coding-nn-params-init/featured.png">
  <meta property="twitter:image" content="https://imaddabbura.github.io/post/coding-nn-params-init/featured.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2018-04-20T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2018-04-20T00:00:00&#43;00:00">
  

  

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.js"></script>
<script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#2962ff",
          "text": "#fff"
        },
        "button": {
          "background": "#fff",
          "text": "#2962ff"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "https://cookies.insites.com"
      }
    })});
</script>



  





  <title>Coding Neural Network - Parameters&#39; Initialization | Imad Dabbura</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Imad Dabbura</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/til/"><span>Today I Learned</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/resume.pdf"><span>Resume</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1 itemprop="name">Coding Neural Network - Parameters&#39; Initialization</h1>

  

  



<meta content="2018-04-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2018-04-20 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    <time>Apr 20, 2018</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    8 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="/post/coding-nn-params-init/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/machine-learning/">Machine Learning</a>, <a href="/categories/deep-learning/">Deep Learning</a></span>
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://imaddabbura.github.io/post/coding-nn-params-init/&amp;text=Coding%20Neural%20Network%20-%20Parameters&amp;#39;%20Initialization" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://imaddabbura.github.io/post/coding-nn-params-init/&amp;t=Coding%20Neural%20Network%20-%20Parameters&amp;#39;%20Initialization" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Coding%20Neural%20Network%20-%20Parameters&amp;#39;%20Initialization&amp;body=https://imaddabbura.github.io/post/coding-nn-params-init/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://imaddabbura.github.io/post/coding-nn-params-init/&amp;title=Coding%20Neural%20Network%20-%20Parameters&amp;#39;%20Initialization" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Coding%20Neural%20Network%20-%20Parameters&amp;#39;%20Initialization%20https://imaddabbura.github.io/post/coding-nn-params-init/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://imaddabbura.github.io/post/coding-nn-params-init/&amp;title=Coding%20Neural%20Network%20-%20Parameters&amp;#39;%20Initialization" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 502px;">
  <div style="position: relative">
    <img src="/post/coding-nn-params-init/featured_huace3f62751a6af1d5e90df648d2f927b_676319_720x0_resize_lanczos_3.png" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <p>Optimization, in Machine Learning/Deep Learning contexts, is the process of changing the model&rsquo;s parameters to improve its performance. In other words, it&rsquo;s the process of finding the best parameters in the predefined hypothesis space to get the best possible performance. There are three kinds of optimization algorithms:</p>
<ul>
<li>Optimization algorithm that is not iterative and simply solves for one point.</li>
<li>Optimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression.</li>
<li>Optimization algorithm that is iterative in nature and applied to a set of problems that have non-convex loss functions such as neural networks. Therefore, parameters&rsquo; initialization plays a critical role in speeding up convergence and achieving lower error rates.</li>
</ul>
<p>In this post, we&rsquo;ll look at three different cases of parameters&rsquo; initialization and see how this affects the error rate:</p>
<ul>
<li>Initialize all parameters to zero.</li>
<li>Initialize parameters to random values from standard normal distribution or uniform distribution and multiply it by a scalar such as 10.</li>
<li>Initialize parameters based on:
<ul>
<li>Xavier recommendation.</li>
<li>Kaiming He recommendation.</li>
</ul>
</li>
</ul>
<p>We&rsquo;ll be using functions we wrote in <a href="https://imaddabbura.github.io/blog/machine%20learning/deep%20learning/2018/04/01/coding-neural-network-fwd-back-prop.html"><em>&ldquo;Coding Neural Network - Forward Propagation and Backpropagation&rdquo;</em></a> post to initialize parameters, compute forward propagation and back-propagation as well as the cross-entropy cost.</p>
<p>To illustrate the above cases, we&rsquo;ll use the cats vs dogs dataset which consists of 50 images for cats and 50 images for dogs. Each image is 150 pixels x 150 pixels on RGB color scale. Therefore, we would have 67,500 features where each column in the input matrix would be one image which means our input data would have 67,500 x 100 dimension.</p>
<p>Let&rsquo;s first load the data and show a sample of two images before we start the helper functions.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Loading packages</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> sys
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> h5py
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#34;../scripts/&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> coding_neural_network_from_scratch <span style="color:#f92672">import</span> (L_model_forward,
</span></span><span style="display:flex;"><span>                                                compute_cost,
</span></span><span style="display:flex;"><span>                                                L_model_backward,
</span></span><span style="display:flex;"><span>                                                update_parameters,
</span></span><span style="display:flex;"><span>                                                accuracy)
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> load_dataset <span style="color:#f92672">import</span> load_dataset_catvsdog
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">%</span>matplotlib inline
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>set_context(<span style="color:#e6db74">&#34;notebook&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#34;fivethirtyeight&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X, Y <span style="color:#f92672">=</span> load_dataset_catvsdog(<span style="color:#e6db74">&#34;../data&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># show a sample of of a cat and a dog image</span>
</span></span><span style="display:flex;"><span>index_cat <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(Y); index_dog <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmin(Y)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>imshow(X[:, index_cat]<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">150</span>, <span style="color:#ae81ff">150</span>, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#34;off&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>imshow(X[:, index_dog]<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">150</span>, <span style="color:#ae81ff">150</span>, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#34;off&#34;</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># standarize the data</span>
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> X <span style="color:#f92672">/</span> <span style="color:#ae81ff">255</span>
</span></span></code></pre></div>


  




<figure>

  <a data-fancybox="" href="/img/coding-nn-from-scratch/sample_images.png" >

<img src="/img/coding-nn-from-scratch/sample_images.png" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Sample images.</h4>
  
</figcaption>

</figure>

<p>We&rsquo;ll write now all the helper functions that will help us initialize parameters based on different methods as well as writing L-layer model that we&rsquo;ll be using to train our neural network.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">initialize_parameters_zeros</span>(layers_dims):
</span></span><span style="display:flex;"><span>    np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">1</span>)               
</span></span><span style="display:flex;"><span>    parameters <span style="color:#f92672">=</span> {}                 
</span></span><span style="display:flex;"><span>    L <span style="color:#f92672">=</span> len(layers_dims)            
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> l <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, L):
</span></span><span style="display:flex;"><span>        parameters[<span style="color:#e6db74">&#34;W&#34;</span> <span style="color:#f92672">+</span> str(l)] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(
</span></span><span style="display:flex;"><span>            (layers_dims[l], layers_dims[l <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>]))
</span></span><span style="display:flex;"><span>        parameters[<span style="color:#e6db74">&#34;b&#34;</span> <span style="color:#f92672">+</span> str(l)] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((layers_dims[l], <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> parameters
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">initialize_parameters_random</span>(layers_dims):
</span></span><span style="display:flex;"><span>    np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">1</span>)               
</span></span><span style="display:flex;"><span>    parameters <span style="color:#f92672">=</span> {}                 
</span></span><span style="display:flex;"><span>    L <span style="color:#f92672">=</span> len(layers_dims)            
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> l <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, L):
</span></span><span style="display:flex;"><span>        parameters[<span style="color:#e6db74">&#34;W&#34;</span> <span style="color:#f92672">+</span> str(l)] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(
</span></span><span style="display:flex;"><span>            layers_dims[l], layers_dims[l <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>]) <span style="color:#f92672">*</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>        parameters[<span style="color:#e6db74">&#34;b&#34;</span> <span style="color:#f92672">+</span> str(l)] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((layers_dims[l], <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> parameters
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">initialize_parameters_he_xavier</span>(layers_dims, initialization_method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;he&#34;</span>):
</span></span><span style="display:flex;"><span>    np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">1</span>)               
</span></span><span style="display:flex;"><span>    parameters <span style="color:#f92672">=</span> {}                 
</span></span><span style="display:flex;"><span>    L <span style="color:#f92672">=</span> len(layers_dims)            
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> initialization_method <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;he&#34;</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> l <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, L):
</span></span><span style="display:flex;"><span>            parameters[<span style="color:#e6db74">&#34;W&#34;</span> <span style="color:#f92672">+</span> str(l)] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(
</span></span><span style="display:flex;"><span>                layers_dims[l],
</span></span><span style="display:flex;"><span>                layers_dims[l <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>]) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">2</span> <span style="color:#f92672">/</span> layers_dims[l <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>            parameters[<span style="color:#e6db74">&#34;b&#34;</span> <span style="color:#f92672">+</span> str(l)] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((layers_dims[l], <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> initialization_method <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;xavier&#34;</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> l <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, L):
</span></span><span style="display:flex;"><span>            parameters[<span style="color:#e6db74">&#34;W&#34;</span> <span style="color:#f92672">+</span> str(l)] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(
</span></span><span style="display:flex;"><span>                layers_dims[l],
</span></span><span style="display:flex;"><span>                layers_dims[l <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>]) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> layers_dims[l <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>            parameters[<span style="color:#e6db74">&#34;b&#34;</span> <span style="color:#f92672">+</span> str(l)] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((layers_dims[l], <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> parameters
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">model</span>(X, Y, layers_dims, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>, num_iterations<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>,
</span></span><span style="display:flex;"><span>          print_cost<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, hidden_layers_activation_fn<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;relu&#34;</span>,
</span></span><span style="display:flex;"><span>          initialization_method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;he&#34;</span>):
</span></span><span style="display:flex;"><span>    np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># initialize cost list</span>
</span></span><span style="display:flex;"><span>    cost_list <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># initialize parameters</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> initialization_method <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;zeros&#34;</span>:
</span></span><span style="display:flex;"><span>        parameters <span style="color:#f92672">=</span> initialize_parameters_zeros(layers_dims)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> initialization_method <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;random&#34;</span>:
</span></span><span style="display:flex;"><span>        parameters <span style="color:#f92672">=</span> initialize_parameters_random(layers_dims)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        parameters <span style="color:#f92672">=</span> initialize_parameters_he_xavier(
</span></span><span style="display:flex;"><span>            layers_dims, initialization_method)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># iterate over num_iterations</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_iterations):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># iterate over L-layers to get the final output and the cache</span>
</span></span><span style="display:flex;"><span>        AL, caches <span style="color:#f92672">=</span> L_model_forward(
</span></span><span style="display:flex;"><span>            X, parameters, hidden_layers_activation_fn)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># compute cost to plot it</span>
</span></span><span style="display:flex;"><span>        cost <span style="color:#f92672">=</span> compute_cost(AL, Y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># iterate over L-layers backward to get gradients</span>
</span></span><span style="display:flex;"><span>        grads <span style="color:#f92672">=</span> L_model_backward(AL, Y, caches, hidden_layers_activation_fn)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># update parameters</span>
</span></span><span style="display:flex;"><span>        parameters <span style="color:#f92672">=</span> update_parameters(parameters, grads, learning_rate)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># append each 100th cost to the cost list</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">and</span> print_cost:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">&#34;The cost after </span><span style="color:#e6db74">{}</span><span style="color:#e6db74"> iterations is: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, cost))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> i <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            cost_list<span style="color:#f92672">.</span>append(cost)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># plot the cost curve</span>
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">8</span>))
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(cost_list)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;Iterations (per hundreds)&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">14</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Cost&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">14</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>title(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Cost curve: learning rate = </span><span style="color:#e6db74">{}</span><span style="color:#e6db74"> and </span><span style="color:#e6db74">{}</span><span style="color:#e6db74"> initialization method&#34;</span><span style="color:#f92672">.</span>format(
</span></span><span style="display:flex;"><span>            learning_rate, initialization_method), y<span style="color:#f92672">=</span><span style="color:#ae81ff">1.05</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> parameters
</span></span></code></pre></div><!-- raw HTML omitted -->
<p>Here, we&rsquo;ll initialize all weight matrices and biases to zeros and see how this would affect the error rate as well as the learning parameters.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># train NN with zeros initialization parameters</span>
</span></span><span style="display:flex;"><span>layers_dims <span style="color:#f92672">=</span> [X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>parameters <span style="color:#f92672">=</span> model(X, Y, layers_dims, hidden_layers_activation_fn<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tanh&#34;</span>,
</span></span><span style="display:flex;"><span>                   initialization_method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;zeros&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accuracy(X, parameters, Y,<span style="color:#e6db74">&#34;tanh&#34;</span>)
</span></span></code></pre></div><pre><code>The cost after 100 iterations is: 0.6931471805599453
The cost after 200 iterations is: 0.6931471805599453
The cost after 300 iterations is: 0.6931471805599453
The cost after 400 iterations is: 0.6931471805599453
The cost after 500 iterations is: 0.6931471805599453
The cost after 600 iterations is: 0.6931471805599453
The cost after 700 iterations is: 0.6931471805599453
The cost after 800 iterations is: 0.6931471805599453
The cost after 900 iterations is: 0.6931471805599453
The cost after 1000 iterations is: 0.6931471805599453

'The accuracy rate is: 50.00%.'
</code></pre>



  




<figure>

  <a data-fancybox="" href="/img/coding-nn-from-scratch/zero_params.png" >

<img src="/img/coding-nn-from-scratch/zero_params.png" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Cost curve using zero intialization method.</h4>
  
</figcaption>

</figure>

<p>As the cost curve shows, the neural network didn&rsquo;t learn anything! That is because of symmetry between all neurons which leads to all neurons have the same update on every iteration. Therefore, regardless of how many iterations we run the optimization algorithms, all the neurons would still get the same update and no learning would happen. As a result, we must <strong>break symmetry</strong> when initializing parameters so that the model would start learning on each update of the gradient descent.</p>
<!-- raw HTML omitted -->
<p>There is no big difference if the random values are initialized from standard normal distribution or uniform distribution so we&rsquo;ll use standard normal distribution in our examples. Also, we&rsquo;ll multiply the random values by a big number such as 10 to show that initializing parameters to big values may cause our optimization to have higher error rates (and even diverge in some cases). Let&rsquo;s now train our neural network where all weight matrices have been intitialized using the following formula:
<code>np.random.randn() * 10</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># train NN with random initialization parameters</span>
</span></span><span style="display:flex;"><span>layers_dims <span style="color:#f92672">=</span> [X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>parameters <span style="color:#f92672">=</span> model(X, Y, layers_dims, hidden_layers_activation_fn<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tanh&#34;</span>,
</span></span><span style="display:flex;"><span>                   initialization_method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;random&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accuracy(X, parameters, Y,<span style="color:#e6db74">&#34;tanh&#34;</span>)
</span></span></code></pre></div><pre><code>The cost after 100 iterations is: 1.2413142077549013
The cost after 200 iterations is: 1.1258751902393416
The cost after 300 iterations is: 1.0989052435267657
The cost after 400 iterations is: 1.0840966471282327
The cost after 500 iterations is: 1.0706953292105978
The cost after 600 iterations is: 1.0574847320236294
The cost after 700 iterations is: 1.0443168708889223
The cost after 800 iterations is: 1.031157857251139
The cost after 900 iterations is: 1.0179838815204902
The cost after 1000 iterations is: 1.004767088515343

'The accuracy rate is: 55.00%.'
</code></pre>



  




<figure>

  <a data-fancybox="" href="/img/coding-nn-from-scratch/random_weights.png" >

<img src="/img/coding-nn-from-scratch/random_weights.png" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Cost curve using random initialization method.</h4>
  
</figcaption>

</figure>

<p>Random initialization here is helping but still the loss function has high value and may take long time to converge and achieve a significantly low value.</p>
<!-- raw HTML omitted -->
<p>We&rsquo;ll explore two initialization methods:</p>
<ul>
<li>Kaiming He method is best applied when activation function applied on hidden layers is Rectified Linear Unit (ReLU). so that the weight on each hidden layer would have the following variance:</li>
</ul>
<p>$$var(W^l) = \frac{2}{n^{l - 1}}$$
We can achieve this by multiplying the random values from standard normal distribution by $\sqrt{\frac{2}{number\ of\ units\ in \ previous\ layer}}$</p>
<ul>
<li>Xavier method is best applied when activation function applied on hidden layers is Hyperbolic Tangent so that the weight on each hidden layer would have the following variance:</li>
</ul>
<p>$$var(W^l) = \frac{1}{n^{l - 1}}$$
We can achieve this by multiplying the random values from standard normal distribution by $\sqrt{\frac{1}{number\ of\ units\ in \ previous\ layer}}$</p>
<p>We&rsquo;ll train the network using both methods and look at the results.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># train NN where all parameters were initialized based on He recommendation</span>
</span></span><span style="display:flex;"><span>layers_dims <span style="color:#f92672">=</span> [X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>parameters <span style="color:#f92672">=</span> model(X, Y, layers_dims, hidden_layers_activation_fn<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tanh&#34;</span>,
</span></span><span style="display:flex;"><span>                   initialization_method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;he&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accuracy(X, parameters, Y, <span style="color:#e6db74">&#34;tanh&#34;</span>)
</span></span></code></pre></div><pre><code>The cost after 100 iterations is: 0.6300611704834093
The cost after 200 iterations is: 0.49092836452522753
The cost after 300 iterations is: 0.46579423512433943
The cost after 400 iterations is: 0.6516254192289226
The cost after 500 iterations is: 0.32487779301799485
The cost after 600 iterations is: 0.4631461605716059
The cost after 700 iterations is: 0.8050310690163623
The cost after 800 iterations is: 0.31739195517372376
The cost after 900 iterations is: 0.3094592175030812
The cost after 1000 iterations is: 0.19934509244449203

'The accuracy rate is: 99.00%.'
</code></pre>



  




<figure>

  <a data-fancybox="" href="/img/coding-nn-from-scratch/he.png" >

<img src="/img/coding-nn-from-scratch/he.png" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Cost curve using He initialization method.</h4>
  
</figcaption>

</figure>

<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># train NN where all parameters were initialized based on Xavier recommendation</span>
</span></span><span style="display:flex;"><span>layers_dims <span style="color:#f92672">=</span> [X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>parameters <span style="color:#f92672">=</span> model(X, Y, layers_dims, hidden_layers_activation_fn<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tanh&#34;</span>,
</span></span><span style="display:flex;"><span>                   initialization_method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;xavier&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accuracy(X, parameters, Y, <span style="color:#e6db74">&#34;tanh&#34;</span>)
</span></span></code></pre></div><pre><code>The cost after 100 iterations is: 0.6351961521800779
The cost after 200 iterations is: 0.548973489787121
The cost after 300 iterations is: 0.47982386652748565
The cost after 400 iterations is: 0.32811768889968684
The cost after 500 iterations is: 0.2793453045790634
The cost after 600 iterations is: 0.3258507563809604
The cost after 700 iterations is: 0.2873032724176074
The cost after 800 iterations is: 0.0924974839405706
The cost after 900 iterations is: 0.07418011931058155
The cost after 1000 iterations is: 0.06204402572328295

'The accuracy rate is: 99.00%.'
</code></pre>



  




<figure>

  <a data-fancybox="" href="/img/coding-nn-from-scratch/xafier.png" >

<img src="/img/coding-nn-from-scratch/xafier.png" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Cost curve using Xavier initialization method.</h4>
  
</figcaption>

</figure>

<p>As shown from applying the four methods, parameters&rsquo; initial values play a huge role in achieving low cost values as well as converging and achieve lower training error rates. The same would apply to test error rate if we had test data.</p>
<!-- raw HTML omitted -->
<p>Deep Learning frameworks make it easier to choose between different initialization methods without worrying about implementing it ourselves. Nonetheless, it&rsquo;s important to understand the critical role initial values of the parameters in the overall performance of the network. Below are some key takeaways:</p>
<ul>
<li>Well chosen initialization values of parameters leads to:
<ul>
<li>Speed up convergence of gradient descent.</li>
<li>Increase the likelihood of gradient descent to find lower training and generalization error rates.</li>
</ul>
</li>
<li>Because we&rsquo;re dealing with iterative optimization algorithms with non-convex loss function, different initializations lead to different results.</li>
<li>Random initialization is used to break symmetry and make sure different hidden units can learn different things.</li>
<li>Don&rsquo;t initialize to values that are too large.</li>
<li>Kaiming He (He) initialization works well for neural networks with ReLU activation function.</li>
<li>Xavier initialization works well for neural networks with Hyperbolic Tangent activation function.</li>
</ul>
<p>The source code that created this post can be found <a href="https://github.com/ImadDabbura/blog-posts/blob/master/notebooks/Coding-Neural-Network-Parameters-Initialization.ipynb">here</a>.</p>

    </div>

    


    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/deep-learning/">Deep Learning</a>
  
  <a class="badge badge-light" href="/tags/ai/">AI</a>
  
</div>



    
      








  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hu057a6ce318cedc9fff5e5d4bcb0cb87d_16356_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="https://imaddabbura.github.io">Imad Dabbura</a></h5>
      <h6 class="card-subtitle">Senior Data Scientist</h6>
      <p class="card-text" itemprop="description">My interests include data science, machine learning and artificial intelligence.</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/#contact" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://twitter.com/imadphd" target="_blank" rel="noopener">
              <i class="fab fa-twitter"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="//www.linkedin.com/in/imaddabbura" >
              <i class="fab fa-linkedin"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/imaddabbura" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="//medium.com/@ImadPhd" >
              <i class="fab fa-medium"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/post/coding-nn-gradient-checking/">Coding Neural Network - Gradient Checking</a></li>
          
          <li><a href="/post/coding-nn-fwd-bckwd-prop/">Coding Neural Network - Forward Propagation and Backpropagtion</a></li>
          
          <li><a href="/post/character-level-language-model/">Character-level Language Model</a></li>
          
          <li><a href="/post/gradient-descent-algorithm/">Gradient Descent Algorithm and Its Variants</a></li>
          
        </ul>
      </div>
      
    

    
    <div class="article-widget">
      
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/post/coding-nn-regularization/" rel="next">Coding Neural Network - Regularization</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/coding-nn-gradient-checking/" rel="prev">Coding Neural Network - Gradient Checking</a>
  </div>
  
</div>

    </div>
    

    
<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "ImadDabbura" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>



  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js" integrity="sha256-0w92bcB21IY5+rGI84MGj52jNfHNbXVeQLrZ0CGdjNY=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/python.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    <script id="dsq-count-scr" src="https://ImadDabbura.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.842f599ee533ad7f6dbd4e00e95d3d79.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
     Imad Dabbura 2018 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
