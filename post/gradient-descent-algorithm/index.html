<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.4.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Imad Dabbura">

  
  
  
    
  
  <meta name="description" content="Optimization refers to the task of minimizing/maximizing an objective function f(x) parameterized by x. In machine/deep learning terminology, it&rsquo;s the task of minimizing the cost/loss function J(w) parameterized by the model&rsquo;s parameters $w \in \mathbb{R}^d$. Optimization algorithms (in case of minimization) have one of the following goals:
 Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum.">

  
  <link rel="alternate" hreflang="en-us" href="https://imaddabbura.github.io/post/gradient-descent-algorithm/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.e8b81181bd18f58d8990b40dc8a5e4b8.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-127825273-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="https://www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://imaddabbura.github.io/post/gradient-descent-algorithm/">

  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@ImadPhd">
  <meta property="twitter:creator" content="@ImadPhd">
  
  <meta property="og:site_name" content="Imad Dabbura">
  <meta property="og:url" content="https://imaddabbura.github.io/post/gradient-descent-algorithm/">
  <meta property="og:title" content="Gradient Descent Algorithm and Its Variants | Imad Dabbura">
  <meta property="og:description" content="Optimization refers to the task of minimizing/maximizing an objective function f(x) parameterized by x. In machine/deep learning terminology, it&rsquo;s the task of minimizing the cost/loss function J(w) parameterized by the model&rsquo;s parameters $w \in \mathbb{R}^d$. Optimization algorithms (in case of minimization) have one of the following goals:
 Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum."><meta property="og:image" content="https://imaddabbura.github.io/post/gradient-descent-algorithm/featured.PNG">
  <meta property="twitter:image" content="https://imaddabbura.github.io/post/gradient-descent-algorithm/featured.PNG"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2017-12-21T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2017-12-21T00:00:00&#43;00:00">
  

  

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.js"></script>
<script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#2962ff",
          "text": "#fff"
        },
        "button": {
          "background": "#fff",
          "text": "#2962ff"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "https://cookies.insites.com"
      }
    })});
</script>



  





  <title>Gradient Descent Algorithm and Its Variants | Imad Dabbura</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Imad Dabbura</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/til/"><span>Today I Learned</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/resume.pdf"><span>Resume</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1 itemprop="name">Gradient Descent Algorithm and Its Variants</h1>

  

  



<meta content="2017-12-21 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2017-12-21 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    <time>Dec 21, 2017</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    10 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="/post/gradient-descent-algorithm/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/machine-learning/">Machine Learning</a>, <a href="/categories/ai/">AI</a>, <a href="/categories/deep-learning/">Deep Learning</a>, <a href="/categories/optimization-algorithms/">Optimization Algorithms</a></span>
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://imaddabbura.github.io/post/gradient-descent-algorithm/&amp;text=Gradient%20Descent%20Algorithm%20and%20Its%20Variants" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://imaddabbura.github.io/post/gradient-descent-algorithm/&amp;t=Gradient%20Descent%20Algorithm%20and%20Its%20Variants" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Gradient%20Descent%20Algorithm%20and%20Its%20Variants&amp;body=https://imaddabbura.github.io/post/gradient-descent-algorithm/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://imaddabbura.github.io/post/gradient-descent-algorithm/&amp;title=Gradient%20Descent%20Algorithm%20and%20Its%20Variants" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Gradient%20Descent%20Algorithm%20and%20Its%20Variants%20https://imaddabbura.github.io/post/gradient-descent-algorithm/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://imaddabbura.github.io/post/gradient-descent-algorithm/&amp;title=Gradient%20Descent%20Algorithm%20and%20Its%20Variants" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

  














</div>


<div class="article-header container featured-image-wrapper mt-4 mb-4" style="max-width: 1200px; max-height: 678px;">
  <div style="position: relative">
    <img src="/post/gradient-descent-algorithm/featured_hu6c17d2d6288aaa5362bfbb89cf77ab07_2413744_1200x0_resize_lanczos_3.PNG" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <p><strong>Optimization</strong> refers to the task of minimizing/maximizing an objective function <em>f(x)</em> parameterized by <em>x</em>. In machine/deep learning terminology, it&rsquo;s the task of minimizing the cost/loss function <em>J(w)</em> parameterized by the model&rsquo;s parameters $w \in \mathbb{R}^d$. Optimization algorithms (in case of minimization) have one of the following goals:</p>
<ul>
<li>Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum.</li>
<li>Find the lowest possible value of the objective function within its neighborhood. That&rsquo;s usually the case if the objective function is not convex as the case in most deep learning problems.</li>
</ul>
<p>There are three kinds of optimization algorithms:</p>
<ul>
<li>Optimization algorithm that is not iterative and simply solves for one point.</li>
<li>Optimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression.</li>
<li>Optimization algorithm that is iterative in nature and applied to a set of problems that have non-convex cost functions such as neural networks. Therefore, parameters&rsquo; initialization plays a critical role in speeding up convergence and achieving lower error rates.</li>
</ul>
<p><strong>Gradient Descent</strong> is the most common optimization algorithm in <em>machine learning</em> and <em>deep learning</em>. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function <em>J(w)</em> w.r.t the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate α. Therefore, we follow the direction of the slope downhill until we reach a local minimum.</p>
<p>In this post, we&rsquo;ll cover gradient descent algorithm and its variants: <em>Batch Gradient Descent, Mini-batch Gradient Descent, and Stochastic Gradient Descent</em>.</p>
<p>Let&rsquo;s first see how gradient descent and its associated steps works on logistic regression before going into the details of its variants. For the sake of simplicity, let&rsquo;s assume that the logistic regression model has only two parameters: weight <em>w</em> and bias <em>b</em>.</p>
<ol>
<li>
<p>Initialize weight <em>w</em> and bias <em>b</em> to any random numbers.</p>
</li>
<li>
<p>Pick a value for the learning rate α. The learning rate determines how big the step would be on each iteration.</p>
<ul>
<li>If α is very small, it would take long time to converge and become computationally expensive.</li>
<li>IF α is large, it may fail to converge and overshoot the minimum.</li>
</ul>
<p>Therefore, plot the cost function against different values of α and pick the value of α that is right before the first value that didn&rsquo;t converge so that we would have a very fast learning algorithm that converges (see figure 1).



  




<figure>

  <a data-fancybox="" href="/img/gradient-descent-algorithms/learning_rate.PNG" >

<img src="/img/gradient-descent-algorithms/learning_rate.PNG" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Gradient descent with different learning rates.</h4>
  <p>
    
    <a href="http://cs231n.github.io/neural-networks-3/"> 
    Source
    </a> 
  </p> 
</figcaption>

</figure>
</p>
<ul>
<li>The most commonly used rates are : <em>0.001, 0.003, 0.01, 0.03, 0.1, 0.3</em>.</li>
</ul>
</li>
<li>
<p>Make sure to scale the data if it&rsquo;s on very different scales. If we don&rsquo;t scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge (see figure 2).



  




<figure>

  <a data-fancybox="" href="/img/gradient-descent-algorithms/normalized-vs-unnormalized.PNG" >

<img src="/img/gradient-descent-algorithms/normalized-vs-unnormalized.PNG" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Gradient descent: normalized versus unnormalized level curves.</h4>
  
</figcaption>

</figure>
</p>
<p>Scale the data to have μ = 0 and σ = 1. Below is the formula for scaling each example:
$$\frac {x_i - \mu}{\sigma}$$</p>
</li>
<li>
<p>On each iteration, take the partial derivative of the cost function <em>J(w)</em> w.r.t each parameter (gradient):</p>
</li>
</ol>
<p>$$\frac{\partial}{\partial w}J(w, b) = \nabla_wJ$$</p>
<p>$$\frac{\partial}{\partial b}J(w, b) = \nabla_bJ$$</p>
<p>The update equations are:</p>
<p>$$w = w - \alpha \nabla_w J$$</p>
<p>$$b = b - \alpha \nabla_b J$$</p>
<ul>
<li>For the sake of illustration, let’s assume we don’t have bias. If the slope of the current value of <em>w &gt; 0</em>, this means that we are to the right of optimal <em>w</em>*. Therefore, the update will be negative, and will start getting close to the optimal values of <em>w</em>*. However, if it’s negative, the update will be positive and will increase the current values of <em>w</em> to converge to the optimal values of <em>w</em>*(see figure 3):



  




<figure>

  <a data-fancybox="" href="/img/gradient-descent-algorithms/gradients.PNG" >

<img src="/img/gradient-descent-algorithms/gradients.PNG" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Gradient descent. An illustration of how gradient descent algorithm uses the first derivative of the loss function to follow downhill it&rsquo;s minimum.</h4>
  
</figcaption>

</figure>

<ul>
<li>Continue the process until the cost function converges. That is, until the error curve becomes flat and doesn&rsquo;t change.</li>
<li>In addition, on each iteration, the step would be in the direction that gives the <em>maximum</em> change since it&rsquo;s perpendicular to level curves at each step.</li>
</ul>
</li>
</ul>
<p>Now let&rsquo;s discuss the three variants of gradient descent algorithm. The main difference between them is the amount of data we use when computing the gradients for each learning step. The trade-off between them is the accuracy of the gradient versus the time complexity to perform each parameter&rsquo;s update (learning step).</p>
<!-- raw HTML omitted -->
<p>Batch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples:
$$w = w - \alpha \nabla_w J$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_epochs):
</span></span><span style="display:flex;"><span>  grad <span style="color:#f92672">=</span> compute_gradient(data, params)
</span></span><span style="display:flex;"><span>  params <span style="color:#f92672">=</span> params <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> grad
</span></span></code></pre></div><p>The main advantages:</p>
<ul>
<li>We can use fixed learning rate during training without worrying about learning rate decay.</li>
<li>It has straight trajectory towards the minimum and it is guaranteed to converge in theory to the global minimum if the loss function is convex and to a local minimum if the loss function is not convex.</li>
<li>It has unbiased estimate of gradients. The more the examples, the lower the standard error.</li>
</ul>
<p>The main disadvantages:</p>
<ul>
<li>Even though we can use vectorized implementation, it may still be slow to go over all examples especially when we have large datasets.</li>
<li>Each step of learning happens after going over all examples where some examples may be redundant and don&rsquo;t contribute much to the update.</li>
</ul>
<!-- raw HTML omitted -->
<p>Instead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on batch size. Therefore, learning happens on each mini-batch of <em>b</em> examples:
$$w = w - \alpha \nabla_w J(x^{{i:i + b}}, y^{{i: i + b}}; w, b)$$</p>
<ul>
<li>Shuffle the training dataset to avoid pre-existing order of examples.</li>
<li>Partition the training dataset into <em>b</em> mini-batches based on the batch size. If the training set size is not divisible by batch size, the remaining will be its own batch.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_epochs):
</span></span><span style="display:flex;"><span>    np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>shuffle(data)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> radom_minibatches(data, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>):
</span></span><span style="display:flex;"><span>        grad <span style="color:#f92672">=</span> compute_gradient(batch, params)
</span></span><span style="display:flex;"><span>        params <span style="color:#f92672">=</span> params <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> grad
</span></span></code></pre></div><p>The batch size is something we can tune. It is usually chosen as power of 2 such as 32, 64, 128, 256, 512, etc. The reason behind it is because some hardware such as GPUs achieve better runtime with common batch sizes such as power of 2.</p>
<p>The main advantages:</p>
<ul>
<li>Faster than Batch version because it goes through a lot less examples than Batch (all examples).</li>
<li>Randomly selecting examples will help avoid redundant examples or examples that are very similar that don&rsquo;t contribute much to the learning.</li>
<li>With batch size &lt; size of training set, it adds noise to the learning process that helps improving generalization error.</li>
<li>Even though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur.</li>
</ul>
<p>The main disadvantages:</p>
<ul>
<li>It won&rsquo;t converge. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges.</li>
<li>Due to the noise, the learning steps have more oscillations (see figure 4) and requires adding learning-decay to decrease the learning rate as we become closer to the minimum.



  




<figure>

  <a data-fancybox="" href="/img/gradient-descent-algorithms/batch-vs-minibatch.PNG" >

<img src="/img/gradient-descent-algorithms/batch-vs-minibatch.PNG" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Gradient descent: batch versus mini-batch loss function.</h4>
  
</figcaption>

</figure>

With large training datasets, we don&rsquo;t usually need more than 2-10 passes over all training examples (epochs). Note: with batch size <em>b = m</em>, we get the Batch Gradient Descent.</li>
</ul>
<!-- raw HTML omitted -->
<p>Instead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example $(x^i, y^i)$. Therefore, learning happens on every example:
$$w = w - \alpha \nabla_w J(x^i, y^i; w, b)$$</p>
<ul>
<li>Shuffle the training dataset to avoid pre-existing order of examples.</li>
<li>Partition the training dataset into <em>m</em> examples.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_epochs):
</span></span><span style="display:flex;"><span>    np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>shuffle(data)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> example <span style="color:#f92672">in</span> data:
</span></span><span style="display:flex;"><span>        grad <span style="color:#f92672">=</span> compute_gradient(example, params)
</span></span><span style="display:flex;"><span>        params <span style="color:#f92672">=</span> params <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> grad
</span></span></code></pre></div><p>It shares most of the advantages and the disadvantages with mini-batch version. Below are the ones that are specific to SGD:</p>
<ul>
<li>It adds even more noise to the learning process than mini-batch that helps improving generalization error. However, this would increase the run time.</li>
<li>We can&rsquo;t utilize vectorization over 1 example and becomes very slow. Also, the variance becomes large since we only use 1 example for each learning step.</li>
</ul>
<p>Below is a graph that shows the gradient descent&rsquo;s variants and their direction towards the minimum:



  




<figure>

  <a data-fancybox="" href="/img/gradient-descent-algorithms/batch-vs-minibatch-vs-stochastic.PNG" >

<img src="/img/gradient-descent-algorithms/batch-vs-minibatch-vs-stochastic.PNG" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Gradient descent variants&rsquo; trajectory towards minimum</h4>
  
</figcaption>

</figure>

As the figure above shows, SGD direction is very noisy compared to mini-batch.</p>
<!-- raw HTML omitted -->
<p>Below are some challenges regarding gradient descent algorithm in general as well as its variants - mainly batch and mini-batch:</p>
<ul>
<li>
<p>Gradient descent is a first-order optimization algorithm, which means it doesn&rsquo;t take into account the second derivatives of the cost function. However, the curvature of the function affects the size of each learning step. The gradient measures the steepness of the curve but the second derivative measures the curvature of the curve. Therefore, if:</p>
<ol>
<li>Second derivative = 0 &ndash;&gt; the curvature is linear. Therefore, the step size = the learning rate α.</li>
<li>Second derivative &gt; 0 &ndash;&gt; the curvature is going upward. Therefore, the step size &lt; the learning rate α and may lead to divergence.</li>
<li>Second derivative &lt; 0 &ndash;&gt; the curvature is going downward. Therefore, the step size &gt; the learning rate α.</li>
</ol>
<p>As a result, the direction that looks promising to the gradient may not be so and may lead to slow the learning process or even diverge.</p>
</li>
<li>
<p>If Hessian matrix has poor conditioning number, i.e. the direction of the most curvature has much more curvature than the direction of the lowest curvature. This will lead the cost function to be very sensitive in some directions and insensitive in other directions. As a result, it will make it harder on the gradient because the direction that looks promising for the gradient may not lead to big changes in the cost function (see figure 6).



  




<figure>

  <a data-fancybox="" href="/img/gradient-descent-algorithms/curvature.PNG" >

<img src="/img/gradient-descent-algorithms/curvature.PNG" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Gradient descent fails to exploit the curvature information contained in the Hessian matrix.</h4>
  
</figcaption>

</figure>
</p>
</li>
<li>
<p>The norm of the gradient $g^Tg$ is supposed to decrease slowly with each learning step because the curve is getting flatter and steepness of the curve will decrease. However, we see that the norm of the gradient is increasing, because of the curvature of the curve. Nonetheless, even though the gradients&rsquo; norm is increasing, we&rsquo;re able to achieve a very low error rates  (see figure 7).



  




<figure>

  <a data-fancybox="" href="/img/gradient-descent-algorithms/gradient_norm.PNG" >

<img src="/img/gradient-descent-algorithms/gradient_norm.PNG" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Gradient norm.</h4>
  <p>
    
    <a href="http://www.deeplearningbook.org/contents/numerical.html"> 
    Source
    </a> 
  </p> 
</figcaption>

</figure>
</p>
</li>
<li>
<p>In small dimensions, local minimum is common; however, in large dimensions, saddle points are more common. Saddle point is when the function curves up in some directions and curves down in other directions. In other words, saddle point looks a minimum from one direction and a maximum from other direction (see figure 8). This happens when at least one eigenvalue of the hessian matrix is negative and the rest of eigenvalues are positive.



  




<figure>

  <a data-fancybox="" href="/img/gradient-descent-algorithms/saddle.PNG" >

<img src="/img/gradient-descent-algorithms/saddle.PNG" >
</a>


<figcaption data-pre="Figure " data-post=":" class="numbered">
  <h4>Saddle point.</h4>
  
</figcaption>

</figure>
</p>
</li>
<li>
<p>As discussed previously, choosing a proper learning rate is hard. Also, for mini-batch gradient descent, we have to adjust the learning rate during the training process to make sure it converges to the local minimum and not wander around it. Figuring out the decay rate of the learning rate is also hard and changes with different datasets.</p>
</li>
<li>
<p>All parameter updates have the same learning rate; however, we may want to perform larger updates to some parameters that have their directional derivatives more inline with the trajectory towards the minimum than other parameters.</p>
</li>
</ul>

    </div>

    


    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/machine-learning/">Machine Learning</a>
  
  <a class="badge badge-light" href="/tags/ai/">AI</a>
  
  <a class="badge badge-light" href="/tags/deep-learning/">Deep Learning</a>
  
  <a class="badge badge-light" href="/tags/optimization-algorithms/">Optimization Algorithms</a>
  
</div>



    
      








  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hu057a6ce318cedc9fff5e5d4bcb0cb87d_16356_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="https://imaddabbura.github.io">Imad Dabbura</a></h5>
      <h6 class="card-subtitle">Senior Data Scientist</h6>
      <p class="card-text" itemprop="description">My interests include data science, machine learning and artificial intelligence.</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/#contact" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://twitter.com/imadphd" target="_blank" rel="noopener">
              <i class="fab fa-twitter"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="//www.linkedin.com/in/imaddabbura" >
              <i class="fab fa-linkedin"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/imaddabbura" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="//medium.com/@ImadPhd" >
              <i class="fab fa-medium"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/post/pred-employee-turnover/">Predicting Employee Turnover</a></li>
          
        </ul>
      </div>
      
    

    
    <div class="article-widget">
      
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/post/character-level-language-model/" rel="next">Character-level Language Model</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/pred-employee-turnover/" rel="prev">Predicting Employee Turnover</a>
  </div>
  
</div>

    </div>
    

    
<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "ImadDabbura" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>



  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js" integrity="sha256-0w92bcB21IY5+rGI84MGj52jNfHNbXVeQLrZ0CGdjNY=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/python.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    <script id="dsq-count-scr" src="https://ImadDabbura.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.9224c1df7774818c46d3c5196c37127b.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    © Imad Dabbura 2018 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
