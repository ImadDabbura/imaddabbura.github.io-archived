<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Imad Dabbura</title>
    <link>https://imaddabbura.github.io/</link>
      <atom:link href="https://imaddabbura.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Imad Dabbura</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Imad Dabbura 2018</copyright><lastBuildDate>Mon, 22 Apr 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://imaddabbura.github.io/img/icon-192.png</url>
      <title>Imad Dabbura</title>
      <link>https://imaddabbura.github.io/</link>
    </image>
    
    <item>
      <title>Transient Ischemic Attack</title>
      <link>https://imaddabbura.github.io/project/transient-ischemic-attack/</link>
      <pubDate>Mon, 22 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://imaddabbura.github.io/project/transient-ischemic-attack/</guid>
      <description>&lt;p&gt;A transient ischemic attack (TIA) is like a stroke, producing similar symptoms, but usually lasting only a few minutes and causing no permanent damage. Often called a ministroke, a transient ischemic attack may be a warning.&lt;/p&gt;

&lt;p&gt;About 1 in 3 people who have a transient ischemic attack will eventually have a stroke, with about half occurring within a year after the transient ischemic attack.&lt;/p&gt;

&lt;p&gt;The goal of this project is to build a binary classifier to predict Transient Ischemic Attack (TIA) and then deploy the best model as a RESTful API.&lt;/p&gt;

&lt;p&gt;Check out the project on &lt;a href=&#34;https://github.com/ImadDabbura/transient-ischemic-attack&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Modeling with Postgres</title>
      <link>https://imaddabbura.github.io/project/data-modeling-with-postgres/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://imaddabbura.github.io/project/data-modeling-with-postgres/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The goal of this project is to build a PostgreSQL database utilizing the data on users activity and songs metadata. Building the database helps us do complex analytics regarding users activity as well as song play analysis.&lt;/p&gt;

&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;

&lt;p&gt;The songs&amp;rsquo; metadata sourse is a subset of the &lt;a href=&#34;https://labrosa.ee.columbia.edu/millionsong/&#34; target=&#34;_blank&#34;&gt;Million Song Dataset&lt;/a&gt;. Also, the users&amp;rsquo; activities is a simulated data using &lt;a href=&#34;https://github.com/Interana/eventsim&#34; target=&#34;_blank&#34;&gt;eventsim&lt;/a&gt;. The data resides in two main directories:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Songs metadata: collection of JSON files that describes the songs such as title, artist name, year, etc.&lt;/li&gt;
&lt;li&gt;Logs data: collection of JSON files where each file covers the users activities over a given day.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;methodology&#34;&gt;Methodology&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ll build the database by optimizing the tables around efficient reads for complex queries. To do that, &lt;strong&gt;Star&lt;/strong&gt; schema will be used utilizing dimensional modeling as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Fact table: songplays.&lt;/li&gt;
&lt;li&gt;Dimensions tables: songs, artist, users, time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The three most important advantages of using Star schema are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Denormalized tables.&lt;/li&gt;
&lt;li&gt;Simplified queries.&lt;/li&gt;
&lt;li&gt;Fast aggregation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-to&#34;&gt;HOW TO&lt;/h2&gt;

&lt;p&gt;The source code is available in three separate &lt;strong&gt;Python&lt;/strong&gt; scripts. Below is a brief description of the main files:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;sql_queries.py&lt;/code&gt; has all the queries needed to both create/drop tables for the database as well as a SQL query to get song_id and artist_id from other tables since they are not provided in logs dataset.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;create_tables.py&lt;/code&gt; creates the database, establish the connection and creates/drops all the tables required using sql_queries module.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;etl.py&lt;/code&gt; build the pipeline that extracts the data from JSON files, does some transformation (such as adding different time attributes from timestamp) and then insert all the data into the corresponding tables.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Therefore, we first run &lt;code&gt;create_tables.py&lt;/code&gt; then &lt;code&gt;etl.py&lt;/code&gt; to create the database, create tables, and then insert the data using the ETL pipeline.&lt;/p&gt;

&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%load_ext sql
%sql postgresql://student:student@127.0.0.1/sparkifydb
%%sql
SELECT COUNT(*) from songplays;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;postgresql://student:***@127.0.0.1/sparkifydb&lt;/p&gt;

&lt;p&gt;1 rows affected.&lt;/p&gt;

&lt;p&gt;Out[1]: count 6820&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;Check out the project on &lt;a href=&#34;https://github.com/ImadDabbura/data-modeling-with-postgres&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conda Essentials Notes</title>
      <link>https://imaddabbura.github.io/post/conda-essentials/</link>
      <pubDate>Mon, 18 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://imaddabbura.github.io/post/conda-essentials/</guid>
      <description>&lt;!-- 


  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/conda/conda.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/conda/conda.png&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;
 --&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Introduction&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Conda&lt;/strong&gt; in an open source package management system that works on all platforms. It is a tool that helps manage packages and environments for different programming languages. Develop a high level understanding of how Conda works helped me at so many levels especially when it comes to managing environments and make my work more reproducable. Below are the notes that I wrote down during my journey of learning Conda and I always refere back to them:&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
General&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Conda packages are files and executables that can in principle contain images, data, noteboeeks, files, etc.&lt;/li&gt;
&lt;li&gt;Conda mainly used in Python ecosystem; however, it can be used with other languages such R, Julia, Scala, etc.&lt;/li&gt;
&lt;li&gt;When installing a package using Conda, it installs its dependencies with it. Also, Conda is able to figure out the platform you&#39;re using without the need to specify the platform when installing packages.&lt;/li&gt;
&lt;li&gt;When installing a package, Conda:

&lt;ul&gt;
&lt;li&gt;Checks the platform.&lt;/li&gt;
&lt;li&gt;Checks the Python version.&lt;/li&gt;
&lt;li&gt;Install the latest version of the package that is compatible with Python.&lt;/li&gt;
&lt;li&gt;If it has dependencies, installs the latest versions of the dependencies that are also compatible with each other.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Under semantic versioning, software is labeled with a three-part version identifier of the form &lt;code&gt;MAJOR.MINOR.PATCH&lt;/code&gt;; the label components are non-negative integers separated by periods. Assuming all software starts at version 0.0.0, the &lt;code&gt;MAJOR&lt;/code&gt; version number is increased when significant new functionality is introduced (often with corresponding API changes). Increases in the &lt;code&gt;MINOR&lt;/code&gt; version number generally reflect improvements (e.g., new features) that avoid backward-incompatible API changes. For instance, adding an optional argument to a function API (in a way that allows old code to run unchanged) is a change worthy of increasing the &lt;code&gt;MINOR&lt;/code&gt; version number. An increment to the &lt;code&gt;PATCH&lt;/code&gt; version number is approriate mostly for bug fixes that preserve the same &lt;code&gt;MAJOR&lt;/code&gt; and MINOR revision numbers. Software patches do not typically introduce new features or change APIs at all (except sometimes to address security issues).&lt;/li&gt;
&lt;li&gt;We can specify &lt;code&gt;MAJOR&lt;/code&gt;, &lt;code&gt;MAJOR.MINOR&lt;/code&gt;, or &lt;code&gt;MAJOR.MINOR.PATCH&lt;/code&gt; when installing any package.&lt;/li&gt;
&lt;li&gt;We can use logical operators to install versions of a package. Examples:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;conda install &#39;python=3.6|3.7&#39;&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;conda install &#39;python=3.6|3.7*&#39;&lt;/code&gt; .&lt;/li&gt;
&lt;li&gt;&lt;code&gt;conda install &#39;python&amp;gt;=3.6, &amp;lt;=3.7&#39;&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Common Commands&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;To update a package, &lt;code&gt;conda update pckg&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;To uninstall a package, &lt;code&gt;conda remove pckg&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;To search what available versions of a specific package is available, use &lt;code&gt;conda search pckg&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;conda list&lt;/code&gt; will list all installed packages.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;conda list -n env-name&lt;/code&gt; will list all packages in the environment env-name.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;conda list pckg&lt;/code&gt; will give information about pckg.&lt;/li&gt;
&lt;li&gt;When installing a pckg without including a channel, it defaults to the main channel that is maintained by Anaconda Inc.&lt;/li&gt;
&lt;li&gt;There other channels where people can upload their packages to and we can reach to those channels when looking for installation such fastai. We use &lt;code&gt;conda install -c fastai fastai&lt;/code&gt;. Here the channel is fastai and the pckg is also fastai.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;conda search -c conda-forge -c fastai --override-channels --platform osx-64 fastai&lt;/code&gt; means:

&lt;ul&gt;
&lt;li&gt;Search for fastai in two channels: conda-forge, fastai.&lt;/li&gt;
&lt;li&gt;override-channels means do not go to default main channel.&lt;/li&gt;
&lt;li&gt;platform specify which platform.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Sometimes we don&#39;t know the channel of the pckg, we can use &lt;code&gt;anaconda search pckg&lt;/code&gt; that will return all the channels that the pckg is at and their versions.&lt;/li&gt;
&lt;li&gt;conda-forge is almost as good as the main channels which is led by the community. It has a lot more packages than the main channel.&lt;/li&gt;
&lt;li&gt;There is no system that rates channels, so be carefel when installing packages from any channel.&lt;/li&gt;
&lt;li&gt;We can list all packages in a channel such as &lt;code&gt;conda search -c conda-forge --override-channels&lt;/code&gt; that will list all packages for the conda-forge channel.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Environments&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Environments are a good practice of documenting data science/software development work.&lt;/li&gt;
&lt;li&gt;Environments are nothing more than a directory that contains all the packages so that when trying to import them, it imports them from this directory only. we can use &lt;code&gt;conda env list&lt;/code&gt; to see all the available environments on our machine.&lt;/li&gt;
&lt;li&gt;To get the packages from a specific environment by name, use &lt;code&gt;conda list -n env-name&lt;/code&gt;. Otherwise, we get the packages from the current environment.&lt;/li&gt;
&lt;li&gt;To activate an environment, use &lt;code&gt;conda activate env-name&lt;/code&gt;. To deactivate, &lt;code&gt;conda deactivate&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Environments usually don&#39;t take a lot of space.&lt;/li&gt;
&lt;li&gt;We can remove environments using &lt;code&gt;conda env remove -n env-name&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;To create an environment, use &lt;code&gt;conda create -n env-name&lt;/code&gt;. We can also add additional package names to install after creation such as &lt;code&gt;conda create -n env-name python=3.6* numpy&amp;gt;=1.1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;To export an environment, use &lt;code&gt;conda env export -n env-name&lt;/code&gt;. This will return the output to the terminal. We can also export to a file. For that use &lt;code&gt;conda env export -n env-name -f env-name.yml&lt;/code&gt;. The &#39;.yml&#39; extension is strongly enouraged. Doing this will assure that all the packages used can be installed by others exactly.&lt;/li&gt;
&lt;li&gt;We can create also an environment from .yml file using &lt;code&gt;conda env create -f env-name.yml&lt;/code&gt;. Note also that if we only use &lt;code&gt;conda env create&lt;/code&gt;, it will look for a file that has .yml extension and has the same name as env-name in the current local directory. Moreover, we can create the .yml file with doing the export ourselves and only specify what is important in our environments.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>K-means Clustering - Algorithm, Applications, Evaluation Methods, and Drawbacks</title>
      <link>https://imaddabbura.github.io/post/kmeans-clustering/</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://imaddabbura.github.io/post/kmeans-clustering/</guid>
      <description>&lt;!-- 


  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/kmeans/kmeans.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/kmeans/kmeans.png&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;
 --&gt;

&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kmeans&#34;&gt;Kmeans Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#application&#34;&gt;Applications&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#2dexample&#34;&gt;Geyser&#39;s Eruptions Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#imagecompression&#34;&gt;Image Compression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#evaluation&#34;&gt;Evaluation Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#drawbacks&#34;&gt;Drawbacks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#39;introduction&#39;&gt;&lt;/a&gt;
&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Clustering
&lt;/h2&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clustering&lt;/strong&gt; is one of the most common exploratory data analysis technique used to get an intuition about the structure of the data. It can be defined as the task of identifying subgroups in the data such that data points in the same subgroup (cluster) are very similar while data points in different clusters are very different. In other words, we try to find homogeneous subgroups within the data such that data points in each cluster are as similar as possible according to a similarity measure such as euclidean-based distance or correlation-based distance. The decision of which similarity measure to use is application-specific.&lt;/p&gt;

&lt;p&gt;Clustering analysis can be done on the basis of features where we try to find subgroups of samples based on features or on the basis of samples where we try to find subgroups of features based on samples. We&#39;ll cover here clustering based on features. Clustering is used in market segmentation; where we try to fin  d customers that are similar to each other whether in terms of behaviors or attributes, image segmentation/compression; where we try to group similar regions together, document clustering based on topics, etc.&lt;/p&gt;

&lt;p&gt;Unlike supervised learning, clustering is considered an unsupervised learning method since we don&#39;t have the ground truth to compare the output of the clustering algorithm to the true labels to evaluate its performance. We only want to try to investigate the structure of the data by grouping the data points into distinct subgroups.&lt;/p&gt;

&lt;p&gt;In this post, we will cover only &lt;strong&gt;Kmeans&lt;/strong&gt; which is considered as one of the most commonly used clustering algorithms due to its simplicity.&lt;/p&gt;

&lt;p&gt;&lt;a id=&#39;kmeans&#39;&gt;&lt;/a&gt;
&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Kmeans Algorithm
&lt;/h2&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kmeans&lt;/strong&gt; algorithm is an iterative algorithm that tries to partition the dataset into &lt;em&gt;K&lt;/em&gt; pre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to &lt;strong&gt;only one group&lt;/strong&gt;. It tries to make the inter-cluster data points as similar as possible while also keeping
the clusters as different (far) as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the cluster&#39;s centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster.&lt;/p&gt;

&lt;p&gt;The way kmeans algorithm works is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Specify number of clusters &lt;em&gt;K&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Initialize centroids by first shuffling the dataset and then randomly selecting &lt;em&gt;K&lt;/em&gt; data points for the centroids without replacement.&lt;/li&gt;
&lt;li&gt;Keep iterating until there is no change to the centroids. i.e assignment of data points to clusters isn&#39;t changing.

&lt;ul&gt;
&lt;li&gt;Compute the sum of the squared distance between data points and all centroids.&lt;/li&gt;
&lt;li&gt;Assign each data point to the closest cluster (centroid).&lt;/li&gt;
&lt;li&gt;Compute the centroids for the clusters by taking the average of all data points that belong to each cluster.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The approach kmeans follows to solve the problem is called &lt;strong&gt;Expectation-Maximization&lt;/strong&gt;. The E-step is assigning the data points to the closest cluster. The M-step is computing the centroid of each cluster. Below is a break down of how we can solve it mathematically (feel free to skip it).&lt;/p&gt;

&lt;p&gt;The objective function is:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[J = \sum_{i = 1}^{m}\sum_{k = 1}^{K}w_{ik}\|x^i - \mu_k\|^2\tag{1}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where $w_{ik} = 1$ for data point $x^i$ if it belongs to cluster &lt;em&gt;k&lt;/em&gt;; otherwise, $w_{ik} = 0$. Also, $\mu_k$ is the centroid of $x^i$&#39;s cluster.&lt;/p&gt;

&lt;p&gt;It&#39;s a minimization problem of two parts. We first minimize J w.r.t. $w_{ik}$ and treat $\mu_k$ fixed. Then we minimize J w.r.t. $\mu&lt;em&gt;k$ and treat $w&lt;/em&gt;{ik}$ fixed. Technically speaking, we differentiate J w.r.t. $w_{ik}$ first and update cluster assignments  (&lt;em&gt;E-step&lt;/em&gt;). Then we differentiate J w.r.t. $\mu_{k}$ and recompute the centroids after the cluster assignments from previous step  (&lt;em&gt;M-step&lt;/em&gt;). Therefore, E-step is:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(\frac{\partial J}{\partial w_{ik}} = \sum_{i = 1}^{m}\sum_{k = 1}^{K}\|x^i - \mu_k\|^2\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(\Rightarrow \begin{equation} w_{ik} = \begin{cases} 1 &amp; \text{if $k = arg min_j\ \|x^i - \mu_j\|^2$} \\\\\\
0 &amp; \text{otherwise}. \end{cases} \end{equation}\tag{2}\)&lt;/span&gt;
In other words, assign the data point $x^i$ to the closest cluster judged by its sum of squared distance from cluster&#39;s centroid.&lt;/p&gt;

&lt;p&gt;And M-step is:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(\ \frac{\partial J}{\partial \mu_k} = 2\sum_{i = 1}^{m}w_{ik}(x^i - \mu_k) = 0\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(\Rightarrow \mu_k = \frac{\sum_{i = 1}^{m}w_{ik}x^i}{\sum_{i = 1}^{m}w_{ik}}\tag{3}\)&lt;/span&gt;
Which translates to recomputing the centroid of each cluster to reflect the new assignments.&lt;/p&gt;

&lt;p&gt;Few things to note here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Since clustering algorithms including kmeans use distance-based measurements to determine the similarity between data points, it&#39;s recommended to standardize the data to have a mean of zero and a standard deviation of one since almost always the features in any dataset would have different units of measurements such as age vs income.&lt;/li&gt;
&lt;li&gt;Given kmeans iterative nature and the random initialization of centroids at the start of the algorithm, different initializations may lead to different clusters since kmeans algorithm may &lt;em&gt;stuck in a local optimum and may not converge to global optimum&lt;/em&gt;. Therefore, it&#39;s recommended to run the algorithm using different initializations of centroids and pick the results of the run that that yielded the lower sum of squared distance.&lt;/li&gt;
&lt;li&gt;Assignment of examples isn&#39;t changing is the same thing as no change in within-cluster variation:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{1}{m_k}\sum_{i = 1}^{m_k}\|x^i - \mu_{c^k}\|^2\tag{4}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&#39;implementation&#39;&gt;&lt;/a&gt;
&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Implementation
&lt;/h2&gt;&lt;/p&gt;

&lt;p&gt;We&#39;ll use simple implementation of kmeans here to just illustrate some concepts. Then we will use &lt;code&gt;sklearn&lt;/code&gt; implementation that is more efficient take care of many things for us.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
from numpy.linalg import norm


class Kmeans:
    &#39;&#39;&#39;Implementing Kmeans algorithm.&#39;&#39;&#39;

    def __init__(self, n_clusters, max_iter=100, random_state=123):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.random_state = random_state

    def initializ_centroids(self, X):
        np.random.RandomState(self.random_state)
        random_idx = np.random.permutation(X.shape[0])
        centroids = X[random_idx[:self.n_clusters]]
        return centroids

    def compute_centroids(self, X, labels):
        centroids = np.zeros((self.n_clusters, X.shape[1]))
        for k in range(self.n_clusters):
            centroids[k, :] = np.mean(X[labels == k, :], axis=0)
        return centroids

    def compute_distance(self, X, centroids):
        distance = np.zeros((X.shape[0], self.n_clusters))
        for k in range(self.n_clusters):
            row_norm = norm(X - centroids[k, :], axis=1)
            distance[:, k] = np.square(row_norm)
        return distance

    def find_closest_cluster(self, distance):
        return np.argmin(distance, axis=1)

    def compute_sse(self, X, labels, centroids):
        distance = np.zeros(X.shape[0])
        for k in range(self.n_clusters):
            distance[labels == k] = norm(X[labels == k] - centroids[k], axis=1)
        return np.sum(np.square(distance))
    
    def fit(self, X):
        self.centroids = self.initializ_centroids(X)
        for i in range(self.max_iter):
            old_centroids = self.centroids
            distance = self.compute_distance(X, old_centroids)
            self.labels = self.find_closest_cluster(distance)
            self.centroids = self.compute_centroids(X, self.labels)
            if np.all(old_centroids == self.centroids):
                break
        self.error = self.compute_sse(X, self.labels, self.centroids)
    
    def predict(self, X):
        distance = self.compute_distance(X, self.centroids)
        return self.find_closest_cluster(distance)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#39;application&#39;&gt;&lt;/a&gt;
&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Applications
&lt;/h2&gt;&lt;/p&gt;

&lt;p&gt;kmeans algorithm is very popular and used in a variety of applications such as market segmentation, document clustering, image segmentation and image compression, etc. The goal usually when we undergo a cluster analysis is either:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Get a meaningful intuition of the structure of the data we&#39;re dealing with.&lt;/li&gt;
&lt;li&gt;Cluster-then-predict where different models will be built for different subgroups if we believe there is a wide variation in the behaviors of different subgroups. An example of that is clustering patients into different subgroups and build a model for each subgroup to predict the probability of the risk of having heart attack.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this post, we&#39;ll apply clustering on two cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Geyser eruptions segmentation (2-D dataset).&lt;/li&gt;
&lt;li&gt;Image compression.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#39;2dexample&#39;&gt;&lt;/a&gt;
&lt;h2 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Kmeans on Geyser&#39;s Eruptions Segmentation
&lt;/h2&gt;&lt;/p&gt;

&lt;p&gt;We&#39;ll first implement the kmeans algorithm on 2D dataset and see how it works. The dataset has 272 observations and 2 features. The data covers the waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. We will try to find &lt;em&gt;K&lt;/em&gt; subgroups within the data points and group them accordingly. Below is the description of the features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;eruptions (float): Eruption time in minutes.&lt;/li&gt;
&lt;li&gt;waiting (int):  Waiting time to next eruption.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&#39;s plot the data first:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Modules
import matplotlib.pyplot as plt
from matplotlib.image import imread
import pandas as pd
import seaborn as sns
from sklearn.datasets.samples_generator import (make_blobs,
                                                make_circles,
                                                make_moons)
from sklearn.cluster import KMeans, SpectralClustering
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_samples, silhouette_score

%matplotlib inline
sns.set_context(&#39;notebook&#39;)
plt.style.use(&#39;fivethirtyeight&#39;)
from warnings import filterwarnings
filterwarnings(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import the data
df = pd.read_csv(&#39;../data/old_faithful.csv&#39;)

# Plot the data
plt.figure(figsize=(6, 6))
plt.scatter(df.iloc[:, 0], df.iloc[:, 1])
plt.xlabel(&#39;Eruption time in mins&#39;)
plt.ylabel(&#39;Waiting time to next eruption&#39;)
plt.title(&#39;Visualization of raw data&#39;);
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/kmeans/geyser_raw.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/kmeans/geyser_raw.png&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;We&#39;ll use this data because it&#39;s easy to plot and visually spot the clusters since its a 2-dimension dataset. It&#39;s obvious that we have 2 clusters. Let&#39;s standardize the data first and run the kmeans algorithm on the standardized data with $K = 2$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Standardize the data
X_std = StandardScaler().fit_transform(df)

# Run local implementation of kmeans
km = Kmeans(n_clusters=2, max_iter=100)
km.fit(X_std)
centroids = km.centroids

# Plot the clustered data
fig, ax = plt.subplots(figsize=(6, 6))
plt.scatter(X_std[km.labels == 0, 0], X_std[km.labels == 0, 1],
            c=&#39;green&#39;, label=&#39;cluster 1&#39;)
plt.scatter(X_std[km.labels == 1, 0], X_std[km.labels == 1, 1],
            c=&#39;blue&#39;, label=&#39;cluster 2&#39;)
plt.scatter(centroids[:, 0], centroids[:, 1], marker=&#39;*&#39;, s=300,
            c=&#39;r&#39;, label=&#39;centroid&#39;)
plt.legend()
plt.xlim([-2, 2])
plt.ylim([-2, 2])
plt.xlabel(&#39;Eruption time in mins&#39;)
plt.ylabel(&#39;Waiting time to next eruption&#39;)
plt.title(&#39;Visualization of clustered data&#39;, fontweight=&#39;bold&#39;)
ax.set_aspect(&#39;equal&#39;);
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/kmeans/geyser_clustered.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/kmeans/geyser_clustered.png&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;The above graph shows the scatter plot of the data colored by the cluster they belong to. In this example, we chose $K = 2$. The symbol &lt;strong&gt;&#39;*&#39;&lt;/strong&gt; is the centroid of each cluster. We can think of those 2 clusters as geyser had different kinds of behaviors under different scenarios.&lt;/p&gt;

&lt;p&gt;Next, we&#39;ll show that different initializations of centroids may yield to different results. I&#39;ll use 9 different &lt;code&gt;random_state&lt;/code&gt; to change the initialization of the centroids and plot the results. The title of each plot will be the sum of squared distance of each initialization.&lt;/p&gt;

&lt;p&gt;As a side note, this dataset is considered very easy and converges in less than 10 iterations. Therefore, to see the effect of random initialization on convergence, I am going to go with 3 iterations to illustrate the concept. However, in real world applications, datasets are not at all that clean and nice!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;n_iter = 9
fig, ax = plt.subplots(3, 3, figsize=(16, 16))
ax = np.ravel(ax)
centers = []
for i in range(n_iter):
    # Run local implementation of kmeans
    km = Kmeans(n_clusters=2,
                max_iter=3,
                random_state=np.random.randint(0, 1000, size=1))
    km.fit(X_std)
    centroids = km.centroids
    centers.append(centroids)
    ax[i].scatter(X_std[km.labels == 0, 0], X_std[km.labels == 0, 1],
                  c=&#39;green&#39;, label=&#39;cluster 1&#39;)
    ax[i].scatter(X_std[km.labels == 1, 0], X_std[km.labels == 1, 1],
                  c=&#39;blue&#39;, label=&#39;cluster 2&#39;)
    ax[i].scatter(centroids[:, 0], centroids[:, 1],
                  c=&#39;r&#39;, marker=&#39;*&#39;, s=300, label=&#39;centroid&#39;)
    ax[i].set_xlim([-2, 2])
    ax[i].set_ylim([-2, 2])
    ax[i].legend(loc=&#39;lower right&#39;)
    ax[i].set_title(f&#39;{km.error:.4f}&#39;)
    ax[i].set_aspect(&#39;equal&#39;)
plt.tight_layout();
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/kmeans/geyser_multiple.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/kmeans/geyser_multiple.png&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;As the graph above shows that we only ended up with two different ways of clusterings based on different initializations. We would pick the one with the lowest sum of squared distance.&lt;/p&gt;

&lt;p&gt;&lt;a id=&#39;imagecompression&#39;&gt;&lt;/a&gt;
&lt;h2 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Kmeans on Image Compression
&lt;/h2&gt;&lt;/p&gt;

&lt;p&gt;In this part, we&#39;ll implement kmeans to compress an image. The image that we&#39;ll be working on is 396 x 396 x 3. Therefore, for each pixel location we would have 3 8-bit integers that specify the red, green, and blue intensity values. Our goal is to reduce the number of colors to 30 and represent (compress) the photo using those 30 colors only. To pick which colors to use, we&#39;ll use kmeans algorithm on the image and treat every pixel as a data point. That means reshape the image from height x width x channels to (height * width) x channel, i,e we would have 396 x 396 = 156,816 data points in 3-dimensional space which are the intensity of RGB. Doing so will allow us to represent the image using the 30 centroids for each pixel and would significantly reduce the size of the image by a factor of 6. The original image size was 396 x 396 x 24 = 3,763,584 bits; however, the new compressed image would be 30 x 24 + 396 x 396 x 4 = 627,984 bits. The huge difference comes from the fact that we&#39;ll be using centroids as a lookup for pixels&#39; colors and that would reduce the size of each pixel location to 4-bit instead of 8-bit.&lt;/p&gt;

&lt;p&gt;From now on we will be using &lt;code&gt;sklearn&lt;/code&gt; implementation of kmeans. Few thing to note here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;n_init&lt;/code&gt; is the number of times of running the kmeans with different centroid&#39;s initialization. The result of the best one will be reported.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tol&lt;/code&gt; is the within-cluster variation metric used to declare convergence.&lt;/li&gt;
&lt;li&gt;The default of &lt;code&gt;init&lt;/code&gt; is &lt;strong&gt;k-means++&lt;/strong&gt; which is supposed to yield a better results than just random initialization of centroids.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Read the image
img = imread(&#39;images/my_image.jpg&#39;)
img_size = img.shape

# Reshape it to be 2-dimension
X = img.reshape(img_size[0] * img_size[1], img_size[2])

# Run the Kmeans algorithm
km = KMeans(n_clusters=30)
km.fit(X)

# Use the centroids to compress the image
X_compressed = km.cluster_centers_[km.labels_]
X_compressed = np.clip(X_compressed.astype(&#39;uint8&#39;), 0, 255)

# Reshape X_recovered to have the same dimension as the original image 128 * 128 * 3
X_compressed = X_compressed.reshape(img_size[0], img_size[1], img_size[2])

# Plot the original and the compressed image next to each other
fig, ax = plt.subplots(1, 2, figsize = (12, 8))
ax[0].imshow(img)
ax[0].set_title(&#39;Original Image&#39;)
ax[1].imshow(X_compressed)
ax[1].set_title(&#39;Compressed Image with 30 colors&#39;)
for ax in fig.axes:
    ax.axis(&#39;off&#39;)
plt.tight_layout();
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/kmeans/image_compression.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/kmeans/image_compression.png&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;We can see the comparison between the original image and the compressed one. The compressed image looks close to the original one which means we&#39;re able to retain the majority of the characteristics of the original image. With smaller number of clusters we would have higher compression rate at the expense of image quality. As a side note, this image compression method is called &lt;em&gt;lossy data compression&lt;/em&gt; because we can&#39;t reconstruct the original image from the compressed image.&lt;/p&gt;

&lt;p&gt;&lt;a id=&#39;evaluation&#39;&gt;&lt;/a&gt;
&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Evaluation Methods
&lt;/h2&gt;&lt;/p&gt;

&lt;p&gt;Contrary to supervised learning where we have the ground truth to evaluate the model&#39;s performance, clustering analysis doesn&#39;t have a solid evaluation metric that we can use to evaluate the outcome of different clustering algorithms. Moreover, since kmeans requires &lt;em&gt;k&lt;/em&gt; as an input and doesn&#39;t learn it from data, there is no right answer in terms of the number of clusters that we should have in any problem. Sometimes domain knowledge and intuition may help but usually that is not the case. In the cluster-predict methodology, we can evaluate how well the models are performing based on different &lt;em&gt;K&lt;/em&gt; clusters since clusters are used in the downstream modeling.&lt;/p&gt;

&lt;p&gt;In this post we&#39;ll cover two metrics that may give us some intuition about &lt;em&gt;k&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Elbow method&lt;/li&gt;
&lt;li&gt;Silhouette analysis&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a id=&#39;elbow&#39;&gt;&lt;/a&gt;
&lt;h2 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Elbow Method
&lt;/h2&gt;
&lt;strong&gt;Elbow&lt;/strong&gt; method gives us an idea on what a good &lt;em&gt;k&lt;/em&gt; number of clusters would be based on the sum of squared distance (SSE) between data points and their assigned clusters&#39; centroids. We pick &lt;em&gt;k&lt;/em&gt; at the spot where SSE starts to flatten out and forming an elbow. We&#39;ll use the geyser dataset and evaluate SSE for different values of &lt;em&gt;k&lt;/em&gt; and see where the curve might form an elbow and flatten out.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Run the Kmeans algorithm and get the index of data points clusters
sse = []
list_k = list(range(1, 10))

for k in list_k:
    km = KMeans(n_clusters=k)
    km.fit(X_std)
    sse.append(km.inertia_)

# Plot sse against k
plt.figure(figsize=(6, 6))
plt.plot(list_k, sse, &#39;-o&#39;)
plt.xlabel(r&#39;Number of clusters *k*&#39;)
plt.ylabel(&#39;Sum of squared distance&#39;);
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/kmeans/elbow.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/kmeans/elbow.png&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;The graph above shows that $k = 2$ is not a bad choice. Sometimes it&#39;s still hard to figure out a good number of clusters to use because the curve is monotonically decreasing and may not show any elbow or has an obvious point where the curve starts flattening out.&lt;/p&gt;

&lt;p&gt;&lt;a id=&#39;silhouette&#39;&gt;&lt;/a&gt;
&lt;h2 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Silhouette Analysis
&lt;/h2&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Silhouette analysis&lt;/strong&gt; can be used to determine the degree of separation between clusters. For each sample:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Compute the average distance from all data points in the same cluster ($a^i$).&lt;/li&gt;
&lt;li&gt;Compute the average distance from all data points in the closest cluster ($b^i$).&lt;/li&gt;
&lt;li&gt;Compute the coefficient:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{b^i - a^i}{max(a^i, b^i)}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The coefficient can take values in the interval .&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If it is 0 --&amp;gt; the sample is very close to the neighboring clusters.&lt;/li&gt;
&lt;li&gt;If it is 1 --&amp;gt; the sample is far away from the neighboring clusters.&lt;/li&gt;
&lt;li&gt;If it is -1 --&amp;gt; the sample is assigned to the wrong clusters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, we want the coefficients to be as big as possible and close to 1 to have a good clusters. We&#39;ll use here geyser dataset again because its cheaper to run the silhouette analysis and it is actually obvious that there is most likely only two groups of data points.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i, k in enumerate([2, 3, 4]):
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)
    
    # Run the Kmeans algorithm
    km = KMeans(n_clusters=k)
    labels = km.fit_predict(X_std)
    centroids = km.cluster_centers_

    # Get silhouette samples
    silhouette_vals = silhouette_samples(X_std, labels)

    # Silhouette plot
    y_ticks = []
    y_lower, y_upper = 0, 0
    for i, cluster in enumerate(np.unique(labels)):
        cluster_silhouette_vals = silhouette_vals[labels == cluster]
        cluster_silhouette_vals.sort()
        y_upper += len(cluster_silhouette_vals)
        ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor=&#39;none&#39;, height=1)
        ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))
        y_lower += len(cluster_silhouette_vals)

    # Get the average silhouette score and plot it
    avg_score = np.mean(silhouette_vals)
    ax1.axvline(avg_score, linestyle=&#39;--&#39;, linewidth=2, color=&#39;green&#39;)
    ax1.set_yticks([])
    ax1.set_xlim([-0.1, 1])
    ax1.set_xlabel(&#39;Silhouette coefficient values&#39;)
    ax1.set_ylabel(&#39;Cluster labels&#39;)
    ax1.set_title(&#39;Silhouette plot for the various clusters&#39;, y=1.02);
    
    # Scatter plot of data colored with labels
    ax2.scatter(X_std[:, 0], X_std[:, 1], c=labels)
    ax2.scatter(centroids[:, 0], centroids[:, 1], marker=&#39;*&#39;, c=&#39;r&#39;, s=250)
    ax2.set_xlim([-2, 2])
    ax2.set_xlim([-2, 2])
    ax2.set_xlabel(&#39;Eruption time in mins&#39;)
    ax2.set_ylabel(&#39;Waiting time to next eruption&#39;)
    ax2.set_title(&#39;Visualization of clustered data&#39;, y=1.02)
    ax2.set_aspect(&#39;equal&#39;)
    plt.tight_layout()
    plt.suptitle(f&#39;Silhouette analysis using k = {k}&#39;,
                 fontsize=16, fontweight=&#39;semibold&#39;, y=1.05);
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/kmeans/silhouette_2.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/kmeans/silhouette_2.png&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;





  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/kmeans/silhouette_3.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/kmeans/silhouette_3.png&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;





  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/kmeans/silhouette_4.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/kmeans/silhouette_4.png&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;As the above plots show, &lt;code&gt;n_clusters=2&lt;/code&gt; has the best average silhouette score of around 0.75 and all clusters being above the average shows that it is actually a good choice. Also, the thickness of the silhouette plot gives an indication of how big each cluster is. The plot shows that cluster 1 has almost double the samples than cluster 2. However, as we increased &lt;code&gt;n_clusters&lt;/code&gt; to 3 and 4, the average silhouette score decreased dramatically to around 0.48 and 0.39 respectively. Moreover, the thickness of silhouette plot started showing wide fluctuations. The bottom line is: Good &lt;code&gt;n_clusters&lt;/code&gt; will have a well above 0.5 silhouette average score as well as all of the clusters have higher than the average score.&lt;/p&gt;

&lt;p&gt;&lt;a id=&#39;drawbacks&#39;&gt;&lt;/a&gt;
&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Drawbacks
&lt;/h2&gt;&lt;/p&gt;

&lt;p&gt;Kmeans algorithm is good in capturing structure of the data if clusters have a spherical-like shape. It always try to construct a nice spherical shape around the centroid. That means, the minute the clusters have a complicated geometric shapes, kmeans does a poor job in clustering the data. We&#39;ll illustrate three cases where kmeans will not perform well.&lt;/p&gt;

&lt;p&gt;First, kmeans algorithm doesn&#39;t let data points that are far-away from each other share the same cluster even though they obviously belong to the same cluster. Below is an example of data points on two different horizontal lines that illustrates how kmeans tries to group half of the data points of each horizontal lines together.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create horizantal data
X = np.tile(np.linspace(1, 5, 20), 2)
y = np.repeat(np.array([2, 4]), 20)
df = np.c_[X, y]

km = KMeans(n_clusters=2)
km.fit(df)
labels = km.predict(df)
centroids = km.cluster_centers_

fig, ax = plt.subplots(figsize=(6, 6))
plt.scatter(X, y, c=labels)
plt.xlim([0, 6])
plt.ylim([0, 6])
plt.text(5.1, 4, &#39;A&#39;, color=&#39;red&#39;)
plt.text(5.1, 2, &#39;B&#39;, color=&#39;red&#39;)
plt.text(2.8, 4.1, &#39;C&#39;, color=&#39;red&#39;)
ax.set_aspect(&#39;equal&#39;)
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/kmeans/horizontal_cluster.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/kmeans/horizontal_cluster.png&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;Kmeans considers the point &#39;B&#39; closer to point &#39;A&#39; than point &#39;C&#39; since they have non-spherical shape. Therefore, points &#39;A&#39; and &#39;B&#39; will be in the same cluster but point &#39;C&#39; will be in a different cluster. Note the &lt;strong&gt;Single Linkage&lt;/strong&gt; hierarchical clustering method gets this right because it doesn&#39;t separate similar points).&lt;/p&gt;

&lt;p&gt;Second, we&#39;ll generate data from multivariate normal distributions with different means and standard deviations. So we would have 3 groups of data where each group was generated from different multivariate normal distribution (different mean/standard deviation). One group will have a lot more data points than the other two combined. Next, we&#39;ll run kmeans on the data with $K = 3$ and see if it will be able to cluster the data correctly. To make the comparison easier, I am going to plot first the data colored based on the distribution it came from. Then I will plot the same data but now colored based on the clusters they have been assigned to.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create data from three different multivariate distributions
X_1 = np.random.multivariate_normal(mean=[4, 0], cov=[[1, 0], [0, 1]], size=75)
X_2 = np.random.multivariate_normal(mean=[6, 6], cov=[[2, 0], [0, 2]], size=250)
X_3 = np.random.multivariate_normal(mean=[1, 5], cov=[[1, 0], [0, 2]], size=20)
df = np.concatenate([X_1, X_2, X_3])

# Run kmeans
km = KMeans(n_clusters=3)
km.fit(df)
labels = km.predict(df)
centroids = km.cluster_centers_

# Plot the data
fig, ax = plt.subplots(1, 2, figsize=(10, 10))
ax[0].scatter(X_1[:, 0], X_1[:, 1])
ax[0].scatter(X_2[:, 0], X_2[:, 1])
ax[0].scatter(X_3[:, 0], X_3[:, 1])
ax[0].set_aspect(&#39;equal&#39;)
ax[1].scatter(df[:, 0], df[:, 1], c=labels)
ax[1].scatter(centroids[:, 0], centroids[:, 1], marker=&#39;o&#39;,
                c=&amp;quot;white&amp;quot;, alpha=1, s=200, edgecolor=&#39;k&#39;)
for i, c in enumerate(centroids):
    ax[1].scatter(c[0], c[1], marker=&#39;$%d$&#39; % i, s=50, alpha=1, edgecolor=&#39;r&#39;)
ax[1].set_aspect(&#39;equal&#39;)
plt.tight_layout()
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/kmeans/random_data.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/kmeans/random_data.png&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;Looks like kmeans couldn&#39;t figure out the clusters correctly. Since it tries to minimize the within-cluster variation, it gives more weight to bigger clusters than smaller ones. In other words, data points in smaller clusters may be left away from the centroid in order to focus more on the larger cluster.&lt;/p&gt;

&lt;p&gt;Last, we&#39;ll generate data that have complicated geometric shapes such as moons and circles within each other and test kmeans on both of the datasets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Cricles
X1 = make_circles(factor=0.5, noise=0.05, n_samples=1500)

# Moons
X2 = make_moons(n_samples=1500, noise=0.05)

fig, ax = plt.subplots(1, 2)
for i, X in enumerate([X1, X2]):
    fig.set_size_inches(18, 7)
    km = KMeans(n_clusters=2)
    km.fit(X[0])
    labels = km.predict(X[0])
    centroids = km.cluster_centers_

    ax[i].scatter(X[0][:, 0], X[0][:, 1], c=labels)
    ax[i].scatter(centroids[0, 0], centroids[0, 1], marker=&#39;*&#39;, s=400, c=&#39;r&#39;)
    ax[i].scatter(centroids[1, 0], centroids[1, 1], marker=&#39;+&#39;, s=300, c=&#39;green&#39;)
plt.suptitle(&#39;Simulated data&#39;, y=1.05, fontsize=22, fontweight=&#39;semibold&#39;)
plt.tight_layout()
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/kmeans/circles_wrong.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/kmeans/circles_wrong.png&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;As expected, kmeans couldn&#39;t figure out the correct clusters for both datasets. However, we can help kmeans perfectly cluster these kind of datasets if we use kernel methods. The idea is we transform to higher dimensional representation that make the data linearly separable (the same idea that we use in SVMs). Different kinds of algorithms work very well in such scenarios such as &lt;code&gt;SpectralClustering&lt;/code&gt;, see below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Cricles
X1 = make_circles(factor=0.5, noise=0.05, n_samples=1500)

# Moons
X2 = make_moons(n_samples=1500, noise=0.05)

fig, ax = plt.subplots(1, 2)
for i, X in enumerate([X1, X2]):
    fig.set_size_inches(18, 7)
    sp = SpectralClustering(n_clusters=2, affinity=&#39;nearest_neighbors&#39;)
    sp.fit(X[0])
    labels = sp.labels_
    ax[i].scatter(X[0][:, 0], X[0][:, 1], c=labels)
plt.suptitle(&#39;Simulated data&#39;, y=1.05, fontsize=22, fontweight=&#39;semibold&#39;)
plt.tight_layout()
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/kmeans/circles_right.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/kmeans/circles_right.png&#34; &gt;
&lt;/a&gt;

&lt;/figure&gt;


&lt;p&gt;&lt;a id=&#39;conclusion&#39;&gt;&lt;/a&gt;
&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Conclusion
&lt;/h2&gt;&lt;/p&gt;

&lt;p&gt;Kmeans clustering is one of the most popular clustering algorithms and usually the first thing practitioners apply when solving clustering tasks to get an idea of the structure of the dataset. The goal of kmeans is to group data points into distinct non-overlapping subgroups. It does a very good job when the clusters have a kind of spherical shapes. However, it suffers as the geometric shapes of clusters deviates from spherical shapes. Moreover, it also doesn&#39;t learn the number of clusters from the data and requires it to be pre-defined. To be a good practitioner, it&#39;s good to know the assumptions behind algorithms/methods so that you would have a pretty good idea about the strength and weakness of each method. This will help you decide when to use each method and under what circumstances. In this post, we covered both strength, weaknesses, and some evaluation methods related to kmeans.&lt;/p&gt;

&lt;p&gt;Below are the main takeaways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Scale/standardize the data when applying kmeans algorithm.&lt;/li&gt;
&lt;li&gt;Elbow method in selecting number of clusters doesn&#39;t usually work because the error function is monotonically decreasing for all $k$s.&lt;/li&gt;
&lt;li&gt;Kmeans gives more weight to the bigger clusters.&lt;/li&gt;
&lt;li&gt;Kmeans assumes spherical shapes of clusters (with radius equal to the distance between the centroid and the furthest data point) and doesn&#39;t work well when clusters are in different shapes such as elliptical clusters.&lt;/li&gt;
&lt;li&gt;If there is overlapping between clusters, kmeans doesn&#39;t have an intrinsic measure for uncertainty for the examples belong to the overlapping region in order to determine for which cluster to assign each data point.&lt;/li&gt;
&lt;li&gt;Kmeans may still cluster the data even if it can&#39;t be clustered such as data that comes from &lt;em&gt;uniform distributions&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The notebook that created this post can be found &lt;a href=&#34;https://github.com/ImadDabbura/blog-posts/blob/master/notebooks/Kmeans-Clustering.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coding Neural Network - Dropout</title>
      <link>https://imaddabbura.github.io/post/coding-nn-dropout/</link>
      <pubDate>Sun, 20 May 2018 00:00:00 +0000</pubDate>
      <guid>https://imaddabbura.github.io/post/coding-nn-dropout/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Dropout&lt;/strong&gt; is a regularization technique. On each iteration, we randomly shut down some neurons (units) on each layer and don&#39;t use those neurons in both forward propagation and back-propagation. Since the units that will be dropped out on each iteration will be random, the learning algorithm will have no idea which neurons will be shut down on every iteration; therefore, force the learning algorithm to spread out the weights and not focus on some specific feattures (units).
Moreover, dropout help improving generalization error by:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Since we drop some units on each iteration, this will lead to smaller network which in turns means simpler network (regularization).&lt;/li&gt;
&lt;li&gt;Can be seen as an approximation to bagging techniques. Each iteration can be viewed as different model since we&#39;re dropping randomly different units on each layer. This means that the error would be the average of errors from all different models (iterations). Therefore, averaging errors from different models especially if those errors are uncorrelated would reduce the overall errors. In the worst case where errors are perfectly correlated, averaging among all models won&#39;t help at all; however, we know that in practice errors have some degree of uncorrelation. As result, it will always improve generalization error.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can use different probabilities on each layer; however, the output layer would always have &lt;code&gt;keep_prob = 1&lt;/code&gt; and the input layer has high &lt;code&gt;keep_prob&lt;/code&gt; such as 0.9 or 1. If a hidden layer has &lt;code&gt;keep_prob = 0.8&lt;/code&gt;, this means that; on each iteration, each unit has 80% probablitity of being included and 20% probability of being dropped out.&lt;/p&gt;

&lt;p&gt;Dropout is used a lot in computer vision problems because we have a lot of features and not a lot of data. Also, features (pixels) next to each other usually don&#39;t add a lot of information. Therefore, models always suffer from overfitting.&lt;/p&gt;

&lt;p&gt;To illustrate how dropout helps us reduce generalization error, we&#39;ll use the same dataset we&#39;ve used in the previous posts. The dataset has images for cats and non-cat. We&#39;ll try to build a neural network to classify if the image has cat or not. Each image is 64 x 64 pixels on RGB scale. Let&#39;s import the data and take a look at the shape as well as a sample of a cat image from the training set.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Loading packages
import os

import h5py
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# local modules
os.chdir(&amp;quot;../scripts/&amp;quot;)
from coding_neural_network_from_scratch import (initialize_parameters,
                                                linear_activation_forward,
                                                compute_cost,
                                                linear_activation_backward,
                                                update_parameters,
                                                accuracy)

%matplotlib inline
sns.set_context(&amp;quot;notebook&amp;quot;)
plt.style.use(&amp;quot;fivethirtyeight&amp;quot;)
plt.rcParams[&#39;figure.figsize&#39;] = (12, 6)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import training data
train_dataset = h5py.File(&amp;quot;../data/train_catvnoncat.h5&amp;quot;)
X_train = np.array(train_dataset[&amp;quot;train_set_x&amp;quot;])
Y_train = np.array(train_dataset[&amp;quot;train_set_y&amp;quot;])

# Plot a sample image
plt.imshow(X_train[50])
plt.axis(&amp;quot;off&amp;quot;);

# Import test data
test_dataset = h5py.File(&amp;quot;../data/test_catvnoncat.h5&amp;quot;)
X_test = np.array(test_dataset[&amp;quot;test_set_x&amp;quot;])
Y_test = np.array(test_dataset[&amp;quot;test_set_y&amp;quot;])

# Transform data
X_train = X_train.reshape(209, -1).T
X_train = X_train / 255
Y_train = Y_train.reshape(-1, 209)

X_test = X_test.reshape(50, -1).T
X_test = X_test / 255
Y_test = Y_test.reshape(-1, 50)

# print the new shape of both training and test datasets
print(&amp;quot;Training data dimensions:&amp;quot;)
print(&amp;quot;X&#39;s dimension: {}, Y&#39;s dimension: {}&amp;quot;.format(X_train.shape, Y_train.shape))
print(&amp;quot;Test data dimensions:&amp;quot;)
print(&amp;quot;X&#39;s dimension: {}, Y&#39;s dimension: {}&amp;quot;.format(X_test.shape, Y_test.shape))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Training data dimensions:
X&#39;s dimension: (12288, 209), Y&#39;s dimension: (1, 209)
Test data dimensions:
X&#39;s dimension: (12288, 50), Y&#39;s dimension: (1, 50)
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/cat_sample.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/cat_sample.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Sample image.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Now, we&#39;ll write the functions needed to apply dropout on both forward propagation and back-propagation. Note that we&#39;ll utilize the functions we wrote in previous posts such as &lt;code&gt;initialize_parameters&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def drop_out_matrices(layers_dims, m, keep_prob):
    np.random.seed(1)
    D = {}
    L = len(layers_dims)

    for l in range(L):
        # initialize the random values for the dropout matrix
        D[str(l)] = np.random.rand(layers_dims[l], m)
        # Convert it to 0/1 to shut down neurons corresponding to each element
        D[str(l)] = D[str(l)] &amp;lt; keep_prob[l]
        assert(D[str(l)].shape == (layers_dims[l], m))
    return D


def L_model_forward(
        X, parameters, D, keep_prob, hidden_layers_activation_fn=&amp;quot;relu&amp;quot;):
    A = X                           # since input matrix A0
    A = np.multiply(A, D[str(0)])
    A /= keep_prob[0]
    caches = []                     # initialize the caches list
    L = len(parameters) // 2        # number of layer in the network

    for l in range(1, L):
        A_prev = A
        A, cache = linear_activation_forward(
            A_prev, parameters[&amp;quot;W&amp;quot; + str(l)], parameters[&amp;quot;b&amp;quot; + str(l)],
            hidden_layers_activation_fn)
        # shut down some units
        A = np.multiply(A, D[str(l)])
        # scale that value of units to keep expected value the same
        A /= keep_prob[l]
        caches.append(cache)

    AL, cache = linear_activation_forward(
        A, parameters[&amp;quot;W&amp;quot; + str(L)], parameters[&amp;quot;b&amp;quot; + str(L)], &amp;quot;sigmoid&amp;quot;)
    AL = np.multiply(AL, D[str(L)])
    AL /= keep_prob[L]
    caches.append(cache)
    assert(AL.shape == (1, X.shape[1]))

    return AL, caches


def L_model_backward(
        AL, Y, caches, D, keep_prob, hidden_layers_activation_fn=&amp;quot;relu&amp;quot;):
    Y = Y.reshape(AL.shape)
    L = len(caches)
    grads = {}

    # dA for output layer
    dAL = np.divide(AL - Y, np.multiply(AL, 1 - AL))
    dAL = np.multiply(dAL, D[str(L)])
    dAL /= keep_prob[L]

    grads[&amp;quot;dA&amp;quot; + str(L - 1)], grads[&amp;quot;dW&amp;quot; + str(L)], grads[
        &amp;quot;db&amp;quot; + str(L)] = linear_activation_backward(
            dAL, caches[L - 1], &amp;quot;sigmoid&amp;quot;)
    grads[&amp;quot;dA&amp;quot; + str(L - 1)] = np.multiply(
        grads[&amp;quot;dA&amp;quot; + str(L - 1)], D[str(L - 1)])
    grads[&amp;quot;dA&amp;quot; + str(L - 1)] /= keep_prob[L - 1]

    for l in range(L - 1, 0, -1):
        current_cache = caches[l - 1]
        grads[&amp;quot;dA&amp;quot; + str(l - 1)], grads[&amp;quot;dW&amp;quot; + str(l)], grads[
            &amp;quot;db&amp;quot; + str(l)] = linear_activation_backward(
                grads[&amp;quot;dA&amp;quot; + str(l)], current_cache,
                hidden_layers_activation_fn)

        grads[&amp;quot;dA&amp;quot; + str(l - 1)] = np.multiply(
            grads[&amp;quot;dA&amp;quot; + str(l - 1)], D[str(l - 1)])
        grads[&amp;quot;dA&amp;quot; + str(l - 1)] /= keep_prob[l - 1]

    return grads


def model_with_dropout(
        X, Y, layers_dims, keep_prob, learning_rate=0.01, num_iterations=3000,
        print_cost=True, hidden_layers_activation_fn=&amp;quot;relu&amp;quot;):
    # get number of examples
    m = X.shape[1]

    # to get consistents output
    np.random.seed(1)

    # initialize parameters
    parameters = initialize_parameters(layers_dims)

    # intialize cost list
    cost_list = []

    # implement gradient descent
    for i in range(num_iterations):
        # Initialize dropout matrices
        D = drop_out_matrices(layers_dims, m, keep_prob)

        # compute forward propagation
        AL, caches = L_model_forward(
            X, parameters, D, keep_prob, hidden_layers_activation_fn)

        # compute regularized cost
        cost = compute_cost(AL, Y)

        # compute gradients
        grads = L_model_backward(
            AL, Y, caches, D, keep_prob, hidden_layers_activation_fn)

        # update parameters
        parameters = update_parameters(parameters, grads, learning_rate)

        # print cost
        if (i + 1) % 100 == 0 and print_cost:
            print(f&amp;quot;The cost after {i + 1} iterations : {cost:.4f}.&amp;quot;)
        # append cost
        if i % 100 == 0:
            cost_list.append(cost)

    # plot the cost curve
    plt.plot(cost_list)
    plt.xlabel(&amp;quot;Iteration (per hundreds)&amp;quot;)
    plt.ylabel(&amp;quot;Cost&amp;quot;)
    plt.title(f&amp;quot;Cost curve for the learning rate = {learning_rate}&amp;quot;)

    return parameters
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we&#39;re ready to build our neural network. First, we&#39;ll build one fully connected network without dropout. That is to say, &lt;code&gt;keep_prob = 1&lt;/code&gt;. Next, we&#39;ll build another network where &lt;code&gt;keep_prob &amp;lt; 1&lt;/code&gt;. Lastly, we&#39;ll compare the generalization error of both networks and see how dropout technique can help us in improving our generalization error.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# setup layers dimensions, number of examples, and keep probabilities list
m = X_train.shape[0]
keep_prob = [1, 1, 1, 1]
layers_dims = [m, 10, 10, 1]

# train NN with no dropout
parameters = model_with_dropout(X_train, Y_train, layers_dims, keep_prob=keep_prob,
                                learning_rate=0.03, num_iterations=1000,
                                hidden_layers_activation_fn=&amp;quot;relu&amp;quot;)

# print the test accuracy
print(&amp;quot;The training accuracy rate: {}&amp;quot;.format(accuracy(X_train, parameters, Y_train, &amp;quot;relu&amp;quot;)[-7:]))
print(&amp;quot;The test accuracy rate: {}&amp;quot;.format(accuracy(X_test, parameters, Y_test, &amp;quot;relu&amp;quot;)[-7:]))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;The cost after 100 iterations : 0.6555.
The cost after 200 iterations : 0.6468.
The cost after 300 iterations : 0.6447.
The cost after 400 iterations : 0.6442.
The cost after 500 iterations : 0.6440.
The cost after 600 iterations : 0.6440.
The cost after 700 iterations : 0.6440.
The cost after 800 iterations : 0.6440.
The cost after 900 iterations : 0.6440.
The cost after 1000 iterations : 0.6440.
The training accuracy rate: 65.55%.
The test accuracy rate: 34.00%.
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/no_dropout.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/no_dropout.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Cost curve with no dropout.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# setup keep probabilities list
keep_prob = [1, 0.5, 0.5, 1]

# train NN with no dropout
parameters = model_with_dropout(X_train, Y_train, layers_dims, keep_prob=keep_prob,
                                learning_rate=0.03, num_iterations=1000,
                                hidden_layers_activation_fn=&amp;quot;relu&amp;quot;)

# print the test accuracy
print(&amp;quot;The training accuracy rate: {}&amp;quot;.format(accuracy(X_train, parameters, Y_train, &amp;quot;relu&amp;quot;)[-7:]))
print(&amp;quot;The test accuracy rate: {}&amp;quot;.format(accuracy(X_test, parameters, Y_test, &amp;quot;relu&amp;quot;)[-7:]))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;The cost after 100 iterations : 0.6555.
The cost after 200 iterations : 0.6467.
The cost after 300 iterations : 0.6445.
The cost after 400 iterations : 0.6437.
The cost after 500 iterations : 0.6412.
The cost after 600 iterations : 0.6338.
The cost after 700 iterations : 0.6108.
The cost after 800 iterations : 0.5367.
The cost after 900 iterations : 0.4322.
The cost after 1000 iterations : 0.3114.
The training accuracy rate: 74.16%.
The test accuracy rate: 44.00%.
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/dropout.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/dropout.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Cost curve with dropout.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;As the results above showed, the network with dropout improved on test accuracy rate by 30%. Note that this is just an illustrative example to show the effectiveness of the dropout technique. We chose an arbitrary probabilities in this example; however, we can tune the dropout probabilities on each layer to yield the best validation loss and accuracy.&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Conclusion
&lt;/h2&gt;

&lt;p&gt;Dropout is a very effective regularization technique that is used a lot in &lt;em&gt;Convolutional Neural Networks&lt;/em&gt;. Below are some takeaways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Set &lt;code&gt;keep_prob = 1&lt;/code&gt; when using gradient checking; otherwise, it won&#39;t work.&lt;/li&gt;
&lt;li&gt;Dropout is used only during training. Don&#39;t use it when testing/predicting new examples.&lt;/li&gt;
&lt;li&gt;The lowest the &lt;code&gt;keep_prob&lt;/code&gt; $\rightarrow$ the simpler the neural network. As &lt;code&gt;keep_prob&lt;/code&gt; decreases, the bias increases and the variance decreases. Therefore, layers with more neurons are expected to have lower &lt;code&gt;keep_prob&lt;/code&gt; to avoid overfitting.&lt;/li&gt;
&lt;li&gt;It&#39;s computationally a cheap way to improve generalization error and help resolve overfitting.&lt;/li&gt;
&lt;li&gt;One can tune &lt;code&gt;keep_prob&lt;/code&gt; to get the best results out of the task at hand.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The source code that created this post can be found &lt;a href=&#34;https://github.com/ImadDabbura/blog-posts/blob/master/notebooks/Coding-Neural-Network-Dropout.ipynb&#34;&gt;here&lt;/a&gt;.
The post is inspired by deeplearning.ai courses.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coding Neural Network - Regularization</title>
      <link>https://imaddabbura.github.io/post/coding-nn-regularization/</link>
      <pubDate>Tue, 08 May 2018 00:00:00 +0000</pubDate>
      <guid>https://imaddabbura.github.io/post/coding-nn-regularization/</guid>
      <description>&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Bias-Variance Trade-off
&lt;/h2&gt;

&lt;p&gt;Generalization (test) error is the most important metric in Machine/Deep Learning. It gives us an estimate on the performance of the model on unseen data. Test error is decomposed into 3 parts (see above figure): &lt;strong&gt;Variance, Squared-Bias, and Irreducible Error&lt;/strong&gt;. Models with high bias are not complex enough (too simple) for the data and tend to underfit. The simplest model is taking the average (mode) of target variable and assign it to all predictions. On the contrary, models with high variance overfit the training data by closely follow (mimick) the training data where the learning algorithm will follow the signal and the noise. Note that as the complexity (flexibility) of the model increases â the model will become less interpretable such as Neural Networks. Below is the bias-variance decomposition:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(MSE = E(y - \widehat{y})^2\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\( = E(y - f + f - \widehat{y})^2\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\( = E((y - f)^2 + 2(y - f)(f - \widehat{y}) + (f - \widehat{y})^2); \quad substitute\ y = f + \epsilon\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\( = E((\epsilon + f - f)^2 + 2(\epsilon + f - f)(f - \widehat{y}) + (f - \widehat{y})^2)\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\( = E(\epsilon)^2 + E(\epsilon)E(f - \widehat{y}) + E(f - \widehat{y})^2; \quad where\ E(\epsilon) = 0\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\( = E(\epsilon)^2 + E(f - \widehat{y})^2;\quad add\ and\ subtract\ E(\widehat{y})\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\( = E(\epsilon)^2 + E(f - E(\widehat{y}) + E(\widehat{y}) - \widehat{y})^2\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\( = E(\epsilon)^2 + E(f - E(\widehat{y}))^2 + E(\widehat{y} - E(\widehat{y}))^2\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(\Rightarrow MSE = var(\widehat{y}) + (Bias(\widehat{y}))^2 + var(\epsilon)\)&lt;/span&gt;
Where:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$var(\epsilon)$: Irreducible error that resulted from omitted features and unmeasured variation with each example.&lt;/li&gt;
&lt;li&gt;$Bias(\widehat{y})$: Error that is introduced by approximating a real-life problem with a simple model.&lt;/li&gt;
&lt;li&gt;$var(\widehat{y})$: amount by which $\widehat{y}$ would change if we estimated it using different data set.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, we can control only the variance and the bias of the $\widehat{y}$ &lt;strong&gt;BUT NOT&lt;/strong&gt; irreducible error. As a result, our job is to try to estimate the right level of complexity to achieve the lowest test error.&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Regularization
&lt;/h2&gt;

&lt;p&gt;Regularization adds stability to the learning algorithm by making it less sensitive to the training data and processes. Since we don&#39;t know and have no access to the true function that we can use to compare our estimated function with it, the best strategy would be to build a very complex model that fits the training data really well (overfitting) and regularize it so that it would have a good generalization (test) error. When using regularization, we try to reduce the generalization error and that may lead to increase the training error in the process which is okay because what we care about is how well the model generalizes. With regularization, we try to bring back the very complex model that suffers from overfitting to a good model by increasing bias and reducing variance. This builds on the assumption that complex model has large parameters and simple model has small parameters.&lt;/p&gt;

&lt;p&gt;Below are some methods used for regularization:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;L2 Parameter Regularization&lt;/strong&gt; It&#39;s also known as &lt;strong&gt;weight decay&lt;/strong&gt;. This method adds L2 norm penalty to the objective function to drive the weights towards the origin. Even though this method shrinks all weights by the same proportion towards zero; however, it will never make any weight to be exactly zero.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;L1 Parameter Regularization (Lasso)&lt;/strong&gt; It can be seen as a feature selection method because; in contrast to L2 regularization, some weights will be actually zero. It shrinks all weights by the same amount by adding L1 norm penalty to the objective function.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dropout&lt;/strong&gt; Dropout can be seen as an approximation to bagging techniques. On each iteration, we randomly shut down some neurons on each layer and don&#39;t use those neurons in both forward propagation and back-propagation. This will force the neural network to spread out weights and not focus on specific neurons because it will never know which neurons will show up on each iteration. Therefore, it can be seen as training different model on each iteration. Also, since we drop some neurons on each iteration, this will lead to smaller network which in turns means simpler network.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Augmentation&lt;/strong&gt; Add fake data by using the training examples and adding distortions to them such as rescaling and rotating the images in the case of image recognition. The idea here is that it&#39;s always better to train the model on more data to achieve better performance. Note that augmented examples don&#39;t add much information to the model as much as independent examples do but still it&#39;s a valid alternative when collecting more data is not feasible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Early Stopping&lt;/strong&gt; This method tries to optimize the cost function and regularize it so that it would have lower generalization error. The way it works is that on each iteration we record the validation error. If the validation error improves, we store a copy of the parameters and will continue until the optimization algorithm terminates. It&#39;s a good method if computational time and resources is an issue for us.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this post, we&#39;ll cover L2 parameter regularization.&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
L2 Parameter Regularization
&lt;/h2&gt;

&lt;p&gt;We normally don&#39;t regularize bias and regularize weights only. We can use hessian matrix and it&#39;s eigenvalues and eigenvectors to see the sensitivity of the weights to the weight decay. The weight $w_i$ will be rescaled using $\frac{\lambda_i}{\lambda_i + \alpha}$ where $\lambda_i$ (eigenvalue) measures the sensitivity of hessian matrix in that direction (eigenvector) and $\alpha$ is the regularized hyperparameter. Therefore,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If $\lambda_i &amp;gt;&amp;gt; \alpha$, the cost function is very sensitive in that direction and the corresponding weight reduces the cost significantly $\Rightarrow$ don&#39;t decay (shrink) much.&lt;/li&gt;
&lt;li&gt;If $\lambda_i &amp;lt;&amp;lt; \alpha$, the cost function is not sensitive in that direction and the corresponding weight doesn&#39;t reduce the cost significantly $\Rightarrow$ decay (shrink) away towards zero.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The objective function (binary cross-entropy) would then change from:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[J = -\frac{1}{m} \sum\limits_{i = 1}^{m}  \large(y^{(i)}\log(a^{[L](i)}) + (1-y^{(i)})\log(1- a^{[L](i)}) \large)\tag{1}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;To:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[J\_{regularized} = \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m}  \large(y^{(i)}\log(a^{[L](i)}) + (1-y^{(i)})\log(1- a^{[L](i)}) \large)}\_\text{cross-entropy cost}  + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_{l=1}^L \sum\limits_{i=1}^{n^l} \sum\limits_{j=1}^{n^{l-1}} W_{j,i}^{[l]2} }_\text{L2 regularization cost} \tag{2}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Also, the new gradients and the update equation would be:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(\nabla_w J_{regularized} = \nabla_w J + \frac{\lambda}{m}w\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(w = w - \alpha\nabla_w J - \alpha\frac{\lambda}{m}w\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(\Rightarrow w = w\underbrace{(1 - \alpha\frac{\lambda}{m})}_\text{weight decay} - \nabla J\)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Note that here $\alpha$ is the learning rate and $\lambda$ is the regularized hyperparameter. As $\lambda$ increases, the bias increases (and the model becomes less flexible) with the following extreme cases (see figure 2):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\lambda = 0$, no regularization.&lt;/li&gt;
&lt;li&gt;$\lambda \rightarrow \infty$, model becomes very simple where all weights are essentially zero. In the case of regression, we would end-up with the intercept only which is equal to the average of the target variable.&lt;/li&gt;
&lt;/ul&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/bias_variance_lambda.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/bias_variance_lambda.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Model complexity (underfitting/overfitting) as a function of regularization parameter $\lambda$.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;It sometimes maybe helpful to see how L2 parameter regularization works using normal equation. The normal quation is:
&lt;span  class=&#34;math&#34;&gt;\(W = (X^TX + \lambda I)^{-1}X^TY\tag{3}\)&lt;/span&gt;
This means that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Adding $\lambda$ to the variance would decrease the weight since $w_i = \frac{cov_{x, y}}{\sigma^2_x}$.&lt;/li&gt;
&lt;li&gt;Even if $X^TX$ is not invertible, adding $\lambda$ to each feature will make it full rank matrix $\Rightarrow$ invertible.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To illustrate how regularization helps us reduce generalization error, we&#39;ll use the cats_vs_dogs dataset. The dataset has images for cats and dogs. We&#39;ll try to build a neural network to classify if the image has a cat or a dog. Each image is 64 x 64 pixels on RGB scale.&lt;/p&gt;

&lt;p&gt;We&#39;ll be using functions we wrote in &lt;a href=&#34;https://nbviewer.jupyter.org/github/ImadDabbura/blog-posts/blob/master/notebooks/Coding-Neural-Network-Forwad-Back-Propagation.ipynb&#34;&gt;&lt;em&gt;&amp;quot;Coding Neural Network - Forward Propagation and Backpropagation&amp;quot;&lt;/em&gt;&lt;/a&gt; post to initialize parameters, compute forward propagation, cross-entropy cost, gradients, etc.&lt;/p&gt;

&lt;p&gt;Let&#39;s import the data and take a look at the shape as well as a sample of a cat image from the training set.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Loading packages
import sys

import h5py
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

sys.path.append(&amp;quot;../scripts/&amp;quot;)
from coding_neural_network_from_scratch import (initialize_parameters,
                                                L_model_forward,
                                                compute_cost,
                                                relu_gradient,
                                                sigmoid_gradient,
                                                tanh_gradient,
                                                update_parameters,
                                                accuracy)
from gradient_checking import dictionary_to_vector
from load_dataset import load_dataset_catvsdog

%matplotlib inline
sns.set_context(&amp;quot;notebook&amp;quot;)
plt.style.use(&amp;quot;fivethirtyeight&amp;quot;)
plt.rcParams[&#39;figure.figsize&#39;] = (12, 6)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import training data
train_dataset = h5py.File(&amp;quot;../data/train_catvnoncat.h5&amp;quot;)
X_train = np.array(train_dataset[&amp;quot;train_set_x&amp;quot;])
Y_train = np.array(train_dataset[&amp;quot;train_set_y&amp;quot;])

# Plot a sample image
plt.imshow(X_train[50])
plt.axis(&amp;quot;off&amp;quot;);

# Import test data
test_dataset = h5py.File(&amp;quot;../data/test_catvnoncat.h5&amp;quot;)
X_test = np.array(test_dataset[&amp;quot;test_set_x&amp;quot;])
Y_test = np.array(test_dataset[&amp;quot;test_set_y&amp;quot;])

# Transform data
X_train = X_train.reshape(209, -1).T
X_train = X_train / 255
Y_train = Y_train.reshape(-1, 209)

X_test = X_test.reshape(50, -1).T
X_test = X_test / 255
Y_test = Y_test.reshape(-1, 50)

# print the new shape of both training and test datasets
print(&amp;quot;Training data dimensions:&amp;quot;)
print(&amp;quot;X&#39;s dimension: {}, Y&#39;s dimension: {}&amp;quot;.format(X_train.shape, Y_train.shape))
print(&amp;quot;Test data dimensions:&amp;quot;)
print(&amp;quot;X&#39;s dimension: {}, Y&#39;s dimension: {}&amp;quot;.format(X_test.shape, Y_test.shape))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Training data dimensions:
X&#39;s dimension: (12288, 209), Y&#39;s dimension: (1, 209)
Test data dimensions:
X&#39;s dimension: (12288, 50), Y&#39;s dimension: (1, 50)
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/cat_sample.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/cat_sample.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Sample image.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;The training set has 209 examples and the test set has 50 examples. Let&#39;s first write all the helper functions that would help us write the multi-layer neural network.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_cost_reg(AL, y, parameters, lambd=0):
    # number of examples
    m = y.shape[1]

    # compute traditional cross entropy cost
    cross_entropy_cost = compute_cost(AL, y)

    # convert parameters dictionary to vector
    parameters_vector = dictionary_to_vector(parameters)

    # compute the regularization penalty
    L2_regularization_penalty = (
        lambd / (2 * m)) * np.sum(np.square(parameters_vector))

    # compute the total cost
    cost = cross_entropy_cost + L2_regularization_penalty

    return cost


def linear_backword_reg(dZ, cache, lambd=0):
    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = (1 / m) * np.dot(dZ, A_prev.T) + (lambd / m) * W
    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)
    dA_prev = np.dot(W.T, dZ)

    assert (dA_prev.shape == A_prev.shape)
    assert (dW.shape == W.shape)
    assert (db.shape == b.shape)

    return dA_prev, dW, db


def linear_activation_backward_reg(dA, cache, activation_fn=&amp;quot;relu&amp;quot;, lambd=0):
    linear_cache, activation_cache = cache

    if activation_fn == &amp;quot;sigmoid&amp;quot;:
        dZ = sigmoid_gradient(dA, activation_cache)
        dA_prev, dW, db = linear_backword_reg(dZ, linear_cache, lambd)

    elif activation_fn == &amp;quot;tanh&amp;quot;:
        dZ = tanh_gradient(dA, activation_cache)
        dA_prev, dW, db = linear_backword_reg(dZ, linear_cache, lambd)

    elif activation_fn == &amp;quot;relu&amp;quot;:
        dZ = relu_gradient(dA, activation_cache)
        dA_prev, dW, db = linear_backword_reg(dZ, linear_cache, lambd)

    return dA_prev, dW, db


def L_model_backward_reg(AL, y, caches, hidden_layers_activation_fn=&amp;quot;relu&amp;quot;,
                         lambd=0):
    y = y.reshape(AL.shape)
    L = len(caches)
    grads = {}

    dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))

    grads[&amp;quot;dA&amp;quot; + str(L - 1)], grads[&amp;quot;dW&amp;quot; + str(L)], grads[&amp;quot;db&amp;quot; + str(L)] =\
        linear_activation_backward_reg(dAL, caches[L - 1], &amp;quot;sigmoid&amp;quot;, lambd)

    for l in range(L - 1, 0, -1):
        current_cache = caches[l - 1]
        grads[&amp;quot;dA&amp;quot; + str(l - 1)], grads[&amp;quot;dW&amp;quot; + str(l)], grads[&amp;quot;db&amp;quot; + str(l)] =\
            linear_activation_backward_reg(
                grads[&amp;quot;dA&amp;quot; + str(l)], current_cache,
                hidden_layers_activation_fn, lambd)

    return grads


def model_with_regularization(
        X, y, layers_dims, learning_rate=0.01,  num_epochs=3000,
        print_cost=False, hidden_layers_activation_fn=&amp;quot;relu&amp;quot;, lambd=0):
    # get number of examples
    m = X.shape[1]

    # to get consistents output
    np.random.seed(1)

    # initialize parameters
    parameters = initialize_parameters(layers_dims)

    # intialize cost list
    cost_list = []

    # implement gradient descent
    for i in range(num_epochs):
        # compute forward propagation
        AL, caches = L_model_forward(
            X, parameters, hidden_layers_activation_fn)

        # compute regularized cost
        reg_cost = compute_cost_reg(AL, y, parameters, lambd)

        # compute gradients
        grads = L_model_backward_reg(
            AL, y, caches, hidden_layers_activation_fn, lambd)

        # update parameters
        parameters = update_parameters(parameters, grads, learning_rate)

        # print cost
        if (i + 1) % 100 == 0 and print_cost:
            print(&amp;quot;The cost after {} iterations: {}&amp;quot;.format(
                (i + 1), reg_cost))

        # append cost
        if i % 100 == 0:
            cost_list.append(reg_cost)

    # plot the cost curve
    plt.plot(cost_list)
    plt.xlabel(&amp;quot;Iterations (per hundreds)&amp;quot;)
    plt.ylabel(&amp;quot;Cost&amp;quot;)
    plt.title(&amp;quot;Cost curve for the learning rate = {}&amp;quot;.format(learning_rate))

    return parameters
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we&#39;re ready to train the neural network. We&#39;ll first build a neural network with no regularization and then one with regularization to see which one has lower generalization error. Note that $\lambda$ should be tuned to get the best results but we&#39;ll here choose an arbitrary value to illustrate the concept. Both neural netwotks would have 2 hidden layers where each hidden layer has 5 units.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# set up layers dimensions
layers_dims = [X_train.shape[0], 5, 5, 1]

# train NN
parameters = model_with_regularization(X_train, Y_train, layers_dims,
                                       learning_rate=0.03, num_epochs=2500, print_cost=True,
                                       hidden_layers_activation_fn=&amp;quot;tanh&amp;quot;,
                                       lambd=0)

# print the test accuracy
print(&amp;quot;The training accuracy rate: {}&amp;quot;.format(accuracy(X_train, parameters, Y_train, &amp;quot;tanh&amp;quot;)[-7:]))
print(&amp;quot;The test accuracy rate: {}&amp;quot;.format(accuracy(X_test, parameters, Y_test, &amp;quot;tanh&amp;quot;)[-7:]))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;The cost after 100 iterations: 0.6555634398145331
The cost after 200 iterations: 0.6467746423961933
The cost after 300 iterations: 0.6446638811282552
The cost after 400 iterations: 0.6441400737542232
The cost after 500 iterations: 0.6440063101787575
The cost after 600 iterations: 0.6439697872317176
The cost after 700 iterations: 0.6439570623358253
The cost after 800 iterations: 0.6439491872993496
The cost after 900 iterations: 0.6439407592837082
The cost after 1000 iterations: 0.6439294591543208
The cost after 1100 iterations: 0.6439131091764411
The cost after 1200 iterations: 0.6438883396380859
The cost after 1300 iterations: 0.6438489715870495
The cost after 1400 iterations: 0.6437825798034876
The cost after 1500 iterations: 0.6436617691190204
The cost after 1600 iterations: 0.6434191397054715
The cost after 1700 iterations: 0.642864008138056
The cost after 1800 iterations: 0.6413476000796884
The cost after 1900 iterations: 0.6360827945885947
The cost after 2000 iterations: 0.6124050450908987
The cost after 2100 iterations: 0.511236045905345
The cost after 2200 iterations: 0.5287658028657057
The cost after 2300 iterations: 0.43124104856359174
The cost after 2400 iterations: 0.38213869447364884
The cost after 2500 iterations: 0.3386708692392079
The training accuracy rate: 82.30%.
The test accuracy rate: 78.00%.
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/loss_no_reg.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/loss_no_reg.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Cost curve with no regularization.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;The training accuracy is 82.30% but the test accuracy is 78%. The difference between training and test accuracy is not that much, i.e. we don&#39;t have a lot of overfitting. Therefore, a little bit of regularization may help such as $\lambda = 0.02$. Values of $\lambda$s that practitioners recommend are: 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# train NN with regularization
parameters = model_with_regularization(X_train, Y_train, layers_dims,
                                       learning_rate=0.03, num_epochs=2500, print_cost=True,
                                       hidden_layers_activation_fn=&amp;quot;tanh&amp;quot;,
                                       lambd=0.02)

# print the test accuracy
print(&amp;quot;The training accuracy rate: {}&amp;quot;.format(accuracy(X_train, parameters, Y_train, &amp;quot;tanh&amp;quot;)[-7:]))
print(&amp;quot;The test accuracy rate: {}&amp;quot;.format(accuracy(X_test, parameters, Y_test, &amp;quot;tanh&amp;quot;)[-7:]))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;The cost after 100 iterations: 0.6558634554205135
The cost after 200 iterations: 0.6470807090618383
The cost after 300 iterations: 0.6449737235917311
The cost after 400 iterations: 0.6444519406797673
The cost after 500 iterations: 0.6443191828114609
The cost after 600 iterations: 0.6442831256251426
The cost after 700 iterations: 0.6442705985766486
The cost after 800 iterations: 0.6442628048800636
The cost after 900 iterations: 0.6442544325786784
The cost after 1000 iterations: 0.6442432311807257
The cost after 1100 iterations: 0.6442270988055475
The cost after 1200 iterations: 0.6442027847231018
The cost after 1300 iterations: 0.6441643410411311
The cost after 1400 iterations: 0.6440998547029029
The cost after 1500 iterations: 0.6439832000181198
The cost after 1600 iterations: 0.6437505375793907
The cost after 1700 iterations: 0.6432228625403317
The cost after 1800 iterations: 0.6417982979158361
The cost after 1900 iterations: 0.6369273437378263
The cost after 2000 iterations: 0.6152774362019153
The cost after 2100 iterations: 0.5207828651496548
The cost after 2200 iterations: 0.5145012356446598
The cost after 2300 iterations: 0.40757220705507585
The cost after 2400 iterations: 0.517757346098386
The cost after 2500 iterations: 0.4574831239241244
The training accuracy rate: 65.55%.
The test accuracy rate: 80.00%.
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/loss_reg.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/loss_reg.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Cost curve with regularization.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;As the results above show, we improved the generalization error by increasing the test accuracy from 78% to 80%. On the other hand, training accuracy decreased from 82.30% to 65.55%.&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Conclusion
&lt;/h2&gt;

&lt;p&gt;Regularization is an effective technique to resolve overfitting. Since we don&#39;t know true distribution of the data, empirical risk, which is based of empirical distribution, is prone to overfitting. Therefore, the best strategy is to fit training data really well and then use a regularization technique so that the model generalizes well. L2 parameter regularization along with Dropout are two of the most widely used regularization technique in machine learning.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;One of the implicit assumptions of regularization techniques such as L2 and L1 parameter regularization is that the value of the parameters should be zero and try to shrink all parameters towards zero. It&#39;s meant to avoid following the training data very well which makes the learning algorithm picks some noise that is not helpful when applied on unseen data.&lt;/li&gt;
&lt;li&gt;The value of $\lambda$ should be tuned to get the best generalization error. We typically use validation set when comparing models with values for $\lambda$s and pick the one with the lowest validation error.&lt;/li&gt;
&lt;li&gt;Only use regularization if the model suffers from overfitting, i.e training error &amp;lt;&amp;lt; validation error.&lt;/li&gt;
&lt;li&gt;If after using regularization the validation error is still high, then we&#39;re most likely in the underfitting region. In other words, our model is still too simple and already has high bias. Therefore, add complexity to the model and then use regularization.&lt;/li&gt;
&lt;li&gt;Since the majority of tasks we try to solve don&#39;t have enough data (or expensive to collect more data), overfitting will be more prevalent in Deep Learning than underfitting given the complexity of neural networks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The source code that created this post can be found &lt;a href=&#34;https://github.com/ImadDabbura/blog-posts/blob/master/notebooks/Coding-Neural-Network-Regularization.ipynb&#34;&gt;here&lt;/a&gt;.
The post is inspired by deeplearning.ai courses.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coding Neural Network - Parameters&#39; Initialization</title>
      <link>https://imaddabbura.github.io/post/coding-nn-params-init/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://imaddabbura.github.io/post/coding-nn-params-init/</guid>
      <description>&lt;p&gt;Optimization, in Machine Learning/Deep Learning contexts, is the process of changing the model&#39;s parameters to improve its performance. In other words, it&#39;s the process of finding the best parameters in the predefined hypothesis space to get the best possible performance. There are three kinds of optimization algorithms:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Optimization algorithm that is not iterative and simply solves for one point.&lt;/li&gt;
&lt;li&gt;Optimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression.&lt;/li&gt;
&lt;li&gt;Optimization algorithm that is iterative in nature and applied to a set of problems that have non-convex loss functions such as neural networks. Therefore, parameters&#39; initialization plays a critical role in speeding up convergence and achieving lower error rates.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this post, we&#39;ll look at three different cases of parameters&#39; initialization and see how this affects the error rate:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Initialize all parameters to zero.&lt;/li&gt;
&lt;li&gt;Initialize parameters to random values from standard normal distribution or uniform distribution and multiply it by a scalar such as 10.&lt;/li&gt;
&lt;li&gt;Initialize parameters based on:

&lt;ul&gt;
&lt;li&gt;Xavier recommendation.&lt;/li&gt;
&lt;li&gt;Kaiming He recommendation.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&#39;ll be using functions we wrote in &lt;a href=&#34;https://imaddabbura.github.io/blog/machine%20learning/deep%20learning/2018/04/01/coding-neural-network-fwd-back-prop.html&#34;&gt;&lt;em&gt;&amp;quot;Coding Neural Network - Forward Propagation and Backpropagation&amp;quot;&lt;/em&gt;&lt;/a&gt; post to initialize parameters, compute forward propagation and back-propagation as well as the cross-entropy cost.&lt;/p&gt;

&lt;p&gt;To illustrate the above cases, we&#39;ll use the cats vs dogs dataset which consists of 50 images for cats and 50 images for dogs. Each image is 150 pixels x 150 pixels on RGB color scale. Therefore, we would have 67,500 features where each column in the input matrix would be one image which means our input data would have 67,500 x 100 dimension.&lt;/p&gt;

&lt;p&gt;Let&#39;s first load the data and show a sample of two images before we start the helper functions.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Loading packages
import sys

import h5py
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

sys.path.append(&amp;quot;../scripts/&amp;quot;)
from coding_neural_network_from_scratch import (L_model_forward,
                                                compute_cost,
                                                L_model_backward,
                                                update_parameters,
                                                accuracy)
from load_dataset import load_dataset_catvsdog

%matplotlib inline
sns.set_context(&amp;quot;notebook&amp;quot;)
plt.style.use(&amp;quot;fivethirtyeight&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X, Y = load_dataset_catvsdog(&amp;quot;../data&amp;quot;)

# show a sample of of a cat and a dog image
index_cat = np.argmax(Y); index_dog = np.argmin(Y)
plt.subplot(1, 2, 1)
plt.imshow(X[:, index_cat].reshape(150, 150, 3))
plt.axis(&amp;quot;off&amp;quot;)
plt.subplot(1, 2, 2)
plt.imshow(X[:, index_dog].reshape(150, 150, 3))
plt.axis(&amp;quot;off&amp;quot;);

# standarize the data
X = X / 255
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/sample_images.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/sample_images.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Sample images.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;We&#39;ll write now all the helper functions that will help us initialize parameters based on different methods as well as writing L-layer model that we&#39;ll be using to train our neural network.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def initialize_parameters_zeros(layers_dims):
    np.random.seed(1)               
    parameters = {}                 
    L = len(layers_dims)            

    for l in range(1, L):
        parameters[&amp;quot;W&amp;quot; + str(l)] = np.zeros(
            (layers_dims[l], layers_dims[l - 1]))
        parameters[&amp;quot;b&amp;quot; + str(l)] = np.zeros((layers_dims[l], 1))

    return parameters


def initialize_parameters_random(layers_dims):
    np.random.seed(1)               
    parameters = {}                 
    L = len(layers_dims)            

    for l in range(1, L):
        parameters[&amp;quot;W&amp;quot; + str(l)] = np.random.randn(
            layers_dims[l], layers_dims[l - 1]) * 10
        parameters[&amp;quot;b&amp;quot; + str(l)] = np.zeros((layers_dims[l], 1))

    return parameters


def initialize_parameters_he_xavier(layers_dims, initialization_method=&amp;quot;he&amp;quot;):
    np.random.seed(1)               
    parameters = {}                 
    L = len(layers_dims)            

    if initialization_method == &amp;quot;he&amp;quot;:
        for l in range(1, L):
            parameters[&amp;quot;W&amp;quot; + str(l)] = np.random.randn(
                layers_dims[l],
                layers_dims[l - 1]) * np.sqrt(2 / layers_dims[l - 1])
            parameters[&amp;quot;b&amp;quot; + str(l)] = np.zeros((layers_dims[l], 1))
    elif initialization_method == &amp;quot;xavier&amp;quot;:
        for l in range(1, L):
            parameters[&amp;quot;W&amp;quot; + str(l)] = np.random.randn(
                layers_dims[l],
                layers_dims[l - 1]) * np.sqrt(1 / layers_dims[l - 1])
            parameters[&amp;quot;b&amp;quot; + str(l)] = np.zeros((layers_dims[l], 1))

    return parameters

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def model(X, Y, layers_dims, learning_rate=0.01, num_iterations=1000,
          print_cost=True, hidden_layers_activation_fn=&amp;quot;relu&amp;quot;,
          initialization_method=&amp;quot;he&amp;quot;):
    np.random.seed(1)

    # initialize cost list
    cost_list = []

    # initialize parameters
    if initialization_method == &amp;quot;zeros&amp;quot;:
        parameters = initialize_parameters_zeros(layers_dims)
    elif initialization_method == &amp;quot;random&amp;quot;:
        parameters = initialize_parameters_random(layers_dims)
    else:
        parameters = initialize_parameters_he_xavier(
            layers_dims, initialization_method)

    # iterate over num_iterations
    for i in range(num_iterations):
        # iterate over L-layers to get the final output and the cache
        AL, caches = L_model_forward(
            X, parameters, hidden_layers_activation_fn)

        # compute cost to plot it
        cost = compute_cost(AL, Y)

        # iterate over L-layers backward to get gradients
        grads = L_model_backward(AL, Y, caches, hidden_layers_activation_fn)

        # update parameters
        parameters = update_parameters(parameters, grads, learning_rate)

        # append each 100th cost to the cost list
        if (i + 1) % 100 == 0 and print_cost:
            print(&amp;quot;The cost after {} iterations is: {}&amp;quot;.format(i + 1, cost))

        if i % 100 == 0:
            cost_list.append(cost)

    # plot the cost curve
    plt.figure(figsize=(12, 8))
    plt.plot(cost_list)
    plt.xlabel(&amp;quot;Iterations (per hundreds)&amp;quot;, fontsize=14)
    plt.ylabel(&amp;quot;Cost&amp;quot;, fontsize=14)
    plt.title(
        &amp;quot;Cost curve: learning rate = {} and {} initialization method&amp;quot;.format(
            learning_rate, initialization_method), y=1.05, fontsize=16)

    return parameters

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Initializing all parameters to zero
&lt;/h2&gt;

&lt;p&gt;Here, we&#39;ll initialize all weight matrices and biases to zeros and see how this would affect the error rate as well as the learning parameters.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# train NN with zeros initialization parameters
layers_dims = [X.shape[0], 5, 5, 1]
parameters = model(X, Y, layers_dims, hidden_layers_activation_fn=&amp;quot;tanh&amp;quot;,
                   initialization_method=&amp;quot;zeros&amp;quot;)

accuracy(X, parameters, Y,&amp;quot;tanh&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;The cost after 100 iterations is: 0.6931471805599453
The cost after 200 iterations is: 0.6931471805599453
The cost after 300 iterations is: 0.6931471805599453
The cost after 400 iterations is: 0.6931471805599453
The cost after 500 iterations is: 0.6931471805599453
The cost after 600 iterations is: 0.6931471805599453
The cost after 700 iterations is: 0.6931471805599453
The cost after 800 iterations is: 0.6931471805599453
The cost after 900 iterations is: 0.6931471805599453
The cost after 1000 iterations is: 0.6931471805599453

&#39;The accuracy rate is: 50.00%.&#39;
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/zero_params.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/zero_params.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Cost curve using zero intialization method.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;As the cost curve shows, the neural network didn&#39;t learn anything! That is because of symmetry between all neurons which leads to all neurons have the same update on every iteration. Therefore, regardless of how many iterations we run the optimization algorithms, all the neurons would still get the same update and no learning would happen. As a result, we must &lt;strong&gt;break symmetry&lt;/strong&gt; when initializing parameters so that the model would start learning on each update of the gradient descent.&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Initializing parameters with big random values
&lt;/h2&gt;

&lt;p&gt;There is no big difference if the random values are initialized from standard normal distribution or uniform distribution so we&#39;ll use standard normal distribution in our examples. Also, we&#39;ll multiply the random values by a big number such as 10 to show that initializing parameters to big values may cause our optimization to have higher error rates (and even diverge in some cases). Let&#39;s now train our neural network where all weight matrices have been intitialized using the following formula:
&lt;code&gt;np.random.randn() * 10&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# train NN with random initialization parameters
layers_dims = [X.shape[0], 5, 5, 1]
parameters = model(X, Y, layers_dims, hidden_layers_activation_fn=&amp;quot;tanh&amp;quot;,
                   initialization_method=&amp;quot;random&amp;quot;)

accuracy(X, parameters, Y,&amp;quot;tanh&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;The cost after 100 iterations is: 1.2413142077549013
The cost after 200 iterations is: 1.1258751902393416
The cost after 300 iterations is: 1.0989052435267657
The cost after 400 iterations is: 1.0840966471282327
The cost after 500 iterations is: 1.0706953292105978
The cost after 600 iterations is: 1.0574847320236294
The cost after 700 iterations is: 1.0443168708889223
The cost after 800 iterations is: 1.031157857251139
The cost after 900 iterations is: 1.0179838815204902
The cost after 1000 iterations is: 1.004767088515343

&#39;The accuracy rate is: 55.00%.&#39;
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/random_weights.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/random_weights.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Cost curve using random initialization method.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Random initialization here is helping but still the loss function has high value and may take long time to converge and achieve a significantly low value.&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Initializing parameters based on He and Xavier recommendations
&lt;/h2&gt;

&lt;p&gt;We&#39;ll explore two initialization methods:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kaiming He method is best applied when activation function applied on hidden layers is Rectified Linear Unit (ReLU). so that the weight on each hidden layer would have the following variance:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(var(W^l) = \frac{2}{n^{l - 1}}\)&lt;/span&gt;
We can achieve this by multiplying the random values from standard normal distribution by $\sqrt{\frac{2}{number\ of\ units\ in \ previous\ layer}}$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Xavier method is best applied when activation function applied on hidden layers is Hyperbolic Tangent so that the weight on each hidden layer would have the following variance:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(var(W^l) = \frac{1}{n^{l - 1}}\)&lt;/span&gt;
We can achieve this by multiplying the random values from standard normal distribution by $\sqrt{\frac{1}{number\ of\ units\ in \ previous\ layer}}$&lt;/p&gt;

&lt;p&gt;We&#39;ll train the network using both methods and look at the results.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# train NN where all parameters were initialized based on He recommendation
layers_dims = [X.shape[0], 5, 5, 1]
parameters = model(X, Y, layers_dims, hidden_layers_activation_fn=&amp;quot;tanh&amp;quot;,
                   initialization_method=&amp;quot;he&amp;quot;)

accuracy(X, parameters, Y, &amp;quot;tanh&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;The cost after 100 iterations is: 0.6300611704834093
The cost after 200 iterations is: 0.49092836452522753
The cost after 300 iterations is: 0.46579423512433943
The cost after 400 iterations is: 0.6516254192289226
The cost after 500 iterations is: 0.32487779301799485
The cost after 600 iterations is: 0.4631461605716059
The cost after 700 iterations is: 0.8050310690163623
The cost after 800 iterations is: 0.31739195517372376
The cost after 900 iterations is: 0.3094592175030812
The cost after 1000 iterations is: 0.19934509244449203

&#39;The accuracy rate is: 99.00%.&#39;
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/he.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/he.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Cost curve using He initialization method.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# train NN where all parameters were initialized based on Xavier recommendation
layers_dims = [X.shape[0], 5, 5, 1]
parameters = model(X, Y, layers_dims, hidden_layers_activation_fn=&amp;quot;tanh&amp;quot;,
                   initialization_method=&amp;quot;xavier&amp;quot;)

accuracy(X, parameters, Y, &amp;quot;tanh&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;The cost after 100 iterations is: 0.6351961521800779
The cost after 200 iterations is: 0.548973489787121
The cost after 300 iterations is: 0.47982386652748565
The cost after 400 iterations is: 0.32811768889968684
The cost after 500 iterations is: 0.2793453045790634
The cost after 600 iterations is: 0.3258507563809604
The cost after 700 iterations is: 0.2873032724176074
The cost after 800 iterations is: 0.0924974839405706
The cost after 900 iterations is: 0.07418011931058155
The cost after 1000 iterations is: 0.06204402572328295

&#39;The accuracy rate is: 99.00%.&#39;
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/xafier.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/xafier.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Cost curve using Xavier initialization method.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;As shown from applying the four methods, parameters&#39; initial values play a huge role in achieving low cost values as well as converging and achieve lower training error rates. The same would apply to test error rate if we had test data.&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Conclusion
&lt;/h2&gt;

&lt;p&gt;Deep Learning frameworks make it easier to choose between different initialization methods without worrying about implementing it ourselves. Nonetheless, it&#39;s important to understand the critical role initial values of the parameters in the overall performance of the network. Below are some key takeaways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Well chosen initialization values of parameters leads to:

&lt;ul&gt;
&lt;li&gt;Speed up convergence of gradient descent.&lt;/li&gt;
&lt;li&gt;Increase the likelihood of gradient descent to find lower training and generalization error rates.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Because we&#39;re dealing with iterative optimization algorithms with non-convex loss function, different initializations lead to different results.&lt;/li&gt;
&lt;li&gt;Random initialization is used to break symmetry and make sure different hidden units can learn different things.&lt;/li&gt;
&lt;li&gt;Don&#39;t initialize to values that are too large.&lt;/li&gt;
&lt;li&gt;Kaiming He (He) initialization works well for neural networks with ReLU activation function.&lt;/li&gt;
&lt;li&gt;Xavier initialization works well for neural networks with Hyperbolic Tangent activation function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The source code that created this post can be found &lt;a href=&#34;https://github.com/ImadDabbura/blog-posts/blob/master/notebooks/Coding-Neural-Network-Parameters-Initialization.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coding Neural Network - Gradient Checking</title>
      <link>https://imaddabbura.github.io/post/coding-nn-gradient-checking/</link>
      <pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://imaddabbura.github.io/post/coding-nn-gradient-checking/</guid>
      <description>&lt;p&gt;In the previous post, &lt;a href=&#34;https://imaddabbura.github.io/blog/machine%20learning/deep%20learning/2018/04/01/coding-neural-network-fwd-back-prop.html&#34;&gt;&lt;em&gt;Coding Neural Network - Forward Propagation and Backpropagation&lt;/em&gt;&lt;/a&gt;, we implemented both forward propagation and backpropagation in &lt;code&gt;numpy&lt;/code&gt;. However, implementing backpropagation from scratch is usually more prune to bugs/errors. Therefore, it&#39;s necessary before running the neural network on training data to check if our implementation of backpropagation is correct. Before we start, let&#39;s revisit what back-propagation is: We loop over the nodes in reverse topological order starting at the final node to compute the derivative of the cost with respect to each edge&#39;s node tail. In other words, we compute the derivative of cost function with respect to all parameters, i.e $\frac{\partial J}{\partial \theta}$ where $\theta$ represents the parameters of the model.&lt;/p&gt;

&lt;p&gt;The way to test our implementation is by computing numerical gradients and compare it with gradients from backpropagation (analytical). There are two way of computing numerical gradients:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Right-hand form:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{J(\theta + \epsilon) - J(\theta)}{\epsilon}\tag{1}\]&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Two-sided form (see figure 2):&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{J(\theta + \epsilon) - J(\theta - \epsilon)}{2 \epsilon}\tag{2}\]&lt;/span&gt;&lt;/p&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/two_sided_gradients.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/two_sided_gradients.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Two-sided numerical gradients.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Two-sided form of approximating the derivative is closer than the right-hand form. Let&#39;s illustrate that with the following example using the function $f(x) = x^2$ by taking its derivative at $x = 3$.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Analytical derivative:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\nabla_x f(x) = 2x\ \Rightarrow\nabla_x f(3) = 6\]&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Two-sided numerical derivative:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{(3 + 1e-2)^2 - (3 - 1e-2)^2}{2 * 1e-2} = 5.999999999999872\]&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Right-hand numerical derivative:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{(3 + 1e-2)^2 - 3^2}{1e-2} = 6.009999999999849\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;As we see above, the difference between analytical derivative and two-sided numerical gradient is almost zero; however, the difference between analytical derivative and right-sided derivative is 0.01. Therefore, we&#39;ll use two-sided epsilon method to compute the numerical gradients.&lt;/p&gt;

&lt;p&gt;In addition, we&#39;ll normalize the difference between numerical. gradients and analytical gradients using the following formula:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\|grad - grad\_{approx}\|_2}{\|grad\|_2 + \|grad\_{approx}\|_2}\tag{3}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;If the difference is $\leq 10^{-7}$, then our implementation is fine; otherwise, we have a mistake somewhere and have to go back and revisit backpropagation code.&lt;/p&gt;

&lt;p&gt;Below are the steps needed to implement gradient checking:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Pick random number of examples from training data to use it when computing both numerical and analytical gradients.

&lt;ul&gt;
&lt;li&gt;Don&#39;t use all examples in the training data because gradient checking is very slow.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Initialize parameters.&lt;/li&gt;
&lt;li&gt;Compute forward propagation and the cross-entropy cost.&lt;/li&gt;
&lt;li&gt;Compute the gradients using our back-propagation implementation.&lt;/li&gt;
&lt;li&gt;Compute the numerical gradients using the two-sided epsilon method.&lt;/li&gt;
&lt;li&gt;Compute the difference between numerical and analytical gradients.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We&#39;ll be using functions we wrote in &lt;em&gt;&amp;quot;Coding Neural Network - Forward Propagation and Backpropagation&amp;quot;&lt;/em&gt; post to initialize parameters, compute forward propagation and back-propagation as well as the cross-entropy cost.&lt;/p&gt;

&lt;p&gt;Let&#39;s first import the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Loading packages
import sys

import h5py
import matplotlib.pyplot as plt
import numpy as np
from numpy.linalg import norm
import seaborn as sns

sys.path.append(&amp;quot;../scripts/&amp;quot;)
from coding_neural_network_from_scratch import (initialize_parameters,
                                                L_model_forward,
                                                L_model_backward,
                                                compute_cost)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import the data
train_dataset = h5py.File(&amp;quot;../data/train_catvnoncat.h5&amp;quot;)
X_train = np.array(train_dataset[&amp;quot;train_set_x&amp;quot;]).T
y_train = np.array(train_dataset[&amp;quot;train_set_y&amp;quot;]).T
X_train = X_train.reshape(-1, 209)
y_train = y_train.reshape(-1, 209)

X_train.shape, y_train.shape
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;((12288, 209), (1, 209))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we&#39;ll write helper functions that faciltate converting parameters and gradients dictionaries into vectors and then re-convert them back to dictionaries.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def dictionary_to_vector(params_dict):
    count = 0
    for key in params_dict.keys():
        new_vector = np.reshape(params_dict[key], (-1, 1))
        if count == 0:
            theta_vector = new_vector
        else:
            theta_vector = np.concatenate((theta_vector, new_vector))
        count += 1

    return theta_vector


def vector_to_dictionary(vector, layers_dims):
    L = len(layers_dims)
    parameters = {}
    k = 0

    for l in range(1, L):
        # Create temp variable to store dimension used on each layer
        w_dim = layers_dims[l] * layers_dims[l - 1]
        b_dim = layers_dims[l]

        # Create temp var to be used in slicing parameters vector
        temp_dim = k + w_dim

        # add parameters to the dictionary
        parameters[&amp;quot;W&amp;quot; + str(l)] = vector[
            k:temp_dim].reshape(layers_dims[l], layers_dims[l - 1])
        parameters[&amp;quot;b&amp;quot; + str(l)] = vector[
            temp_dim:temp_dim + b_dim].reshape(b_dim, 1)

        k += w_dim + b_dim

    return parameters


def gradients_to_vector(gradients):
    # Get the number of indices for the gradients to iterate over
    valid_grads = [key for key in gradients.keys()
                   if not key.startswith(&amp;quot;dA&amp;quot;)]
    L = len(valid_grads)// 2
    count = 0
    
    # Iterate over all gradients and append them to new_grads list
    for l in range(1, L + 1):
        if count == 0:
            new_grads = gradients[&amp;quot;dW&amp;quot; + str(l)].reshape(-1, 1)
            new_grads = np.concatenate(
                (new_grads, gradients[&amp;quot;db&amp;quot; + str(l)].reshape(-1, 1)))
        else:
            new_grads = np.concatenate(
                (new_grads, gradients[&amp;quot;dW&amp;quot; + str(l)].reshape(-1, 1)))
            new_grads = np.concatenate(
                (new_grads, gradients[&amp;quot;db&amp;quot; + str(l)].reshape(-1, 1)))
        count += 1
        
    return new_grads
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we&#39;ll write the gradient checking function that will compute the difference between the analytical and numerical gradients and tell us if our implementation of back-propagation is correct. We&#39;ll randomly choose 1 example to compute the difference.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def forward_prop_cost(X, parameters, Y, hidden_layers_activation_fn=&amp;quot;tanh&amp;quot;):
    # Compute forward prop
    AL, _ = L_model_forward(X, parameters, hidden_layers_activation_fn)

    # Compute cost
    cost = compute_cost(AL, Y)

    return cost


def gradient_check(
        parameters, gradients, X, Y, layers_dims, epsilon=1e-7,
        hidden_layers_activation_fn=&amp;quot;tanh&amp;quot;):
    # Roll out parameters and gradients dictionaries
    parameters_vector = dictionary_to_vector(parameters)
    gradients_vector = gradients_to_vector(gradients)

    # Create vector of zeros to be used with epsilon
    grads_approx = np.zeros_like(parameters_vector)

    for i in range(len(parameters_vector)):
        # Compute cost of theta + epsilon
        theta_plus = np.copy(parameters_vector)
        theta_plus[i] = theta_plus[i] + epsilon
        j_plus = forward_prop_cost(
            X, vector_to_dictionary(theta_plus, layers_dims), Y,
            hidden_layers_activation_fn)

        # Compute cost of theta - epsilon
        theta_minus = np.copy(parameters_vector)
        theta_minus[i] = theta_minus[i] - epsilon
        j_minus = forward_prop_cost(
            X, vector_to_dictionary(theta_minus, layers_dims), Y,
            hidden_layers_activation_fn)

        # Compute numerical gradients
        grads_approx[i] = (j_plus - j_minus) / (2 * epsilon)

    # Compute the difference of numerical and analytical gradients
    numerator = norm(gradients_vector - grads_approx)
    denominator = norm(grads_approx) + norm(gradients_vector)
    difference = numerator / denominator

    if difference &amp;gt; 10e-7:
        print (&amp;quot;\033[31mThere is a mistake in back-propagation &amp;quot; +\
               &amp;quot;implementation. The difference is: {}&amp;quot;.format(difference))
    else:
        print (&amp;quot;\033[32mThere implementation of back-propagation is fine! &amp;quot;+\
               &amp;quot;The difference is: {}&amp;quot;.format(difference))

    return difference
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set up neural network architecture
layers_dims = [X_train.shape[0], 5, 5, 1]

# Initialize parameters
parameters = initialize_parameters(layers_dims)

# Randomly selecting 1 example from training data
perms = np.random.permutation(X_train.shape[1])
index = perms[:1]

# Compute forward propagation
AL, caches = L_model_forward(X_train[:, index], parameters, &amp;quot;tanh&amp;quot;)

# Compute analytical gradients
gradients = L_model_backward(AL, y_train[:, index], caches, &amp;quot;tanh&amp;quot;)

# Compute difference of numerical and analytical gradients
difference = gradient_check(parameters, gradients, X_train[:, index], y_train[:, index], layers_dims)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There implementation of back-propagation is fine! The difference is: 3.0220555297630148e-09&lt;/p&gt;

&lt;p&gt;Congratulations! Our implementation is correct ð&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Conclusion
&lt;/h2&gt;

&lt;p&gt;Below are some key takeaways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Two-sided numerical gradient approximates the analytical gradients more closely than right-side form.&lt;/li&gt;
&lt;li&gt;Since gradient checking is very slow:

&lt;ul&gt;
&lt;li&gt;Apply it on one or few training examples.&lt;/li&gt;
&lt;li&gt;Turn it off when training neural network after making sure that backpropagation&#39;s implementation is correct.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Gradient checking doesn&#39;t work when applying drop-out method. Use keep-prob = 1 to check gradient checking and then change it when training neural network.&lt;/li&gt;
&lt;li&gt;Epsilon = $10e-7$ is a common value used for the difference between analytical gradient and numerical gradient. If the difference is less than 10e-7 then the implementation of backpropagation is correct.&lt;/li&gt;
&lt;li&gt;Thanks to &lt;em&gt;Deep Learning&lt;/em&gt; frameworks such as Tensorflow and Pytorch, we may find ourselves rarely implement backpropagation because such frameworks compute that for us; however, it&#39;s a good practice to understand what happens under the hood to become a good Deep Learning practitioner.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The source code that created this post can be found &lt;a href=&#34;https://github.com/ImadDabbura/blog-posts/blob/master/notebooks/Coding-Neural-Network-Gradient-Checking.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coding Neural Network - Forward Propagation and Backpropagtion</title>
      <link>https://imaddabbura.github.io/post/coding-nn-fwd-bckwd-prop/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://imaddabbura.github.io/post/coding-nn-fwd-bckwd-prop/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Why Neural Networks?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;According to &lt;em&gt;Universal Approximate Theorem&lt;/em&gt;, Neural Networks can approximate as well as learn and represent any function given a large enough layer and desired error margin. The way neural network learns the true function is by building complex representations on top of simple ones. On each hidden layer, the neural network learns new feature space by first compute the affine (linear) transformations of the given inputs and then apply non-linear function which in turn will be the input of the next layer. This process will continue until we reach the output layer. Therefore, we can define neural network as information flows from inputs through hidden layers towards the output. For a 3-layers neural network, the learned function would be: $f(x) = f_3(f_2(f_1(x)))$ where:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$f_1(x)$: Function learned on first hidden layer&lt;/li&gt;
&lt;li&gt;$f_2(x)$: Function learned on second hidden layer&lt;/li&gt;
&lt;li&gt;$f_3(x)$: Function learned on output layer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, on each layer we learn different representation that gets more complicated with later hidden layers.Below is an example of a 3-layers neural network (we don&#39;t count input layer):&lt;/p&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/neural_net.jpg&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/neural_net.jpg&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Neural Network with two hidden layers.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;For example, computers can&#39;t understand images directly and don&#39;t know what to do with pixels data. However, a neural network can build a simple representation of the image in the early hidden layers that identifies edges. Given the first hidden layer output, it can learn corners and contours. Given the second hidden layer, it can learn parts such as nose. Finally, it can learn the object identity.&lt;/p&gt;

&lt;p&gt;Since &lt;strong&gt;truth is never linear&lt;/strong&gt; and representation is very critical to the performance of a machine learning algorithm, neural network can help us build very complex models and leave it to the algorithm to learn such representations without worrying about feature engineering that takes practitioners very long time and effort to curate a good representation.&lt;/p&gt;

&lt;p&gt;The post has two parts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#CodingNN&#34;&gt;Coding the neural network&lt;/a&gt;: This entails writing all the helper functions that would allow us to implement a multi-layer neural network. While doing so, I&#39;ll explain the theoretical parts whenever possible and give some advices on implementations.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Application&#34;&gt;Application&lt;/a&gt;: We&#39;ll implement the neural network we coded in the first part on image recognition problem to see if the network we built will be able to detect if the image has a cat or a dog and see it working :)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This post will be the first in a series of posts that cover implementing neural network in numpy including &lt;em&gt;gradient checking, parameter initialization, L2 regularization, dropout&lt;/em&gt;. The code that created this post can be found &lt;a href=&#34;https://nbviewer.jupyter.org/github/ImadDabbura/blog-posts/blob/master/notebooks/Coding-Neural-Network-Forwad-Back-Propagation.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import packages
import h5py
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;CodingNN&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
I. Coding The Neural Network
&lt;/h2&gt;

&lt;h3 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Forward Propagation
&lt;/h3&gt;

&lt;p&gt;The input $X$ provides the initial information that then propagates to the hidden units at each layer and finally produce the output $\widehat{Y}$. The architecture of the network entails determining its depth, width, and activation functions used on each layer. &lt;strong&gt;Depth&lt;/strong&gt; is the number of hidden layers. &lt;strong&gt;Width&lt;/strong&gt; is the number of units (nodes) on each hidden layer since we don&#39;t control neither input layer nor output layer dimensions. There are quite a few set of activation functions such &lt;em&gt;Rectified Linear Unit, Sigmoid, Hyperbolic tangent, etc&lt;/em&gt;. Research has proven that deeper networks outperform networks with more hidden units. Therefore, it&#39;s always better and won&#39;t hurt to train a deeper network (with diminishing returns).&lt;/p&gt;

&lt;p&gt;Lets first introduce some notations that will be used throughout the post:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$W^l$: Weights matrix for the $l^{th}$ layer&lt;/li&gt;
&lt;li&gt;$b^l$: Bias vector for the $l^{th}$ layer&lt;/li&gt;
&lt;li&gt;$Z^l$: Linear (affine) transformations of given inputs for the $l^{th}$ layer&lt;/li&gt;
&lt;li&gt;$g^l$: Activation function applied on the $l^{th}$ layer&lt;/li&gt;
&lt;li&gt;$A^l$: Post-activation output for the $l^{th}$ layer&lt;/li&gt;
&lt;li&gt;$dW^l$: Derivative of the cost function w.r.t  $W^l$ ($\frac{\partial J}{\partial W^l}$)&lt;/li&gt;
&lt;li&gt;$db^l$: Derivative of the cost function w.r.t $b^l$ ($\frac{\partial J}{\partial b^l})$)&lt;/li&gt;
&lt;li&gt;$dZ^l$: Derivative of the cost function w.r.t $Z^l$ ($\frac{\partial J}{\partial Z^l}$)&lt;/li&gt;
&lt;li&gt;$dA^l$: Derivative of the cost function w.r.t $A^l$ ($\frac{\partial J}{\partial A^l}$)&lt;/li&gt;
&lt;li&gt;$n^l$: Number of units (nodes) of the $l^{th}$ layer&lt;/li&gt;
&lt;li&gt;$m$: Number of examples&lt;/li&gt;
&lt;li&gt;$L$: Number of layers in the network (not including the input layer)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Next, we&#39;ll write down the dimensions of a multi-layer neural network in the general form to help us in matrix multiplication because one of the major challenges in implementing a neural network is getting the dimensions right.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$W^l$, $dW^l$: Number of units (nodes) in $l^{th}$ layer x Number of units (nodes) in $l - 1$ layer&lt;/li&gt;
&lt;li&gt;$b^l$, $db^l$: Number of units (nodes) in $l^{th}$ layer x 1&lt;/li&gt;
&lt;li&gt;$Z^l$, $dZ^l$: Number of units (nodes) in $l^{th}$ layer x number of examples&lt;/li&gt;
&lt;li&gt;$A^l$, $dA^l$: Number of units (nodes) in $l^{th}$ layer x number of examples&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The two equations we need to implement forward propagations are:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(Z^l = W^lA^{l - 1} + b ^l\tag1\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(A^l = g^l(Z^l) = g^l(W^lA^{l - 1} + b ^l)\tag2\)&lt;/span&gt;
These computations will take place on each layer.&lt;/p&gt;

&lt;p&gt;&lt;h3 style=&#34;font-family: Georgia; font-size:1.3em;color:purple; font-style:bold&#34;&gt;
Parameters Initialization
&lt;/h3&gt;&lt;br&gt;
We&#39;ll first initialize the weight matrices and the bias vectors. It&#39;s important to note that we shouldn&#39;t initialize all the parameters to zero because doing so will lead the gradients to be equal and on each iteration the output would be the same and the learning algorithm won&#39;t learn anything. Therefore, it&#39;s important to randomly initialize the parameters to values between 0 and 1. It&#39;s also recommended to multiply the random values by small scalar such as 0.01 to make the activation units active and be on the regions where activation functions&#39; derivatives are not close to zero.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initialize parameters
def initialize_parameters(layers_dims):
    np.random.seed(1)               
    parameters = {}
    L = len(layers_dims)            

    for l in range(1, L):           
        parameters[&amp;quot;W&amp;quot; + str(l)] = np.random.randn(
            layers_dims[l], layers_dims[l - 1]) * 0.01
        parameters[&amp;quot;b&amp;quot; + str(l)] = np.zeros((layers_dims[l], 1))

        assert parameters[&amp;quot;W&amp;quot; + str(l)].shape == (
            layers_dims[l], layers_dims[l - 1])
        assert parameters[&amp;quot;b&amp;quot; + str(l)].shape == (layers_dims[l], 1)

    return parameters
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;h3 style=&#34;font-family: Georgia; font-size:1.3em;color:purple; font-style:bold&#34;&gt;
Activation Functions
&lt;/h3&gt;&lt;br&gt;
There is no definitive guide for which activation function works best on specific problems. It&#39;s a trial and error process where one should try different set of functions and see which one works best on the problem at hand. We&#39;ll cover 4 of the most commonly used activation functions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sigmoid function ($\sigma$)&lt;/strong&gt;: $g(z) = \frac{1}{1 + e^{-z}}$. It&#39;s recommended to be used only on the output layer so that we can easily interpret the output as probabilities since it has restricted output between 0 and 1. One of the main disadvantages for using sigmoid function on hidden layers is that the gradient is very close to zero over a large portion of its domain which makes it slow and harder for the learning algorithm to learn.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hyperbolic Tangent function&lt;/strong&gt;: $g(z) = \frac{e^z - e&lt;sup&gt;{-z}}{e&lt;/sup&gt;z + e^{-z}}$. It&#39;s superior to sigmoid function in which the mean of its output is very close to zero, which in other words center the output of the activation units around zero and make the range of values very small which means faster to learn. The disadvantage that it shares with sigmoid function is that the gradient is very small on good portion of the domain.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rectified Linear Unit (ReLU)&lt;/strong&gt;: $g(z) = max(0, z)$. The models that are close to linear are easy to optimize. Since ReLU shares a lot of the properties of linear functions, it tends to work well on most of the problems. The only issue is that the derivative is not defined at $z = 0$, which we can overcome by assigning the derivative to 0 at $z = 0$. However, this means that for $z\leq 0$ the gradient is zero and again can&#39;t learn.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Leaky Rectified Linear Unit&lt;/strong&gt;: $g(z) = max(\alpha*z, z)$. It overcomes the zero gradient issue from ReLU and assigns $\alpha$ which is a small value for $z\leq 0$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you&#39;re not sure which activation function to choose, start with ReLU.
Next, we&#39;ll implement the above activation functions and draw a graph for each one to make it easier to see the domain and range of each function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define activation functions that will be used in forward propagation
def sigmoid(Z):
    A = 1 / (1 + np.exp(-Z))
    return A, Z


def tanh(Z):
    A = np.tanh(Z)
    return A, Z


def relu(Z):
    A = np.maximum(0, Z)
    return A, Z


def leaky_relu(Z):
    A = np.maximum(0.1 * Z, Z)

    return A, Z
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot the 4 activation functions
z = np.linspace(-10, 10, 100)

# Computes post-activation outputs
A_sigmoid, z = sigmoid(z)
A_tanh, z = tanh(z)
A_relu, z = relu(z)
A_leaky_relu, z = leaky_relu(z)

# Plot sigmoid
plt.figure(figsize=(12, 8))
plt.subplot(2, 2, 1)
plt.plot(z, A_sigmoid, label = &amp;quot;Function&amp;quot;)
plt.plot(z, A_sigmoid * (1 - A_sigmoid), label = &amp;quot;Derivative&amp;quot;)
plt.legend(loc = &amp;quot;upper left&amp;quot;)
plt.xlabel(&amp;quot;z&amp;quot;)
plt.ylabel(r&amp;quot;$\frac{1}{1 + e^{-z}}$&amp;quot;)
plt.title(&amp;quot;Sigmoid Function&amp;quot;, fontsize = 16)
# Plot tanh
plt.subplot(2, 2, 2)
plt.plot(z, A_tanh, &#39;b&#39;, label = &amp;quot;Function&amp;quot;)
plt.plot(z, 1 - np.square(A_tanh), &#39;r&#39;,label = &amp;quot;Derivative&amp;quot;)
plt.legend(loc = &amp;quot;upper left&amp;quot;)
plt.xlabel(&amp;quot;z&amp;quot;)
plt.ylabel(r&amp;quot;$\frac{e^z - e^{-z}}{e^z + e^{-z}}$&amp;quot;)
plt.title(&amp;quot;Hyperbolic Tangent Function&amp;quot;, fontsize = 16)
# plot relu
plt.subplot(2, 2, 3)
plt.plot(z, A_relu, &#39;g&#39;)
plt.xlabel(&amp;quot;z&amp;quot;)
plt.ylabel(r&amp;quot;$max\{0, z\}$&amp;quot;)
plt.title(&amp;quot;ReLU Function&amp;quot;, fontsize = 16)
# plot leaky relu
plt.subplot(2, 2, 4)
plt.plot(z, A_leaky_relu, &#39;y&#39;)
plt.xlabel(&amp;quot;z&amp;quot;)
plt.ylabel(r&amp;quot;$max\{0.1z, z\}$&amp;quot;)
plt.title(&amp;quot;Leaky ReLU Function&amp;quot;, fontsize = 16)
plt.tight_layout();
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/activation_fns.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/activation_fns.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Activation functions and their derivatives.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;&lt;h3 style=&#34;font-family: Georgia; font-size:1.3em;color:purple; font-style:bold&#34;&gt;
Feed Forward
&lt;/h3&gt;&lt;br&gt;
Given its inputs from previous layer, each unit computes affine transformation $z = W^Tx + b$ and then apply an activation function $g(z)$ such as ReLU element-wise. During the process, we&#39;ll store (cache) all variables computed and used on each layer to be used in back-propagation. We&#39;ll write first two helper functions that will be used in the L-model forward propagation to make it easier to debug. Keep in mind that on each layer, we may have different activation function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define helper functions that will be used in L-model forward prop
def linear_forward(A_prev, W, b):
    Z = np.dot(W, A_prev) + b
    cache = (A_prev, W, b)

    return Z, cache


def linear_activation_forward(A_prev, W, b, activation_fn):
    assert activation_fn == &amp;quot;sigmoid&amp;quot; or activation_fn == &amp;quot;tanh&amp;quot; or \
        activation_fn == &amp;quot;relu&amp;quot;

    if activation_fn == &amp;quot;sigmoid&amp;quot;:
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = sigmoid(Z)

    elif activation_fn == &amp;quot;tanh&amp;quot;:
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = tanh(Z)

    elif activation_fn == &amp;quot;relu&amp;quot;:
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = relu(Z)

    assert A.shape == (W.shape[0], A_prev.shape[1])

    cache = (linear_cache, activation_cache)

    return A, cache


def L_model_forward(X, parameters, hidden_layers_activation_fn=&amp;quot;relu&amp;quot;):
    A = X                           
    caches = []                     
    L = len(parameters) // 2        

    for l in range(1, L):
        A_prev = A
        A, cache = linear_activation_forward(
            A_prev, parameters[&amp;quot;W&amp;quot; + str(l)], parameters[&amp;quot;b&amp;quot; + str(l)],
            activation_fn=hidden_layers_activation_fn)
        caches.append(cache)

    AL, cache = linear_activation_forward(
        A, parameters[&amp;quot;W&amp;quot; + str(L)], parameters[&amp;quot;b&amp;quot; + str(L)],
        activation_fn=&amp;quot;sigmoid&amp;quot;)
    caches.append(cache)

    assert AL.shape == (1, X.shape[1])

    return AL, caches
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;h3 style=&#34;font-family: Georgia; font-size:1.3em;color:purple; font-style:bold&#34;&gt;
Cost
&lt;/h3&gt;&lt;br&gt;
We&#39;ll use the binary &lt;strong&gt;Cross-Entropy&lt;/strong&gt; cost. It uses the log-likelihood method to estimate its error. The cost is:
&lt;span  class=&#34;math&#34;&gt;\(J(W, b) = -\frac{1}{m}\sum_{i = 1}^m\big(y^ilog(\widehat{y^i}) + (1 - y^i)log(1 - \widehat{y^i}\big)\tag3\)&lt;/span&gt;
The above cost function is convex; however, neural network usually stuck on a local minimum and is not guaranteed to find the optimal parameters. We&#39;ll use here gradient-based learning.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute cross-entropy cost
def compute_cost(AL, y):
    m = y.shape[1]              
    cost = - (1 / m) * np.sum(
        np.multiply(y, np.log(AL)) + np.multiply(1 - y, np.log(1 - AL)))

    return cost
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Back-Propagation
&lt;/h2&gt;

&lt;p&gt;Allows the information to go back from the cost backward through the network in order to compute the gradient. Therefore, loop over the nodes starting at the final node in reverse topological order to compute the derivative of the final node output with respect to each edge&#39;s node tail. Doing so will help us know who is responsible for the most error and change the parameters in that direction. The following derivatives&#39; formulas will help us write the back-propagate functions:
&lt;span  class=&#34;math&#34;&gt;\(dA^L = \frac{A^L - Y}{A^L(1 - A^L)}\tag4\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(dZ^L = A^L - Y\tag5\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(dW^l = \frac{1}{m}dZ^l{A^{l - 1}}^T\tag6\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(db^l = \frac{1}{m}\sum_i(dZ^l)\tag7\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(dA^{l - 1} = {W^l}^TdZ^l\tag8\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(dZ^{l} = dA^l*g^{&#39;l}(Z^l)\tag9\)&lt;/span&gt;
Since $b^l$ is always a vector, the sum would be across rows (since each column is an example).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define derivative of activation functions w.r.t z that will be used in back-propagation
def sigmoid_gradient(dA, Z):
    A, Z = sigmoid(Z)
    dZ = dA * A * (1 - A)

    return dZ


def tanh_gradient(dA, Z):
    A, Z = tanh(Z)
    dZ = dA * (1 - np.square(A))

    return dZ


def relu_gradient(dA, Z):
    A, Z = relu(Z)
    dZ = np.multiply(dA, np.int64(A &amp;gt; 0))

    return dZ


# define helper functions that will be used in L-model back-prop
def linear_backword(dZ, cache):
    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = (1 / m) * np.dot(dZ, A_prev.T)
    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)
    dA_prev = np.dot(W.T, dZ)

    assert dA_prev.shape == A_prev.shape
    assert dW.shape == W.shape
    assert db.shape == b.shape

    return dA_prev, dW, db


def linear_activation_backward(dA, cache, activation_fn):
    linear_cache, activation_cache = cache

    if activation_fn == &amp;quot;sigmoid&amp;quot;:
        dZ = sigmoid_gradient(dA, activation_cache)
        dA_prev, dW, db = linear_backword(dZ, linear_cache)

    elif activation_fn == &amp;quot;tanh&amp;quot;:
        dZ = tanh_gradient(dA, activation_cache)
        dA_prev, dW, db = linear_backword(dZ, linear_cache)

    elif activation_fn == &amp;quot;relu&amp;quot;:
        dZ = relu_gradient(dA, activation_cache)
        dA_prev, dW, db = linear_backword(dZ, linear_cache)

    return dA_prev, dW, db


def L_model_backward(AL, y, caches, hidden_layers_activation_fn=&amp;quot;relu&amp;quot;):
    y = y.reshape(AL.shape)
    L = len(caches)
    grads = {}

    dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))

    grads[&amp;quot;dA&amp;quot; + str(L - 1)], grads[&amp;quot;dW&amp;quot; + str(L)], grads[
        &amp;quot;db&amp;quot; + str(L)] = linear_activation_backward(
            dAL, caches[L - 1], &amp;quot;sigmoid&amp;quot;)

    for l in range(L - 1, 0, -1):
        current_cache = caches[l - 1]
        grads[&amp;quot;dA&amp;quot; + str(l - 1)], grads[&amp;quot;dW&amp;quot; + str(l)], grads[
            &amp;quot;db&amp;quot; + str(l)] = linear_activation_backward(
                grads[&amp;quot;dA&amp;quot; + str(l)], current_cache,
                hidden_layers_activation_fn)

    return grads


# define the function to update both weight matrices and bias vectors
def update_parameters(parameters, grads, learning_rate):
    L = len(parameters) // 2

    for l in range(1, L + 1):
        parameters[&amp;quot;W&amp;quot; + str(l)] = parameters[
            &amp;quot;W&amp;quot; + str(l)] - learning_rate * grads[&amp;quot;dW&amp;quot; + str(l)]
        parameters[&amp;quot;b&amp;quot; + str(l)] = parameters[
            &amp;quot;b&amp;quot; + str(l)] - learning_rate * grads[&amp;quot;db&amp;quot; + str(l)]

    return parameters
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#34;Application&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
II. Application
&lt;/h2&gt;

&lt;p&gt;The dataset that we&#39;ll be working on has 209 images. Each image is 64 x 64 pixels on RGB scale. We&#39;ll build a neural network to classify if the image has a cat or not. Therefore, $y^i \in {0, 1}.$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We&#39;ll first load the images.&lt;/li&gt;
&lt;li&gt;Show sample image for a cat.&lt;/li&gt;
&lt;li&gt;Reshape input matrix so that each column would be one example. Also, since each image is 64 x 64 x 3, we&#39;ll end up having 12,288 features for each image. Therefore, the input matrix would be 12,288 x 209.&lt;/li&gt;
&lt;li&gt;Standardize the data so that the gradients don&#39;t go out of control. Also, it will help hidden units have similar range of values. For now, we&#39;ll divide every pixel by 255 which shouldn&#39;t be an issue. However, it&#39;s better to standardize the data to have a mean of 0 and a standard deviation of 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import training dataset
train_dataset = h5py.File(&amp;quot;../data/train_catvnoncat.h5&amp;quot;)
X_train = np.array(train_dataset[&amp;quot;train_set_x&amp;quot;])
y_train = np.array(train_dataset[&amp;quot;train_set_y&amp;quot;])

test_dataset = h5py.File(&amp;quot;../data/test_catvnoncat.h5&amp;quot;)
X_test = np.array(test_dataset[&amp;quot;test_set_x&amp;quot;])
y_test = np.array(test_dataset[&amp;quot;test_set_y&amp;quot;])

# print the shape of input data and label vector
print(f&amp;quot;&amp;quot;&amp;quot;Original dimensions:\n{20 * &#39;-&#39;}\nTraining: {X_train.shape}, {y_train.shape}
Test: {X_test.shape}, {y_test.shape}&amp;quot;&amp;quot;&amp;quot;)

# plot cat image
plt.figure(figsize=(6, 6))
plt.imshow(X_train[50])
plt.axis(&amp;quot;off&amp;quot;);

# Transform input data and label vector
X_train = X_train.reshape(209, -1).T
y_train = y_train.reshape(-1, 209)

X_test = X_test.reshape(50, -1).T
y_test = y_test.reshape(-1, 50)

# standarize the data
X_train = X_train / 255
X_test = X_test / 255

print(f&amp;quot;&amp;quot;&amp;quot;\nNew dimensions:\n{15 * &#39;-&#39;}\nTraining: {X_train.shape}, {y_train.shape}
Test: {X_test.shape}, {y_test.shape}&amp;quot;&amp;quot;&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Original dimensions:
--------------------
Training: (209, 64, 64, 3), (209,)
Test: (50, 64, 64, 3), (50,)

New dimensions:
---------------
Training: (12288, 209), (1, 209)
Test: (12288, 50), (1, 50)
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/cat_sample.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/cat_sample.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Sample image.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Now, our dataset is ready to be used and test our neural network implementation. Let&#39;s first write &lt;strong&gt;multi-layer model&lt;/strong&gt; function to implement gradient-based learning using predefined number of iterations and learning rate.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define the multi-layer model using all the helper functions we wrote before
def L_layer_model(
        X, y, layers_dims, learning_rate=0.01, num_iterations=3000,
        print_cost=True, hidden_layers_activation_fn=&amp;quot;relu&amp;quot;):
    np.random.seed(1)

    # initialize parameters
    parameters = initialize_parameters(layers_dims)

    # intialize cost list
    cost_list = []

    # iterate over num_iterations
    for i in range(num_iterations):
        # iterate over L-layers to get the final output and the cache
        AL, caches = L_model_forward(
            X, parameters, hidden_layers_activation_fn)

        # compute cost to plot it
        cost = compute_cost(AL, y)

        # iterate over L-layers backward to get gradients
        grads = L_model_backward(AL, y, caches, hidden_layers_activation_fn)

        # update parameters
        parameters = update_parameters(parameters, grads, learning_rate)

        # append each 100th cost to the cost list
        if (i + 1) % 100 == 0 and print_cost:
            print(f&amp;quot;The cost after {i + 1} iterations is: {cost:.4f}&amp;quot;)

        if i % 100 == 0:
            cost_list.append(cost)

    # plot the cost curve
    plt.figure(figsize=(10, 6))
    plt.plot(cost_list)
    plt.xlabel(&amp;quot;Iterations (per hundreds)&amp;quot;)
    plt.ylabel(&amp;quot;Loss&amp;quot;)
    plt.title(f&amp;quot;Loss curve for the learning rate = {learning_rate}&amp;quot;)

    return parameters


def accuracy(X, parameters, y, activation_fn=&amp;quot;relu&amp;quot;):
    probs, caches = L_model_forward(X, parameters, activation_fn)
    labels = (probs &amp;gt;= 0.5) * 1
    accuracy = np.mean(labels == y) * 100

    return f&amp;quot;The accuracy rate is: {accuracy:.2f}%.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we&#39;ll train two versions of the neural network where each one will use different activation function on hidden layers: One will use rectified linear unit (&lt;strong&gt;ReLU&lt;/strong&gt;) and the second one will use hyperbolic tangent function (&lt;strong&gt;tanh&lt;/strong&gt;). Finally we&#39;ll use the parameters we get from both neural networks to classify training examples and compute the training accuracy rates for each version to see which activation function works best on this problem.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Setting layers dims
layers_dims = [X_train.shape[0], 5, 5, 1]

# NN with tanh activation fn
parameters_tanh = L_layer_model(
    X_train, y_train, layers_dims, learning_rate=0.03, num_iterations=3000,
    hidden_layers_activation_fn=&amp;quot;tanh&amp;quot;)

# Print the accuracy
accuracy(X_test, parameters_tanh, y_test, activation_fn=&amp;quot;tanh&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;The cost after 100 iterations is: 0.6556
The cost after 200 iterations is: 0.6468
The cost after 300 iterations is: 0.6447
The cost after 400 iterations is: 0.6441
The cost after 500 iterations is: 0.6440
The cost after 600 iterations is: 0.6440
The cost after 700 iterations is: 0.6440
The cost after 800 iterations is: 0.6439
The cost after 900 iterations is: 0.6439
The cost after 1000 iterations is: 0.6439
The cost after 1100 iterations is: 0.6439
The cost after 1200 iterations is: 0.6439
The cost after 1300 iterations is: 0.6438
The cost after 1400 iterations is: 0.6438
The cost after 1500 iterations is: 0.6437
The cost after 1600 iterations is: 0.6434
The cost after 1700 iterations is: 0.6429
The cost after 1800 iterations is: 0.6413
The cost after 1900 iterations is: 0.6361
The cost after 2000 iterations is: 0.6124
The cost after 2100 iterations is: 0.5112
The cost after 2200 iterations is: 0.5288
The cost after 2300 iterations is: 0.4312
The cost after 2400 iterations is: 0.3821
The cost after 2500 iterations is: 0.3387
The cost after 2600 iterations is: 0.2349
The cost after 2700 iterations is: 0.2206
The cost after 2800 iterations is: 0.1927
The cost after 2900 iterations is: 0.4669
The cost after 3000 iterations is: 0.1040

&#39;The accuracy rate is: 68.00%.&#39;
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/loss_tanh.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/loss_tanh.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Loss curve with tanh activation function.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# NN with relu activation fn
parameters_relu = L_layer_model(
    X_train, y_train, layers_dims, learning_rate=0.03, num_iterations=3000,
    hidden_layers_activation_fn=&amp;quot;relu&amp;quot;)

# Print the accuracy
accuracy(X_test, parameters_relu, y_test, activation_fn=&amp;quot;relu&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;The cost after 100 iterations is: 0.6556
The cost after 200 iterations is: 0.6468
The cost after 300 iterations is: 0.6447
The cost after 400 iterations is: 0.6441
The cost after 500 iterations is: 0.6440
The cost after 600 iterations is: 0.6440
The cost after 700 iterations is: 0.6440
The cost after 800 iterations is: 0.6440
The cost after 900 iterations is: 0.6440
The cost after 1000 iterations is: 0.6440
The cost after 1100 iterations is: 0.6439
The cost after 1200 iterations is: 0.6439
The cost after 1300 iterations is: 0.6439
The cost after 1400 iterations is: 0.6439
The cost after 1500 iterations is: 0.6439
The cost after 1600 iterations is: 0.6439
The cost after 1700 iterations is: 0.6438
The cost after 1800 iterations is: 0.6437
The cost after 1900 iterations is: 0.6435
The cost after 2000 iterations is: 0.6432
The cost after 2100 iterations is: 0.6423
The cost after 2200 iterations is: 0.6395
The cost after 2300 iterations is: 0.6259
The cost after 2400 iterations is: 0.5408
The cost after 2500 iterations is: 0.5262
The cost after 2600 iterations is: 0.4727
The cost after 2700 iterations is: 0.4386
The cost after 2800 iterations is: 0.3493
The cost after 2900 iterations is: 0.1877
The cost after 3000 iterations is: 0.3641

&#39;The accuracy rate is: 42.00%.&#39;
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/loss_relu.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/coding-nn-from-scratch/loss_relu.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Loss curve with ReLU activation function.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Please note that the accuracy rates above are expected to overestimate the generalization accuracy rates.&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Conclusion
&lt;/h2&gt;

&lt;p&gt;The purpose of this post is to code Deep Neural Network step-by-step and explain the important concepts while doing that. We don&#39;t really care about the accuracy rate at this moment since there are tons of things we could&#39;ve done to increase the accuracy which would be the subject of following posts. Below are some takeaways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Even if neural network can represent any function, it may fail to learn for two reasons:

&lt;ol&gt;
&lt;li&gt;The optimization algorithm may fail to find the best value for the parameters of the desired (true) function.
It can stuck in a local optimum.&lt;/li&gt;
&lt;li&gt;The learning algorithm may find different functional form that is different than the intended function due to overfitting.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Even if neural network rarely converges and always stuck in a local minimum, it is still able to reduce the cost significantly and come up with very complex models with high test accuracy.&lt;/li&gt;
&lt;li&gt;The neural network we used in this post is standard fully connected network. However, there are two other kinds of networks:

&lt;ul&gt;
&lt;li&gt;Convolutional NN: Where not all nodes are connected. It&#39;s best in class for image recognition.&lt;/li&gt;
&lt;li&gt;Recurrent NN: There is a feedback connections where output of the model is fed back into itself. It&#39;s used mainly in sequence modeling.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The fully connected neural network also forgets what happened in previous steps and also doesn&#39;t know anything about the output.&lt;/li&gt;
&lt;li&gt;There are number of hyperparameters that we can tune using cross validation to get the best performance of our network:

&lt;ol&gt;
&lt;li&gt;Learning rate ($\alpha$): Determines how big the step for each update of parameters.

&lt;ul&gt;
&lt;li&gt;Small $\alpha$ leads to slow convergence and may become computationally very expensive.&lt;/li&gt;
&lt;li&gt;Large $\alpha$ may lead to overshooting where our learning algorithm may never converge.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Number of hidden layers (depth): The more hidden layers the better, but comes at a cost computationally.&lt;/li&gt;
&lt;li&gt;Number of units per hidden layer (width): Research proven that huge number of hidden units per layer doesn&#39;t add to the improvement of the network.&lt;/li&gt;
&lt;li&gt;Activation function: Which function to use on hidden layers differs among applications and domains. It&#39;s a trial and error process to try different functions and see which one works best.&lt;/li&gt;
&lt;li&gt;Number of iterations.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Standardize data would help activation units have similar range of values and avoid gradients to go out of control.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>epsilon-Greedy Algorithm</title>
      <link>https://imaddabbura.github.io/post/epsilon-greedy-algorithm/</link>
      <pubDate>Sat, 31 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://imaddabbura.github.io/post/epsilon-greedy-algorithm/</guid>
      <description>&lt;p&gt;A/B testing can be defined as a randomized controlled experiment that allows us to test if there is a causal relationship between a change to a website/app and the user behavior. The change can be visible such as location of a button on the homepage or invisible such as the ranking/recommendation algorithms and backend infrastructure.&lt;/p&gt;

&lt;p&gt;Web/Mobile developers and business stakeholders always face the following dilemma: Should we try out all ideas and explore all options continuously? Or should we exploit the best available option and stick to it?
The answer is, as in most cases, will be a trade-off between the two extremes. If we explore all the time, we&#39;ll collect a lot of data and waste resources in testing inferior ideas and missing sales (e-commerce case). However, if we only exploit the available option and never try new ideas, we would be left behind and loose in the long-term with ever-changing markets.&lt;/p&gt;

&lt;p&gt;In this series, we&#39;ll explore solutions offered by &lt;strong&gt;Multi-armed Bandit Algorithms&lt;/strong&gt; that have two main advantages over traditional A/B testing:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Smoothly decrease exploration over time instead of sudden jumps.&lt;/li&gt;
&lt;li&gt;Focus resources on better options and not keep evaluating inferior options during the life of the experiment.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What is &lt;strong&gt;Bandit Algorithms&lt;/strong&gt;? Bandit Algorithms are algorithms that try to learn a rule of selecting a sequence of options that balance exploring available options and getting enough knowledge about each option and maximize profits by selecting the best option. Note that during the experiment, we only have knowledge about the options we tried. Therefore, every time we select an option that&#39;s not the best one, we incur an opportunity cost of not selecting the best option; however, we also gain a new knowledge (feedback) about the selected option. In other words, we need to have enough feedback about each option to learn the best option. As a result, the best strategy would be to explore more at the beginning of the experiment until we know the best option and then start exploiting that option.&lt;/p&gt;

&lt;p&gt;&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
epsilon-Greedy Algorithm&lt;/h2&gt;&lt;br&gt;
In this notebook, we&#39;ll cover &lt;strong&gt;epsilon-Greedy Algorithm&lt;/strong&gt;. Greedy Algorithm can be defined as the algorithm that picks the best currently available option without taking into consideration the long-term effect of that decision, which may happen to be a suboptimal decision. Given that, we can define epsilon-Greedy Algorithm as a Greedy Algorithm that adds some randomness when deciding between options: Instead of picking always the best available option, randomly explore other options with a probability = ð or pick the best option with a probability = 1 - ð. Therefore, we can add randomness to the algorithm by increasing ð, which will make the algorithm explores other options more frequently. Additionally, ð is a hyper-parameter that needs to be tuned based on the experiment, i.e. there is no value that works best on all experiments.
Let&#39;s explore how the algorithm works assuming we have two options: A and B (we can think of them as Control and Treatment groups). For each new user:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Assume we have a coin that has a probability of coming heads = ð and a probability of coming tails = 1 - ð. Therefore,

&lt;ul&gt;
&lt;li&gt;If it comes heads, explore randomly the available options (exploration).&lt;/li&gt;
&lt;li&gt;The probability of selecting any option is 1/2.&lt;/li&gt;
&lt;li&gt;If it comes tails, select the best option (exploitation).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a result, the probability of selecting any option randomly if we have N options is ð 1/N; however, the probability of selecting the best option is 1 - ð (see figure 1).&lt;/p&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/bandit-algorithms/epsilon_greedy.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/bandit-algorithms/epsilon_greedy.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;epsilon-Greedy Algorithm.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Let&#39;s import the needed packages and implement the algorithm.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import packages
import os
import sys

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Add module path to system path
sys.path.append(os.path.abspath(&amp;quot;../&amp;quot;))
from utils import plot_algorithm, compare_algorithms

%matplotlib inline
plt.style.use(&amp;quot;fivethirtyeight&amp;quot;)
sns.set_context(&amp;quot;notebook&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class EpsilonGreedy:
    def __init__(self, epsilon, counts=None, values=None):
        self.epsilon = epsilon
        self.counts = counts
        self.values = values

    def initialize(self, n_arms):
        self.counts = np.zeros(n_arms, dtype=int)
        self.values = np.zeros(n_arms, dtype=float)

    def select_arm(self):
        z = np.random.random()
        if z &amp;gt; self.epsilon:
            # Pick the best arm
            return np.argmax(self.values)
        # Randomly pick any arm with prob 1 / len(self.counts)
        return np.random.randint(0, len(self.values))

    def update(self, chosen_arm, reward):
        # Increment chosen arm&#39;s count by one
        self.counts[chosen_arm] += 1
        n = self.counts[chosen_arm]

        # Recompute the estimated value of chosen arm using new reward
        value = self.values[chosen_arm]
        new_value = value * ((n - 1) / n) + reward / n
        self.values[chosen_arm] = new_value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Few things to note from the above implementation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Initialization of values (rewards) affect the long term performance of the algorithm.&lt;/li&gt;
&lt;li&gt;The larger the sample size (N), the less influential the rewards from the recent options since we are using the average of each option in the values array.&lt;/li&gt;
&lt;li&gt;Values array will store the estimated values (average) of each option.&lt;/li&gt;
&lt;li&gt;Counts is just an internal counter that keeps track of the number of times we selected each option.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Monte Carlo Simulations&lt;/h2&gt;&lt;br&gt;
In order to evaluate the algorithm, we will use Monte Carlo simulations. We&#39;ll use 5000 simulations to overcome the randomness generated from the random number generator. Also, we&#39;ll use Bernoulli distribution to get the reward from each option on each run. For each simulation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Initialize the algorithm with no prior knowledge.&lt;/li&gt;
&lt;li&gt;Loop over the time horizon:

&lt;ul&gt;
&lt;li&gt;Select the option.&lt;/li&gt;
&lt;li&gt;Draw the reward for the selected option using Bernoulli distribution and the probability defined.&lt;/li&gt;
&lt;li&gt;Update the counts and estimated values of selected arm.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&#39;ll define the % of reward (probability) of each option and test the performance of the algorithm using three different metrics:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Probability of selecting the best option.&lt;/li&gt;
&lt;li&gt;Average rewards. This metric is a better approximation if the options are similar.&lt;/li&gt;
&lt;li&gt;Cumulative rewards. The previous two metrics are not fair metrics for algorithms with large epsilon where they sacrifice by exploring more options; however, cumulative rewards is what we should care about.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Moreover, we&#39;ll evaluate the algorithm using 5 different values of ð: 0.1, 0.2, 0.3, 0.4, 0.5. Since in the literature they use &lt;em&gt;arm&lt;/em&gt; instead of &lt;em&gt;option&lt;/em&gt; for historical reasons, we&#39;ll be using &lt;em&gt;arm&lt;/em&gt; and &lt;em&gt;option&lt;/em&gt; interchangeably.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class BernoulliArm:
    def __init__(self, p):
        self.p = p

    def draw(self):
        z = np.random.random()
        if z &amp;gt; self.p:
            return 0.0
        return 1.0


def test_algorithm(algo, arms, num_simulations, horizon):
    # Initialize rewards and chosen_arms with zero 2d arrays
    chosen_arms = np.zeros((num_simulations, horizon))
    rewards = np.zeros((num_simulations, horizon))

    # Loop over all simulations
    for sim in range(num_simulations):
        # Re-initialize algorithm&#39;s counts and values arrays
        algo.initialize(len(arms))

        # Loop over all time horizon
        for t in range(horizon):
            # Select arm
            chosen_arm = algo.select_arm()
            chosen_arms[sim, t] = chosen_arm

            # Draw from Bernoulli distribution to get rewards
            reward = arms[chosen_arm].draw()
            rewards[sim, t] = reward

            # Update the algorithms&#39; count and estimated values
            algo.update(chosen_arm, reward)

    # Average rewards across all sims and compute cumulative rewards
    average_rewards = np.mean(rewards, axis=0)
    cumulative_rewards = np.cumsum(average_rewards)

    return chosen_arms, average_rewards, cumulative_rewards
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)
# Average reward by arm
means = [0.1, 0.1, 0.1, 0.1, 0.9]
n_arms = len(means)
# Shuffle the arms
np.random.shuffle(means)
# Each arm will follow and Bernoulli distribution
arms = list(map(lambda mu: BernoulliArm(mu), means))
# Get the index of the best arm to test if algorithm will be able to learn that
best_arm_index = np.argmax(means)
# Define epsilon value to check the performance of the algorithm using each one
epsilon = [0.1, 0.2, 0.3, 0.4, 0.5]

# Plot the epsilon-Greedy algorithm
plot_algorithm(alg_name=&amp;quot;epsilon-Greedy&amp;quot;, arms=arms, best_arm_index=best_arm_index,
               hyper_params=epsilon, num_simulations=5000, horizon=500, label=&amp;quot;eps&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/bandit-algorithms/epsilon_greedy_v1.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/bandit-algorithms/epsilon_greedy_v1.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;epsilon-Greedy Algorithm: Vey Different Options.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Few thing to note from the above graphs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Regardless of the epsilon values, all algorithms learned the best option.&lt;/li&gt;
&lt;li&gt;The algorithm picks options randomly; therefore, it&#39;s not guaranteed to always pick the best option even if it found that option. That&#39;s the main reason why none of the algorithms achieved a probability = 1 of selecting the best option or average rewards = % rewards of the best option even after they learned the best option.&lt;/li&gt;
&lt;li&gt;As ð increases --&amp;gt; increase the exploration --&amp;gt; increases the chance of picking options randomly instead of the best option.&lt;/li&gt;
&lt;li&gt;Algorithms with higher epsilon learn quicker but don&#39;t use that knowledge in exploiting the best option.&lt;/li&gt;
&lt;li&gt;Using accuracy in picking the best option and average rewards metrics, the algorithm ð = 0.1 outperforms the rest; however, cumulative rewards metric shows that it takes that algorithm long time to outperform the algorithm with ð = 0.2.&lt;/li&gt;
&lt;li&gt;Depends on time planned to run the experiment, different values of epsilons may be more optimal. For example, ð = 0.2 is the best value for almost anything at or below 400.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&#39;s run the experiment again to see how would the algorithm behave under the following settings:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Only two options.&lt;/li&gt;
&lt;li&gt;50 options.&lt;/li&gt;
&lt;li&gt;5 option that are very similar.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)
# Average reward by arm
means = [0.1, 0.9]
n_arms = len(means)
# Shuffle the arms
np.random.shuffle(means)
# Each arm will follow and Bernoulli distribution
arms = list(map(lambda mu: BernoulliArm(mu), means))
# Get the index of the best arm to test if algorithm will be able to learn that
best_arm_index = np.argmax(means)
# Define epsilon value to check the performance of the algorithm using each one
epsilon = [0.1, 0.2, 0.3, 0.4, 0.5]

# Plot the epsilon-Greedy algorithm
plot_algorithm(alg_name=&amp;quot;epsilon-Greedy&amp;quot;, arms=arms, best_arm_index=best_arm_index,
               hyper_params=epsilon, num_simulations=5000, horizon=500, label=&amp;quot;eps&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/bandit-algorithms/epsilon_greedy_v2.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/bandit-algorithms/epsilon_greedy_v2.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;epsilon-Greedy Algorithm: Two Options.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)
# Average reward by arm
means = [i for i in np.random.random(50)]
n_arms = len(means)
# Shuffle the arms
np.random.shuffle(means)
# Each arm will follow and Bernoulli distribution
arms = list(map(lambda mu: BernoulliArm(mu), means))
# Get the index of the best arm to test if algorithm will be able to learn that
best_arm_index = np.argmax(means)
# Define epsilon value to check the performance of the algorithm using each one
epsilon = [0.1, 0.2, 0.3, 0.4, 0.5]

# Plot the epsilon-Greedy algorithm
plot_algorithm(alg_name=&amp;quot;epsilon-Greedy&amp;quot;, arms=arms, best_arm_index=best_arm_index,
               hyper_params=epsilon, num_simulations=5000, horizon=250, label=&amp;quot;eps&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/bandit-algorithms/epsilon_greedy_v3.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/bandit-algorithms/epsilon_greedy_v3.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;epsilon-Greedy Algorithm: Large Number of Options.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)
# Average reward by arm
means = [0.2, 0.18, 0.22, 0.19, 0.21]
n_arms = len(means)
# Shuffle the arms
np.random.shuffle(means)
# Each arm will follow and Bernoulli distribution
arms = list(map(lambda mu: BernoulliArm(mu), means))
# Get the index of the best arm to test if algorithm will be able to learn that
best_arm_index = np.argmax(means)
# Define epsilon value to check the performance of the algorithm using each one
epsilon = [0.1, 0.2, 0.3, 0.4, 0.5]

# Plot the epsilon-Greedy algorithm
plot_algorithm(alg_name=&amp;quot;epsilon-Greedy&amp;quot;, arms=arms, best_arm_index=best_arm_index,
               hyper_params=epsilon, num_simulations=5000, horizon=500, label=&amp;quot;eps&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/bandit-algorithms/epsilon_greedy_v4.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/bandit-algorithms/epsilon_greedy_v4.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;epsilon-Greedy Algorithm: Very Similar Options.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;ul&gt;
&lt;li&gt;When we had lower number of options, all algorithms were faster at learning the best option which can be seen by the steepness of all curves of the first two graphs when time &amp;lt; 100. As a result, all algorithms had higher cumulative rewards than when we had 5 options.&lt;/li&gt;
&lt;li&gt;Having large number of options made it hard on all algorithms to learn the best option and may need a lot more time to figure it out.&lt;/li&gt;
&lt;li&gt;Lastly, when options are very similar (in terms of rewards), the probability of selecting the best option by all algorithms decreases over time. Let&#39;s take the algorithm with ð = 0.1 and see why is this the case. After some investigation, the algorithm was struggling in differentiating between the best option and the second best option since the difference between the % rewards is 1%. Therefore, the probability of selecting the best arm was around 50%.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Annealing epsilon-Greedy Algorithm&lt;/h2&gt;&lt;br&gt;
Epsilon value plays a major role in the performance of epsilon-Greedy algorithm and has to be tuned to the best of our knowledge in terms of the expectations of the estimated rewards of each option. Nonetheless, this estimation suffers from high uncertainty since most of the times either we have no clue what might work or the results would be against our intuition as user experience research has shown in multiple studies. Therefore, isn&#39;t it nice if we can avoid setting up the epsilon values and make the algorithm parameter-free? That&#39;s what &lt;strong&gt;Annealing epsilon-Greedy Algorithm&lt;/strong&gt; does. We specify the rule of decaying epsilon with time and let the algorithm runs with no hyper-parameter configurations. The rule of we will use here is: ð = 1/&lt;em&gt;log(time + 0.0000001)&lt;/em&gt;. As we can see, at the beginning of the experiment, ð would be close to Inf and that means a lot of exploration; however, as time goes, ð start approaching zero and the algorithm would exploit more and more by selecting the best option.&lt;/p&gt;

&lt;p&gt;We will evaluate the Annealed version using the same settings as before and compare it to standard version.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class AnnealingEpsilonGreedy(EpsilonGreedy):
    def __init__(self, counts=None, values=None):
        self.counts = counts
        self.values = values

    def select_arm(self):
        # Epsilon decay schedule
        t = np.sum(self.counts) + 1
        epsilon = 1 / np.log(t + 0.0000001)

        z = np.random.random()
        if z &amp;gt; epsilon:
            # Pick the best arm
            return np.argmax(self.values)
        # Randomly pick any arm with prob 1 / len(self.counts)
        return np.random.randint(0, len(self.values))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed(1)
# Average reward by arm
means = [0.1, 0.1, 0.1, 0.1, 0.9]
n_arms = len(means)
# Shuffle the arms
np.random.shuffle(means)
# Each arm will follow and Bernoulli distribution
arms = list(map(lambda mu: BernoulliArm(mu), means))
# Get the index of the best arm to test if algorithm will be able to learn that
best_arm_index = np.argmax(means)

# Plot the epsilon-Greedy algorithm
plot_algorithm(alg_name=&amp;quot;Annealing epsilon-Greedy&amp;quot;, arms=arms, best_arm_index=best_arm_index,
               num_simulations=5000, horizon=500)
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/bandit-algorithms/annealing_epsilon_greedy.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/bandit-algorithms/annealing_epsilon_greedy.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Annealing epsilon-Greedy Algorithm.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Even though the accuracy of selecting the best option and the average rewards of the annealing epsilon-Greedy Algorithm is lower than the standard version, it has higher cumulative rewards. Also, since the real world is uncertain and we may not have any clue about the designed options, it may be preferred to use the annealing version under some scenarios.&lt;/p&gt;

&lt;p&gt;&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold\&#34;&gt;
Conclusion&lt;/h2&gt;&lt;br&gt;
epsilon-Greedy Algorithm works by going back and forth between exploration with probability = ð and exploitation with probability 1 - ð. Below are some takeaways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Setting the value of epsilon:

&lt;ul&gt;
&lt;li&gt;If we set ð = 1, we would only explore the available options with a probability = 1/N of selecting any option. This will enable us to explore a lot of ideas at the expense of wasting resources by evaluating inferior options.&lt;/li&gt;
&lt;li&gt;If we set ð = 0, we would exploit the best option and never explore any new idea. This strategy would leave up behind our competitors given that the markets are so volatile.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Exploration should be high at the beginning of the experiment to gain the knowledge about all the available options. It should decrease as a function of time where at some point after having enough data about all options, the algorithm should focus on exploiting the best option.&lt;/li&gt;
&lt;li&gt;All algorithms with different epsilon values learned the best option; however, they differ by the level of randomness of each algorithm in keep randomly exploring available options.&lt;/li&gt;
&lt;li&gt;To get the best results of any Bandit algorithm, we should have a lot of data, which means to run the experiment longer in most cases.&lt;/li&gt;
&lt;li&gt;For experiments that run for short period of time, traditional A/B testing may be better.&lt;/li&gt;
&lt;li&gt;Initialization of estimated rewards can affect the long-term performance of the algorithm. As a result, we may need to use previous experience and intuition to guide our initial values.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Source code that created this post can be found &lt;a href=&#34;https://nbviewer.jupyter.org/github/ImadDabbura/blog-posts/blob/master/notebooks/epsilon-Greedy-Algorithm.ipynb&#34;&gt;here&lt;/a&gt;.
The repo that hosts all Bandit Algorithms&#39; implementations and testing frameworks can be found &lt;a href=&#34;https://github.com/ImadDabbura/bandit-algorithms&#34;&gt;here&lt;/a&gt;.
The post is inspired by &lt;a href=&#34;http://shop.oreilly.com/product/0636920027393.do&#34;&gt;Bandit Algorithms for Website Optimization&lt;/a&gt; book.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting Loan Repayment</title>
      <link>https://imaddabbura.github.io/post/pred-loan-repayment/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://imaddabbura.github.io/post/pred-loan-repayment/</guid>
      <description>&lt;p&gt;&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Introduction&lt;/h2&gt;&lt;br&gt;
The two most critical questions in the lending industry are: 1) How risky is the borrower? 2) Given the borrower&#39;s risk, should we lend him/her? The answer to the first question determines the interest rate the borrower would have. Interest rate measures among other things (such as time value of money) the riskness of the borrower, i.e. the riskier the borrower, the higher the interest rate. With interest rate in mind, we can then determine if the borrower is eligible for the loan.&lt;/p&gt;

&lt;p&gt;Investors (lenders) provide loans to borrowers in exchange for the promise of repayment with interest. That means the lender only makes profit (interest) if the borrower pays off the loan. However, if he/she doesn&#39;t repay the loan, then the lender loses money.&lt;/p&gt;

&lt;p&gt;We&#39;ll be using publicly available data from &lt;a href=&#34;https://www.lendingclub.com/info/download-data.action&#34;&gt;LendingClub.com&lt;/a&gt;. The data covers the 9,578 loans funded by the platform between May 2007 and February 2010. The interest rate is provided to us for each borrower. Therefore, so we&#39;ll address the second question indirectly by trying to predict if the borrower will repay the loan by its mature date or not. Through this excerise we&#39;ll illustrate three modeling concepts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What to do with missing values.&lt;/li&gt;
&lt;li&gt;Techniques used with imbalanced classification problems.&lt;/li&gt;
&lt;li&gt;Illustrate how to build an ensemble model using two methods: blending and stacking, which most likely gives us a boost in performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below is a short description of each feature in the data set:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;credit_policy&lt;/strong&gt;: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;purpose&lt;/strong&gt;: The purpose of the loan such as: credit_card, debt_consolidation, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;int_rate&lt;/strong&gt;: The interest rate of the loan (proportion).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;installment&lt;/strong&gt;: The monthly installments (\$) owed by the borrower if the loan is funded.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;log_annual_inc&lt;/strong&gt;: The natural log of the annual income of the borrower.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;dti&lt;/strong&gt;: The debt-to-income ratio of the borrower.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fico&lt;/strong&gt;: The FICO credit score of the borrower.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;days_with_cr_line&lt;/strong&gt;: The number of days the borrower has had a credit line.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;revol_bal&lt;/strong&gt;: The borrower&#39;s revolving balance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;revol_util&lt;/strong&gt;: The borrower&#39;s revolving line utilization rate.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;inq_last_6mths&lt;/strong&gt;: The borrower&#39;s number of inquiries by creditors in the last 6 months.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;delinq_2yrs&lt;/strong&gt;: The number of times the borrower had been 30+ days past due on a payment in the past 2 years.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pub_rec&lt;/strong&gt;: The borrower&#39;s number of derogatory public records.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;not_fully_paid&lt;/strong&gt;: indicates whether the loan was not paid back in full (the borrower either defaulted or the borrower was deemed unlikely to pay it back).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&#39;s load the data and check:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Data types of each feature&lt;/li&gt;
&lt;li&gt;If we have missing values&lt;/li&gt;
&lt;li&gt;If we have imbalanced data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Source code that created this post can be found &lt;a href=&#34;https://nbviewer.jupyter.org/github/ImadDabbura/blog-posts/blob/master/notebooks/Predicting-Loan-Repayment.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import fancyimpute
from imblearn.pipeline import make_pipeline as imb_make_pipeline
from imblearn.over_sampling import RandomOverSampler, SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.ensemble import BalancedBaggingClassifier, EasyEnsemble
from mlens.visualization import corrmat
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from sklearn.preprocessing import Imputer, RobustScaler, FunctionTransformer
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier
from sklearn.ensemble.partial_dependence import plot_partial_dependence
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (roc_auc_score, confusion_matrix,
                             accuracy_score, roc_curve,
                             precision_recall_curve, f1_score)
from sklearn.pipeline import make_pipeline
import xgboost as xgb
from keras import models, layers, optimizers

os.chdir(&amp;quot;../&amp;quot;)
from scripts.plot_roc import plot_roc_and_pr_curves
os.chdir(&amp;quot;notebooks/&amp;quot;)

%matplotlib inline
plt.style.use(&amp;quot;fivethirtyeight&amp;quot;)
sns.set_context(&amp;quot;notebook&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load the data
df = pd.read_csv(&amp;quot;../data/loans.csv&amp;quot;)

# Check both the datatypes and if there is missing values
print(f&amp;quot;Data types:\n{11 * &#39;-&#39;}&amp;quot;)
print(f&amp;quot;{df.dtypes}\n&amp;quot;)
print(f&amp;quot;Sum of null values in each feature:\n{35 * &#39;-&#39;}&amp;quot;)
print(f&amp;quot;{df.isnull().sum()}&amp;quot;)
df.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Data types:
-----------
[30mcredit_policy          int64
purpose               object
int_rate             float64
installment          float64
log_annual_inc       float64
dti                  float64
fico                   int64
days_with_cr_line    float64
revol_bal              int64
revol_util           float64
inq_last_6mths       float64
delinq_2yrs          float64
pub_rec              float64
not_fully_paid         int64
dtype: object

Sum of null values in each feature:
-----------------------------------
[30mcredit_policy         0
purpose               0
int_rate              0
installment           0
log_annual_inc        4
dti                   0
fico                  0
days_with_cr_line    29
revol_bal             0
revol_util           62
inq_last_6mths       29
delinq_2yrs          29
pub_rec              29
not_fully_paid        0
dtype: int64
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style&gt;
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;credit_policy&lt;/th&gt;
      &lt;th&gt;purpose&lt;/th&gt;
      &lt;th&gt;int_rate&lt;/th&gt;
      &lt;th&gt;installment&lt;/th&gt;
      &lt;th&gt;log_annual_inc&lt;/th&gt;
      &lt;th&gt;dti&lt;/th&gt;
      &lt;th&gt;fico&lt;/th&gt;
      &lt;th&gt;days_with_cr_line&lt;/th&gt;
      &lt;th&gt;revol_bal&lt;/th&gt;
      &lt;th&gt;revol_util&lt;/th&gt;
      &lt;th&gt;inq_last_6mths&lt;/th&gt;
      &lt;th&gt;delinq_2yrs&lt;/th&gt;
      &lt;th&gt;pub_rec&lt;/th&gt;
      &lt;th&gt;not_fully_paid&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;debt_consolidation&lt;/td&gt;
      &lt;td&gt;0.1189&lt;/td&gt;
      &lt;td&gt;829.10&lt;/td&gt;
      &lt;td&gt;11.350407&lt;/td&gt;
      &lt;td&gt;19.48&lt;/td&gt;
      &lt;td&gt;737&lt;/td&gt;
      &lt;td&gt;5639.958333&lt;/td&gt;
      &lt;td&gt;28854&lt;/td&gt;
      &lt;td&gt;52.1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;credit_card&lt;/td&gt;
      &lt;td&gt;0.1071&lt;/td&gt;
      &lt;td&gt;228.22&lt;/td&gt;
      &lt;td&gt;11.082143&lt;/td&gt;
      &lt;td&gt;14.29&lt;/td&gt;
      &lt;td&gt;707&lt;/td&gt;
      &lt;td&gt;2760.000000&lt;/td&gt;
      &lt;td&gt;33623&lt;/td&gt;
      &lt;td&gt;76.7&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;debt_consolidation&lt;/td&gt;
      &lt;td&gt;0.1357&lt;/td&gt;
      &lt;td&gt;366.86&lt;/td&gt;
      &lt;td&gt;10.373491&lt;/td&gt;
      &lt;td&gt;11.63&lt;/td&gt;
      &lt;td&gt;682&lt;/td&gt;
      &lt;td&gt;4710.000000&lt;/td&gt;
      &lt;td&gt;3511&lt;/td&gt;
      &lt;td&gt;25.6&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;debt_consolidation&lt;/td&gt;
      &lt;td&gt;0.1008&lt;/td&gt;
      &lt;td&gt;162.34&lt;/td&gt;
      &lt;td&gt;11.350407&lt;/td&gt;
      &lt;td&gt;8.10&lt;/td&gt;
      &lt;td&gt;712&lt;/td&gt;
      &lt;td&gt;2699.958333&lt;/td&gt;
      &lt;td&gt;33667&lt;/td&gt;
      &lt;td&gt;73.2&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;credit_card&lt;/td&gt;
      &lt;td&gt;0.1426&lt;/td&gt;
      &lt;td&gt;102.92&lt;/td&gt;
      &lt;td&gt;11.299732&lt;/td&gt;
      &lt;td&gt;14.97&lt;/td&gt;
      &lt;td&gt;667&lt;/td&gt;
      &lt;td&gt;4066.000000&lt;/td&gt;
      &lt;td&gt;4740&lt;/td&gt;
      &lt;td&gt;39.5&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get number of positve and negative examples
pos = df[df[&amp;quot;not_fully_paid&amp;quot;] == 1].shape[0]
neg = df[df[&amp;quot;not_fully_paid&amp;quot;] == 0].shape[0]
print(f&amp;quot;Positive examples = {pos}&amp;quot;)
print(f&amp;quot;Negative examples = {neg}&amp;quot;)
print(f&amp;quot;Proportion of positive to negative examples = {(pos / neg) * 100:.2f}%&amp;quot;)
plt.figure(figsize=(8, 6))
sns.countplot(df[&amp;quot;not_fully_paid&amp;quot;])
plt.xticks((0, 1), [&amp;quot;Paid fully&amp;quot;, &amp;quot;Not paid fully&amp;quot;])
plt.xlabel(&amp;quot;&amp;quot;)
plt.ylabel(&amp;quot;Count&amp;quot;)
plt.title(&amp;quot;Class counts&amp;quot;, y=1, fontdict={&amp;quot;fontsize&amp;quot;: 20});
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Positive examples = 1533
Negative examples = 8045
Proportion of positive to negative examples = 19.06%
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/loan-repayment/class_counts.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/loan-repayment/class_counts.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Class counts.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;It looks like we have only one categorical feature (&amp;quot;purpose&amp;quot;). Also, six features have missing values (no missing values in labels). Moreover, the data set is pretty imbalanced as expected where positive examples (&amp;quot;not paid fully&amp;quot;) are only 19%. We&#39;ll explain in the next section how to handle all of them after giving an overview of ensemble methods.&lt;/p&gt;

&lt;p&gt;&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Modeling&lt;/h2&gt;&lt;br&gt;
&lt;strong&gt;Ensemble methods&lt;/strong&gt; can be defined as combining several different models (base learners) into final model (meta learner) to reduce the generalization error. It relies on the assumption that each model would look at a different aspect of the data which yield to capturing part of the truth. Combining good performing models the were trained independently will capture more of the truth than a single model. Therefore, this would result in more accurate predictions and lower generalization errors.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Almost always ensemble model performance gets improved as we add more models.&lt;/li&gt;
&lt;li&gt;Try to combine models that are as much different as possible. This will reduce the correlation between the models that will improve the performance of the ensemble model that will lead to significantly outperform the best model. In the worst case where all models are perfectly correlated, the ensemble would have the same performance as the best model and sometimes even lower if some models are very bad. As a result, pick models that are as good as possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Diï¬erent ensemble methods construct the ensemble of models in diï¬erent ways. Below are the most common methods:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Blending: Averaging the predictions of all models.&lt;/li&gt;
&lt;li&gt;Bagging: Build different models on different datasets and then take the majority vote from all the models. Given the original dataset, we sample with replacement to get the same size of the original dataset. Therefore, each dataset will include, on average, 2/3 of the original data and the rest 1/3 will be duplicates. Since each model will be built on a different dataset, it can be seen as a different model. &lt;em&gt;Random Forest&lt;/em&gt; improves on default bagging trees by reducing the likelihood of strong features to picked on every split. In other words, it reduces the number of features available at each split from $n$ features to, for example, $n/2$ or $log(n)$ features. This will reduce correlation --&amp;gt; reduce variance.&lt;/li&gt;
&lt;li&gt;Boosting: Build models sequentially. That means each model learns from the residuals of the previous model. The output will be all output of each single model weighted by the learning rate ($\lambda$). It reduces the bias resulted from bagging by learning sequentially from residuals of previous trees (models).&lt;/li&gt;
&lt;li&gt;Stacking: Build k models called base learners. Then fit a model to the output of the base learners to predict the final output.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since we&#39;ll be using Random Fores (bagging) and Gradient Boosting (boosting) classifiers as base learners in the ensemble model, we&#39;ll illustrate only averaging and stacking ensemble methods. Therefore, modeling parts would be consisted of three parts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Strategies to deal with missing values.&lt;/li&gt;
&lt;li&gt;Strategies to deal with imbalanced datasets.&lt;/li&gt;
&lt;li&gt;Build ensemble models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before going further, the following data preprocessing steps will be applicable to all models:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Create dummy variables from the feature &amp;quot;purpose&amp;quot; since its nominal (not ordinal) categorical variable. It&#39;s also a good practice to drop the first one to avoid linear dependency between the resulted features since some algorithms may struggle with this issue.&lt;/li&gt;
&lt;li&gt;Split the data into training set (70%), and test set (30%). Training set will be used to fit the model, and test set will be to evaluate the best model to get an estimation of generalization error. Instead of having validation set to tune hyperparameters and evaluate different models, we&#39;ll use 10-folds cross validation because it&#39;s more reliable estimate of generalization error.&lt;/li&gt;
&lt;li&gt;Standardize the data. We&#39;ll be using &lt;code&gt;RobustScaler&lt;/code&gt; so that the standarization will be less influenced by the outliers, i.e. more robust. It centers the data around the median and scale it using &lt;em&gt;interquartile range (IQR)&lt;/em&gt;. This step will be included in the pipelines for each model as a transformer so we will not do it separately.&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create dummy variables from the feature purpose
df = pd.get_dummies(df, columns=[&amp;quot;purpose&amp;quot;], drop_first=True)
df.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style&gt;
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;credit_policy&lt;/th&gt;
      &lt;th&gt;int_rate&lt;/th&gt;
      &lt;th&gt;installment&lt;/th&gt;
      &lt;th&gt;log_annual_inc&lt;/th&gt;
      &lt;th&gt;dti&lt;/th&gt;
      &lt;th&gt;fico&lt;/th&gt;
      &lt;th&gt;days_with_cr_line&lt;/th&gt;
      &lt;th&gt;revol_bal&lt;/th&gt;
      &lt;th&gt;revol_util&lt;/th&gt;
      &lt;th&gt;inq_last_6mths&lt;/th&gt;
      &lt;th&gt;delinq_2yrs&lt;/th&gt;
      &lt;th&gt;pub_rec&lt;/th&gt;
      &lt;th&gt;not_fully_paid&lt;/th&gt;
      &lt;th&gt;purpose_credit_card&lt;/th&gt;
      &lt;th&gt;purpose_debt_consolidation&lt;/th&gt;
      &lt;th&gt;purpose_educational&lt;/th&gt;
      &lt;th&gt;purpose_home_improvement&lt;/th&gt;
      &lt;th&gt;purpose_major_purchase&lt;/th&gt;
      &lt;th&gt;purpose_small_business&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.1189&lt;/td&gt;
      &lt;td&gt;829.10&lt;/td&gt;
      &lt;td&gt;11.350407&lt;/td&gt;
      &lt;td&gt;19.48&lt;/td&gt;
      &lt;td&gt;737&lt;/td&gt;
      &lt;td&gt;5639.958333&lt;/td&gt;
      &lt;td&gt;28854&lt;/td&gt;
      &lt;td&gt;52.1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.1071&lt;/td&gt;
      &lt;td&gt;228.22&lt;/td&gt;
      &lt;td&gt;11.082143&lt;/td&gt;
      &lt;td&gt;14.29&lt;/td&gt;
      &lt;td&gt;707&lt;/td&gt;
      &lt;td&gt;2760.000000&lt;/td&gt;
      &lt;td&gt;33623&lt;/td&gt;
      &lt;td&gt;76.7&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.1357&lt;/td&gt;
      &lt;td&gt;366.86&lt;/td&gt;
      &lt;td&gt;10.373491&lt;/td&gt;
      &lt;td&gt;11.63&lt;/td&gt;
      &lt;td&gt;682&lt;/td&gt;
      &lt;td&gt;4710.000000&lt;/td&gt;
      &lt;td&gt;3511&lt;/td&gt;
      &lt;td&gt;25.6&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.1008&lt;/td&gt;
      &lt;td&gt;162.34&lt;/td&gt;
      &lt;td&gt;11.350407&lt;/td&gt;
      &lt;td&gt;8.10&lt;/td&gt;
      &lt;td&gt;712&lt;/td&gt;
      &lt;td&gt;2699.958333&lt;/td&gt;
      &lt;td&gt;33667&lt;/td&gt;
      &lt;td&gt;73.2&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.1426&lt;/td&gt;
      &lt;td&gt;102.92&lt;/td&gt;
      &lt;td&gt;11.299732&lt;/td&gt;
      &lt;td&gt;14.97&lt;/td&gt;
      &lt;td&gt;667&lt;/td&gt;
      &lt;td&gt;4066.000000&lt;/td&gt;
      &lt;td&gt;4740&lt;/td&gt;
      &lt;td&gt;39.5&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;h3 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Strategies to deal with missing values&lt;/h3&gt;&lt;br&gt;
Almost always real world data sets have missing values. This can be due, for example, users didn&#39;t fill some part of the forms or some transformations happened while collecting and cleaning the data before they send it to you. Sometimes missing values are informative and weren&#39;t generated randomly. Therefore, it&#39;s a good practice to add binary features to check if there is missing values in each row for each feature that has missing values. In our case, six features have missing values so we would add six binary features one for each feature. For example, &amp;quot;log_annual_inc&amp;quot; feature has missing values, so we would add a feature &amp;quot;is_log_annual_inc_missing&amp;quot; that takes the values $\in {0, 1}$. Good thing is that the missing values are in the predictors only and not the labels. Below are some of the most common strategies for dealing with missing values:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Simply delete all examples that have any missing values. This is usually done if the missing values are very small compared to the size of the data set and the missing values were random. In other words, the added binary features did not improve the model. One disadvantage for this strategy is that the model will throw an error when test data has missing values at prediction.&lt;/li&gt;
&lt;li&gt;Impute the missing values using the mean of each feature separately.&lt;/li&gt;
&lt;li&gt;Impute the missing values using the median of each feature separately.&lt;/li&gt;
&lt;li&gt;Use &lt;em&gt;Multivariate Imputation by Chained Equations (MICE)&lt;/em&gt;. The main disadvantage of MICE is that we can&#39;t use it as a transformer in sklearn pipelines and it requires to use the full data set when imputing the missing values. This means that there will be a risk of data leakage since we&#39;re using both training and test sets to impute the missing values. The following steps explain how MICE works:

&lt;ul&gt;
&lt;li&gt;First step: Impute the missing values using the mean of each feature separately.&lt;/li&gt;
&lt;li&gt;Second step: For each feature that has missing values, we take all other features as predictors (including the ones that had missing values) and try to predict the values for this feature using linear regression for example. The predicted values will replace the old values for that feature. We do this for all features that have missing values, i.e. each feature will be used once as a target variable to predict its values and the rest of the time as a predictor to predict other features&#39; values. Therefore, one complete cycle (iteration) will be done once we run the model $k$ times to predict the $k$ features that have missing values. For our data set, each iteration will run the linear regression 6 times to predict the 6 features.&lt;/li&gt;
&lt;li&gt;Third step: Repeat step 2 until there is not much of change between predictions.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Impute the missing values using K-Nearest Neighbors. We compute distance between all examples (excluding missing values) in the data set and take the average of k-nearest neighbors of each missing value. There&#39;s no implementation for it yet in sklearn and it&#39;s pretty inefficient to compute it since we&#39;ll have to go through all examples to calculate distances. Therefore, we&#39;ll skip this strategy in this post.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To evaluate each strategy, we&#39;ll use &lt;em&gt;Random Forest&lt;/em&gt; classifier with hyperparameters&#39; values guided by &lt;a href=&#34;https://arxiv.org/pdf/1708.05070.pdf&#34;&gt;Data-driven Advice for Applying Machine Learning to Bioinformatics Problems&lt;/a&gt; as a starting point.&lt;/p&gt;

&lt;p&gt;Let&#39;s first create binary features for missing values and then prepare the data for each strategy discussed above. Next, we&#39;ll compute the 10-folds cross validation &lt;em&gt;AUC&lt;/em&gt; score for all the models using training data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create binary features to check if the example is has missing values for all features that have missing values
for feature in df.columns:
    if np.any(np.isnan(df[feature])):
        df[&amp;quot;is_&amp;quot; + feature + &amp;quot;_missing&amp;quot;] = np.isnan(df[feature]) * 1

df.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style&gt;
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;credit_policy&lt;/th&gt;
      &lt;th&gt;int_rate&lt;/th&gt;
      &lt;th&gt;installment&lt;/th&gt;
      &lt;th&gt;log_annual_inc&lt;/th&gt;
      &lt;th&gt;dti&lt;/th&gt;
      &lt;th&gt;fico&lt;/th&gt;
      &lt;th&gt;days_with_cr_line&lt;/th&gt;
      &lt;th&gt;revol_bal&lt;/th&gt;
      &lt;th&gt;revol_util&lt;/th&gt;
      &lt;th&gt;inq_last_6mths&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;purpose_educational&lt;/th&gt;
      &lt;th&gt;purpose_home_improvement&lt;/th&gt;
      &lt;th&gt;purpose_major_purchase&lt;/th&gt;
      &lt;th&gt;purpose_small_business&lt;/th&gt;
      &lt;th&gt;is_log_annual_inc_missing&lt;/th&gt;
      &lt;th&gt;is_days_with_cr_line_missing&lt;/th&gt;
      &lt;th&gt;is_revol_util_missing&lt;/th&gt;
      &lt;th&gt;is_inq_last_6mths_missing&lt;/th&gt;
      &lt;th&gt;is_delinq_2yrs_missing&lt;/th&gt;
      &lt;th&gt;is_pub_rec_missing&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.1189&lt;/td&gt;
      &lt;td&gt;829.10&lt;/td&gt;
      &lt;td&gt;11.350407&lt;/td&gt;
      &lt;td&gt;19.48&lt;/td&gt;
      &lt;td&gt;737&lt;/td&gt;
      &lt;td&gt;5639.958333&lt;/td&gt;
      &lt;td&gt;28854&lt;/td&gt;
      &lt;td&gt;52.1&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.1071&lt;/td&gt;
      &lt;td&gt;228.22&lt;/td&gt;
      &lt;td&gt;11.082143&lt;/td&gt;
      &lt;td&gt;14.29&lt;/td&gt;
      &lt;td&gt;707&lt;/td&gt;
      &lt;td&gt;2760.000000&lt;/td&gt;
      &lt;td&gt;33623&lt;/td&gt;
      &lt;td&gt;76.7&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.1357&lt;/td&gt;
      &lt;td&gt;366.86&lt;/td&gt;
      &lt;td&gt;10.373491&lt;/td&gt;
      &lt;td&gt;11.63&lt;/td&gt;
      &lt;td&gt;682&lt;/td&gt;
      &lt;td&gt;4710.000000&lt;/td&gt;
      &lt;td&gt;3511&lt;/td&gt;
      &lt;td&gt;25.6&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.1008&lt;/td&gt;
      &lt;td&gt;162.34&lt;/td&gt;
      &lt;td&gt;11.350407&lt;/td&gt;
      &lt;td&gt;8.10&lt;/td&gt;
      &lt;td&gt;712&lt;/td&gt;
      &lt;td&gt;2699.958333&lt;/td&gt;
      &lt;td&gt;33667&lt;/td&gt;
      &lt;td&gt;73.2&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.1426&lt;/td&gt;
      &lt;td&gt;102.92&lt;/td&gt;
      &lt;td&gt;11.299732&lt;/td&gt;
      &lt;td&gt;14.97&lt;/td&gt;
      &lt;td&gt;667&lt;/td&gt;
      &lt;td&gt;4066.000000&lt;/td&gt;
      &lt;td&gt;4740&lt;/td&gt;
      &lt;td&gt;39.5&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Original Data
X = df.loc[:, df.columns != &amp;quot;not_fully_paid&amp;quot;].values
y = df.loc[:, df.columns == &amp;quot;not_fully_paid&amp;quot;].values.flatten()
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, shuffle=True, random_state=123, stratify=y)
print(f&amp;quot;Original data shapes: {X_train.shape, X_test.shape}&amp;quot;)

# Drop NA and remove binary columns
train_indices_na = np.max(np.isnan(X_train), axis=1)
test_indices_na = np.max(np.isnan(X_test), axis=1)
X_train_dropna, y_train_dropna = X_train[~train_indices_na, :][:, :-6], y_train[~train_indices_na]
X_test_dropna, y_test_dropna = X_test[~test_indices_na, :][:, :-6], y_test[~test_indices_na]
print(f&amp;quot;After dropping NAs: {X_train_dropna.shape, X_test_dropna.shape}&amp;quot;)

# MICE data
mice = fancyimpute.MICE(verbose=0)
X_mice = mice.complete(X)
X_train_mice, X_test_mice, y_train_mice, y_test_mice = train_test_split(
    X_mice, y, test_size=0.2, shuffle=True, random_state=123, stratify=y)
print(f&amp;quot;MICE data shapes: {X_train_mice.shape, X_test_mice.shape}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Original data shapes: ((7662, 24), (1916, 24))
After dropping NAs: ((7611, 18), (1905, 18))
MICE data shapes: ((7662, 24), (1916, 24))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Build random forest classifier
rf_clf = RandomForestClassifier(n_estimators=500,
                                max_features=0.25,
                                criterion=&amp;quot;entropy&amp;quot;,
                                class_weight=&amp;quot;balanced&amp;quot;)
# Build base line model -- Drop NA&#39;s
pip_baseline = make_pipeline(RobustScaler(), rf_clf)
scores = cross_val_score(pip_baseline,
                         X_train_dropna, y_train_dropna,
                         scoring=&amp;quot;roc_auc&amp;quot;, cv=10)
print(f&amp;quot;Baseline model&#39;s average AUC: {scores.mean():.3f}&amp;quot;)

# Build model with mean imputation
pip_impute_mean = make_pipeline(Imputer(strategy=&amp;quot;mean&amp;quot;),
                                RobustScaler(), rf_clf)
scores = cross_val_score(pip_impute_mean,
                         X_train, y_train,
                         scoring=&amp;quot;roc_auc&amp;quot;, cv=10)
print(f&amp;quot;Mean imputation model&#39;s average AUC: {scores.mean():.3f}&amp;quot;)

# Build model with median imputation
pip_impute_median = make_pipeline(Imputer(strategy=&amp;quot;median&amp;quot;),
                                  RobustScaler(), rf_clf)
scores = cross_val_score(pip_impute_median,
                         X_train, y_train,
                         scoring=&amp;quot;roc_auc&amp;quot;, cv=10)
print(f&amp;quot;Median imputation model&#39;s average AUC: {scores.mean():.3f}&amp;quot;)

# Build model using MICE imputation
pip_impute_mice = make_pipeline(RobustScaler(), rf_clf)
scores = cross_val_score(pip_impute_mice,
                         X_train_mice, y_train_mice,
                         scoring=&amp;quot;roc_auc&amp;quot;, cv=10)
print(f&amp;quot;MICE imputation model&#39;s average AUC: {scores.mean():.3f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Baseline model&#39;s average AUC: 0.651
Mean imputation model&#39;s average AUC: 0.651
Median imputation model&#39;s average AUC: 0.651
MICE imputation model&#39;s average AUC: 0.656
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&#39;s plot the feature importances to check if the added binary features added anything to the model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# fit RF to plot feature importances
rf_clf.fit(RobustScaler().fit_transform(Imputer(strategy=&amp;quot;median&amp;quot;).fit_transform(X_train)), y_train)

# Plot features importance
importances = rf_clf.feature_importances_
indices = np.argsort(rf_clf.feature_importances_)[::-1]
plt.figure(figsize=(12, 6))
plt.bar(range(1, 25), importances[indices], align=&amp;quot;center&amp;quot;)
plt.xticks(range(1, 25), df.columns[df.columns != &amp;quot;not_fully_paid&amp;quot;][indices], rotation=90)
plt.title(&amp;quot;Feature Importance&amp;quot;, {&amp;quot;fontsize&amp;quot;: 16});
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/loan-repayment/feature_importance.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/loan-repayment/feature_importance.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Random Forest feature importance.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Guided by the 10-fold cross validation &lt;em&gt;AUC&lt;/em&gt; scores, it looks like all strategies have comparable results and missing values were generated randomly. Also, the added six binary features showed no importance when plotting feature importances from &lt;em&gt;Random Forest&lt;/em&gt; classifier. Therefore, it&#39;s safe to drop those features and use &lt;em&gt;Median Imputation&lt;/em&gt; method as a transformer later on in the pipeline.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Drop generated binary features
X_train = X_train[:, :-6]
X_test = X_test[:, :-6]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;h3 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Strategies to deal with imbalanced datasets&lt;/h3&gt;&lt;br&gt;
Classification problems in most real world applications have imbalanced data sets. In other words, the positive examples (minority class) are a lot less than negative examples (majority class). We can see that in spam detection, ads click, loan approvals, etc. In our example, the positive examples (people who haven&#39;t fully paid) were only 19% from the total examples. Therefore, accuracy is no longer a good measure of performance for different models because if we simply predict all examples to belong to the negative class, we achieve 81% accuracy. Better metrics for imbalanced data sets are &lt;em&gt;AUC&lt;/em&gt; (area under the ROC curve) and f1-score. However, that&#39;s not enough because class imbalance influences a learning algorithm during training by making the decision rule biased towards the majority class by implicitly learns a model that optimizes the predictions based on the majority class in the dataset. As a result, we&#39;ll explore different methods to overcome class imbalance problem.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Under-Sample: Under-sample the majority class with or w/o replacement by making the number of positive and negative examples equal. One of the drawbacks of under-sampling is that it ignores a good portion of training data that has valuable information. In our example, it would loose around 6500 examples. However, it&#39;s very fast to train.&lt;/li&gt;
&lt;li&gt;Over-Sample: Over-sample the minority class with or w/o replacement by making the number of positive and negative examples equal. We&#39;ll add around 6500 samples from the training data set with this strategy. It&#39;s a lot more computationally expensive than under-sampling. Also, it&#39;s more prune to overfitting due to repeated examples.&lt;/li&gt;
&lt;li&gt;EasyEnsemble: Sample several subsets from the majority class, build a classifier on top of each sampled data, and combine the output of all classifiers. More details can be found &lt;a href=&#34;http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tsmcb09.pdf&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Synthetic Minority Oversampling Technique (SMOTE): It over-samples the minority class but using synthesized examples. It operates on feature space not the data space. Here how it works:

&lt;ul&gt;
&lt;li&gt;Compute the k-nearest neighbors for all minority samples.&lt;/li&gt;
&lt;li&gt;Randomly choose number between 1-k.&lt;/li&gt;
&lt;li&gt;For each feature:

&lt;ul&gt;
&lt;li&gt;Compute the difference between minority sample and its randomly chosen neighbor (from previous step).&lt;/li&gt;
&lt;li&gt;Multiply the difference by random number between 0 and 1.&lt;/li&gt;
&lt;li&gt;Add the obtained feature to the synthesized sample attributes.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Repeat the above until we get the number of synthesized samples needed. More information can be found &lt;a href=&#34;https://www.jair.org/media/953/live-953-2037-jair.pdf&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are other methods such as &lt;code&gt;EditedNearestNeighbors&lt;/code&gt; and &lt;code&gt;CondensedNearestNeighbors&lt;/code&gt; that we will not cover in this post and are rarely used in practice.&lt;/p&gt;

&lt;p&gt;In most applications, misclassifying the minority class (false negative) is a lot more expensive than misclassifying the majority class (false positive). In the context of lending, loosing money by lending to a risky borrower who is more likely to not fully pay the loan back is a lot more costly than missing the opportunity of lending to trust-worthy borrower (less risky). As a result, we can use &lt;code&gt;class_weight&lt;/code&gt; that changes the weight of misclassifying positive example in the loss function. Also, we can use different cut-offs assign examples to classes. By default, 0.5 is the cut-off; however, we see more often in applications such as lending that the cut-off is less than 0.5. Note that changing the cut-off from the default 0.5 reduce the overall accuracy but may improve the accuracy of predicting positive/negative examples.&lt;/p&gt;

&lt;p&gt;We&#39;ll evaluate all the above methods plus the original model without resampling as a baseline model using the same &lt;em&gt;Random Forest&lt;/em&gt; classifier we used in the missing values section.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Build random forest classifier (same config)
rf_clf = RandomForestClassifier(n_estimators=500,
                                max_features=0.25,
                                criterion=&amp;quot;entropy&amp;quot;,
                                class_weight=&amp;quot;balanced&amp;quot;)

# Build model with no sampling
pip_orig = make_pipeline(Imputer(strategy=&amp;quot;mean&amp;quot;),
                         RobustScaler(),
                         rf_clf)
scores = cross_val_score(pip_orig,
                         X_train, y_train,
                         scoring=&amp;quot;roc_auc&amp;quot;, cv=10)
print(f&amp;quot;Original model&#39;s average AUC: {scores.mean():.3f}&amp;quot;)

# Build model with undersampling
pip_undersample = imb_make_pipeline(Imputer(strategy=&amp;quot;mean&amp;quot;),
                                    RobustScaler(),
                                    RandomUnderSampler(), rf_clf)
scores = cross_val_score(pip_undersample,
                         X_train, y_train,
                         scoring=&amp;quot;roc_auc&amp;quot;, cv=10)
print(f&amp;quot;Under-sampled model&#39;s average AUC: {scores.mean():.3f}&amp;quot;)

# Build model with oversampling
pip_oversample = imb_make_pipeline(Imputer(strategy=&amp;quot;mean&amp;quot;),
                                    RobustScaler(),
                                    RandomOverSampler(), rf_clf)
scores = cross_val_score(pip_oversample,
                         X_train, y_train,
                         scoring=&amp;quot;roc_auc&amp;quot;, cv=10)
print(f&amp;quot;Over-sampled model&#39;s average AUC: {scores.mean():.3f}&amp;quot;)

# Build model with EasyEnsemble
resampled_rf = BalancedBaggingClassifier(base_estimator=rf_clf,
                                         n_estimators=10, random_state=123)
pip_resampled = make_pipeline(Imputer(strategy=&amp;quot;mean&amp;quot;),
                              RobustScaler(), resampled_rf)
                             
scores = cross_val_score(pip_resampled,
                         X_train, y_train,
                         scoring=&amp;quot;roc_auc&amp;quot;, cv=10)
print(f&amp;quot;EasyEnsemble model&#39;s average AUC: {scores.mean():.3f}&amp;quot;)

# Build model with SMOTE
pip_smote = imb_make_pipeline(Imputer(strategy=&amp;quot;mean&amp;quot;),
                              RobustScaler(),
                              SMOTE(), rf_clf)
scores = cross_val_score(pip_smote,
                         X_train, y_train,
                         scoring=&amp;quot;roc_auc&amp;quot;, cv=10)
print(f&amp;quot;SMOTE model&#39;s average AUC: {scores.mean():.3f}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Original model&#39;s average AUC: 0.652
Under-sampled model&#39;s average AUC: 0.656
Over-sampled model&#39;s average AUC: 0.651
EasyEnsemble model&#39;s average AUC: 0.665
SMOTE model&#39;s average AUC: 0.641
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;EasyEnsemble method has the highest 10-folds CV with average AUC = 0.665.&lt;/p&gt;

&lt;p&gt;&lt;h3 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Build Ensemble models&lt;/h3&gt;&lt;br&gt;
We&#39;ll build ensemble models using three different models as base learners:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Gradient Boosting&lt;/li&gt;
&lt;li&gt;Support Vector Classifier&lt;/li&gt;
&lt;li&gt;Random Forest&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The ensemble models will be built using two different methods:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Blending (average) ensemble model. Fits the base learners to the training data and then, at test time, average the predictions generated by all the base learners.

&lt;ul&gt;
&lt;li&gt;Use VotingClassifier from sklearn that:&lt;/li&gt;
&lt;li&gt;Fits all the base learners on the training data&lt;/li&gt;
&lt;li&gt;At test time, use all base learners to predict test data and then take the average of all predictions.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Stacked ensemble model: Fits the base learners to the training data. Next, use those trained base learners to generate predictions (meta-features) used by the meta-learner (assuming we have only one layer of base learners). There are few different ways of training stacked ensemble model:

&lt;ul&gt;
&lt;li&gt;Fitting the base learners to all training data and then generate predictions using the same training data it was used to fit those learners. This method is more prune to overfitting because the meta learner will give more weights to the base learner who memorized the training data better, i.e. meta-learner won&#39;t generate well and would overfit.&lt;/li&gt;
&lt;li&gt;Split the training data into 2 to 3 different parts that will be used for training, validation, and generate predictions. It&#39;s a suboptimal method because held out sets usually have higher variance and different splits give different results as well as learning algorithms would have fewer data to train.&lt;/li&gt;
&lt;li&gt;Use k-folds cross validation where we split the data into k-folds. We fit the base learners to the (k - 1) folds and use the fitted models to generate predictions of the held out fold. We repeat the process until we generate the predictions for all the k-folds. When done, refit the base learners to the full training data. This method is more reliable and will give models that memorize the data less weight. Therefore, it generalizes better on future data.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&#39;ll use logistic regression as the meta-learner for the stacked model. Note that we can use k-folds cross validation to validate and tune the hyperparameters of the meta learner. We will not tune the hyperparameters of any of the base learners or the meta-learner; however, we will use some of the values recommended by the &lt;a href=&#34;https://arxiv.org/pdf/1708.05070.pdf&#34;&gt;Pennsylvania Benchmarking Paper&lt;/a&gt;. Additionally, we won&#39;t use EasyEnsemble in training because, after some experimentation, it didn&#39;t improve the AUC of the ensemble model more than 2% on average and it was computationally very expensive. In practice, we sometimes are willing to give up small improvements if the model would become a lot more complex computationally. Therefore, we will use &lt;code&gt;RandomUnderSampler&lt;/code&gt;. Also, we&#39;ll impute the missing values and standardize the data beforehand so that it would shorten the code of the ensemble models and allows use to avoid using &lt;code&gt;Pipeline&lt;/code&gt;. Additionally, we will plot ROC and PR curves using test data and evaluate the performance of all models.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Impute the missing data using features means
imp = Imputer()
imp.fit(X_train)
X_train = imp.transform(X_train)
X_test = imp.transform(X_test)

# Standardize the data
std = RobustScaler()
std.fit(X_train)
X_train = std.transform(X_train)
X_test = std.transform(X_test)

# Implement RandomUnderSampler
random_undersampler = RandomUnderSampler()
X_res, y_res = random_undersampler.fit_sample(X_train, y_train)
# Shuffle the data
perms = np.random.permutation(X_res.shape[0])
X_res = X_res[perms]
y_res = y_res[perms]
X_res.shape, y_res.shape
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;((2452, 18), (2452,))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define base learners
xgb_clf = xgb.XGBClassifier(objective=&amp;quot;binary:logistic&amp;quot;,
                            learning_rate=0.03,
                            n_estimators=500,
                            max_depth=1,
                            subsample=0.4,
                            random_state=123)

svm_clf = SVC(gamma=0.1,
                C=0.01,
                kernel=&amp;quot;poly&amp;quot;,
                degree=3,
                coef0=10.0,
                probability=True)

rf_clf = RandomForestClassifier(n_estimators=300,
                                max_features=&amp;quot;sqrt&amp;quot;,
                                criterion=&amp;quot;gini&amp;quot;,
                                min_samples_leaf=5,
                                class_weight=&amp;quot;balanced&amp;quot;)

# Define meta-learner
logreg_clf = LogisticRegression(penalty=&amp;quot;l2&amp;quot;,
                                C=100,
                                fit_intercept=True)

# Fitting voting clf --&amp;gt; average ensemble
voting_clf = VotingClassifier([(&amp;quot;xgb&amp;quot;, xgb_clf),
                               (&amp;quot;svm&amp;quot;, svm_clf),
                               (&amp;quot;rf&amp;quot;, rf_clf)],
                              voting=&amp;quot;soft&amp;quot;,
                              flatten_transform=True)
voting_clf.fit(X_res, y_res)
xgb_model, svm_model, rf_model = voting_clf.estimators_
models = {&amp;quot;xgb&amp;quot;: xgb_model, &amp;quot;svm&amp;quot;: svm_model,
          &amp;quot;rf&amp;quot;: rf_model, &amp;quot;avg_ensemble&amp;quot;: voting_clf}

# Build first stack of base learners
first_stack = make_pipeline(voting_clf,
                            FunctionTransformer(lambda X: X[:, 1::2]))
# Use CV to generate meta-features
meta_features = cross_val_predict(first_stack,
                                  X_res, y_res,
                                  cv=10,
                                  method=&amp;quot;transform&amp;quot;)
# Refit the first stack on the full training set
first_stack.fit(X_res, y_res)
# Fit the meta learner
second_stack = logreg_clf.fit(meta_features, y_res)

# Plot ROC and PR curves using all models and test data
fig, axes = plt.subplots(1, 2, figsize=(14, 6))
for name, model in models.items():
            model_probs = model.predict_proba(X_test)[:, 1:]
            model_auc_score = roc_auc_score(y_test, model_probs)
            fpr, tpr, _ = roc_curve(y_test, model_probs)
            precision, recall, _ = precision_recall_curve(y_test, model_probs)
            axes[0].plot(fpr, tpr, label=f&amp;quot;{name}, auc = {model_auc_score:.3f}&amp;quot;)
            axes[1].plot(recall, precision, label=f&amp;quot;{name}&amp;quot;)
stacked_probs = second_stack.predict_proba(first_stack.transform(X_test))[:, 1:]
stacked_auc_score = roc_auc_score(y_test, stacked_probs)
fpr, tpr, _ = roc_curve(y_test, stacked_probs)
precision, recall, _ = precision_recall_curve(y_test, stacked_probs)
axes[0].plot(fpr, tpr, label=f&amp;quot;stacked_ensemble, auc = {stacked_auc_score:.3f}&amp;quot;)
axes[1].plot(recall, precision, label=&amp;quot;stacked_ensembe&amp;quot;)
axes[0].legend(loc=&amp;quot;lower right&amp;quot;)
axes[0].set_xlabel(&amp;quot;FPR&amp;quot;)
axes[0].set_ylabel(&amp;quot;TPR&amp;quot;)
axes[0].set_title(&amp;quot;ROC curve&amp;quot;)
axes[1].legend()
axes[1].set_xlabel(&amp;quot;recall&amp;quot;)
axes[1].set_ylabel(&amp;quot;precision&amp;quot;)
axes[1].set_title(&amp;quot;PR curve&amp;quot;)
plt.tight_layout()
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/loan-repayment/roc_pr_curve.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/loan-repayment/roc_pr_curve.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;ROC and PR curves.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;As we can see from the chart above, stacked ensemble model didn&#39;t improve the performance. One of the major reasons are that the base learners are considerably highly correlated especially &lt;em&gt;Random Forest&lt;/em&gt; and &lt;em&gt;Gradient Boosting&lt;/em&gt; (see the correlation matrix below).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot the correlation between base learners
probs_df = pd.DataFrame(meta_features, columns=[&amp;quot;xgb&amp;quot;, &amp;quot;svm&amp;quot;, &amp;quot;rf&amp;quot;])
corrmat(probs_df.corr(), inflate=True);
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/loan-repayment/corr_matrix.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/loan-repayment/corr_matrix.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Correlation matrix.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;In addition, with classification problems where False Negatives are a lot more expensive than False Positives, we may want to have a model with a high precision rather than high recall, i.e. the probability of the model to identify positive examples from randomly selected examples. Below is the confusion matrix:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;second_stack_probs = second_stack.predict_proba(first_stack.transform(X_test))
second_stack_preds = second_stack.predict(first_stack.transform(X_test))
conf_mat = confusion_matrix(y_test, second_stack_preds)
# Define figure size and figure ratios
plt.figure(figsize=(16, 8))
plt.matshow(conf_mat, cmap=plt.cm.Reds, alpha=0.2)
for i in range(2):
    for j in range(2):
        plt.text(x=j, y=i, s=conf_mat[i, j], ha=&amp;quot;center&amp;quot;, va=&amp;quot;center&amp;quot;)
plt.title(&amp;quot;Confusion matrix&amp;quot;, y=1.1, fontdict={&amp;quot;fontsize&amp;quot;: 20})
plt.xlabel(&amp;quot;Predicted&amp;quot;, fontdict={&amp;quot;fontsize&amp;quot;: 14})
plt.ylabel(&amp;quot;Actual&amp;quot;, fontdict={&amp;quot;fontsize&amp;quot;: 14});
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/loan-repayment/confusion_matrix.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/loan-repayment/confusion_matrix.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Confusion matrix.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Let&#39;s finally check the partial dependence plots to see what are the most important features and their relationships with whether the borrower will most likely pay the loan in full before mature data. we will plot only the top 8 features to make it easier to read. Note that the partial plots are based on Gradient Boosting model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot partial dependence plots
gbrt = GradientBoostingClassifier(loss=&amp;quot;deviance&amp;quot;,
                                  learning_rate=0.1,
                                  n_estimators=100,
                                  max_depth=3,
                                  random_state=123)
gbrt.fit(X_res, y_res)
fig, axes = plot_partial_dependence(gbrt, X_res,
                                    np.argsort(gbrt.feature_importances_)[::-1][:8],
                                    n_cols=4,
                                    feature_names=df.columns[:-6],
                                    figsize=(14, 8))
plt.subplots_adjust(top=0.9)
plt.suptitle(&amp;quot;Partial dependence plots of borrower not fully paid\n&amp;quot;
             &amp;quot;the loan based on top most influential features&amp;quot;)
for ax in axes: ax.set_xticks(())
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/loan-repayment/partial_plots.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/loan-repayment/partial_plots.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Partial dependence plots.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;As we might expected, borrowers with lower annual income and less FICO scores are less likely to pay the loan fully; however, borrowers with lower interest rates (riskier) and smaller installments are more likely to pay the loan fully.&lt;/p&gt;

&lt;p&gt;&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Conclusion&lt;/h2&gt;&lt;br&gt;
Most classification problems in the real world are imbalanced. Also, almost always data sets have missing values. In this post, we covered strategies to deal with both missing values and imbalanced data sets. We also explored different ways of building ensembles in sklearn. Below are some takeaway points:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There is no definitive guide of which algorithms to use given any situation. What may work on some data sets may not necessarily work on others. Therefore, always evaluate methods using cross validation to get a reliable estimates.&lt;/li&gt;
&lt;li&gt;Sometimes we may be willing to give up some improvement to the model if that would increase the complexity much more than the percentage change in the improvement to the evaluation metrics.&lt;/li&gt;
&lt;li&gt;In some classification problems, &lt;em&gt;False Negatives&lt;/em&gt; are a lot more expensive than &lt;em&gt;False Positives&lt;/em&gt;. Therefore, we can reduce cut-off points to reduce the False Negatives.&lt;/li&gt;
&lt;li&gt;When building ensemble models, try to use good models that are as different as possible to reduce correlation between the base learners. We could&#39;ve enhanced our stacked ensemble model by adding &lt;em&gt;Dense Neural Network&lt;/em&gt; and some other kind of base learners as well as adding more layers to the stacked model.&lt;/li&gt;
&lt;li&gt;EasyEnsemble usually performs better than any other resampling methods.&lt;/li&gt;
&lt;li&gt;Missing values sometimes add more information to the model than we might expect. One way of capturing it is to add binary features for each feature that has missing values to check if each example is missing or not.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Character-level Language Model</title>
      <link>https://imaddabbura.github.io/post/character-level-language-model/</link>
      <pubDate>Thu, 22 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://imaddabbura.github.io/post/character-level-language-model/</guid>
      <description>&lt;!-- 


  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/character-level-language-model/text_sample.jpg&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/character-level-language-model/text_sample.jpg&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Iphone&amp;rsquo;s text suggestion.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;
 --&gt;

&lt;p&gt;Have you ever wondered how Gmail automatic reply works? Or how your phone suggests next word when texting? Or even how a Neural Network can generate musical notes? The general way of generating a sequence of text is to train a model to predict the next word/character given all previous words/characters. Such model is called a &lt;strong&gt;Statistical Language Model&lt;/strong&gt;. What is a statistical language model? A statistical language model tries to capture the statistical structure (latent space) of training text it&#39;s trained on. Usually &lt;strong&gt;Recurrent Neural Network (RNN)&lt;/strong&gt; models family are used to train the model due to the fact that they are very powerful and expressive in which they remember and process past information through their high dimensional hidden state units. The main goal of any language model is to learn the joint probability distribution of sequences of characters/words in a training text, i.e. trying to learn the joint probability function. For example, if we&#39;re trying to predict a sequence of $T$ words, we try to get the joint probability $P(w_1, w_2, ..., w_T)$ as big as we can which is equal to the product of all conditional probabilities $\prod_{t = 1}^T P(w_t/w_{t-1})$ at all time steps (t).&lt;/p&gt;

&lt;p&gt;In this post, we&#39;ll cover the &lt;strong&gt;Character-Level Language Model&lt;/strong&gt; where almost all the concepts hold for any other language models such as word-language models. The main task of the character-level language model is to predict the next character given all previous characters in a sequence of data, i.e. generates text character by character. More formally, given a training sequence $(x^1, ... , x^T)$, the RNN uses the sequence of its output vectors $(o^1, ... , o^T)$ to obtain a sequence of predictive distributions $P(x&lt;sup&gt;t/x&lt;/sup&gt;{t-1}) = softmax(o^t)$.&lt;/p&gt;

&lt;p&gt;Let&#39;s illustrate how the character-level language model works using my first name (&amp;quot;imad&amp;quot;) as an example (see figure 1 for all the details of this example).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We first build a vocabulary dictionary using all the unique letters of the names in the corpus as keys and the index of each letter starting from zero (since Python is a zero-indexed language) in an ascending order. For our example, the vocabulary dictionary would be: {&amp;quot;a&amp;quot;: 0, &amp;quot;d&amp;quot;: 1, &amp;quot;i&amp;quot;: 2, &amp;quot;m&amp;quot;: 3}. Therefore, &amp;quot;imad&amp;quot; would become a list of the following integers: [2, 3, 0, 1].&lt;/li&gt;
&lt;li&gt;Convert the input and the output characters to lists of integers using the vocabulary dictionary. In this post, we&#39;ll assume that $x^1 = \vec{0}$ for all examples. Therefore, $y = &amp;quot;imad&amp;quot;$ and $x = \vec{0}\ + &amp;quot;ima&amp;quot;$. In other words, $x^{t + 1} = y^t$ which gives us: $y = [2, 3, 0, 1]$ and $x = [\vec{0}, 2, 3, 0]$.&lt;/li&gt;
&lt;li&gt;For each character in the input:

&lt;ol&gt;
&lt;li&gt;Convert the input characters into one-hot vectors. Notice how the first character $x^1 = \vec{0}$.&lt;/li&gt;
&lt;li&gt;Compute the hidden state layer.&lt;/li&gt;
&lt;li&gt;Compute the output layer and then pass it through softmax to get the results as probabilities.&lt;/li&gt;
&lt;li&gt;Feed the target character at time step (t) as the input character at time step $(t + 1)$.&lt;/li&gt;
&lt;li&gt;Go back to step A and repeat until we finish all the letters in the name.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The objective is to make the green numbers as big as we can and the red numbers as small as we can in the probability distribution layer. The reason is that the true index should have the highest probability by making it as close as we can to 1. The way to do that is to measure the loss using cross-entropy and then compute the gradients of the loss w.r.t. all parameters to update them in the opposite of the gradient direction. Repeating the process over many times where each time we adjust the parameters based on the gradient direction --&amp;gt; model will be able to correctly predict next characters given all previous ones using all names in the training text. Notice that hidden state $h^4$ has all past information about all characters.&lt;/p&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/character-level-language-model/char_level_example.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/character-level-language-model/char_level_example.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Illustrative example of character-level language model using RNN.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: To shorten the length of the post, I deleted all the docstrings of python functions and I didn&#39;t include some functions that i didn&#39;t think are necessary to understand the main concepts. The notebook and the script that created this post can be found &lt;a href=&#34;https://nbviewer.jupyter.org/github/ImadDabbura/blog-posts/blob/master/notebooks/Character-LeveL-Language-Model.ipynb&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://github.com/ImadDabbura/blog-posts/blob/master/scripts/character_level_language_model.py&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Training&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;http://deron.meranda.us/data/census-derived-all-first.txt&#34;&gt;dataset&lt;/a&gt; we&#39;ll be using has 5,163 names: 4,275 male names, 1,219 female names, and 331 names that can be both female and male names. The RNN architecture we&#39;ll be using to train the character-level language model is called &lt;strong&gt;many to many&lt;/strong&gt; where time steps of the input $(T_x)$ = time steps of the output $(T_y)$. In other words, the sequence of the input and output are synced (see figure 2).&lt;/p&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/character-level-language-model/rnn_architecture.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/character-level-language-model/rnn_architecture.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;RNN architecture: many to many.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;The character-level language model will be trained on names; which means after we&#39;re done with training the model, we&#39;ll be able to generate some interesting names :).&lt;/p&gt;

&lt;p&gt;In this section, we&#39;ll go over four main parts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#fwd_prop&#34;&gt;Forward propagation&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bckwrd_prop&#34;&gt;Backpropagation&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sampling&#34;&gt;Sampling&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#train&#34;&gt;Fitting the model&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a id=&#39;fwd_prop&#39;&gt;&lt;/a&gt;
&lt;h3 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Forward Propagation&lt;/h3&gt;&lt;/p&gt;

&lt;p&gt;We&#39;ll be using Stochastic Gradient Descent (SGD) where each batch consists of only one example. In other words, the RNN model will learn from each example (name) separately, i.e. run both forward and backward passes on each example and update parameters accordingly. Below are all the steps needed for a forward pass:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Create a vocabulary dictionary using the unique lower case letters.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Create a character to index dictionary that maps each character to its corresponding index in an ascending order. For example, &amp;quot;a&amp;quot; would have index 1 (since python is a zero index language and we&#39;ll reserve 0 index to EOS &amp;quot;\n&amp;quot;) and &amp;quot;z&amp;quot; would have index 26. We will use this dictionary in converting names into lists of integers where each letter will be represented as one-hot vector.&lt;/li&gt;
&lt;li&gt;Create an index to character dictionary that maps indices to characters. This dictionary will be used to convert the output of the RNN model into characters which will be translated into names.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Initialize parameters: weights will be initialized to small random numbers from standard normal distribution to break symmetry and make sure different hidden units learn different things. On the other hand, biases will be initialized to zeros.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$W_{hh}$: weight matrix connecting previous hidden state $h^{t - 1}$ to current hidden state $h^t$.&lt;/li&gt;
&lt;li&gt;$W_{xh}$: weight matrix connecting input $x^t$ to hidden state $h^t$.&lt;/li&gt;
&lt;li&gt;$b$: hidden state bias vector.&lt;/li&gt;
&lt;li&gt;$W_{hy}$: weight matrix connecting hidden state $h^t$ to output $o^t$.&lt;/li&gt;
&lt;li&gt;$c$: output bias vector.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Convert input $x^t$ and output $y^t$ into one-hot vector each. The dimension of the one-hot vector is vocab_size x 1. Everything will be zero except for the index of the letter at (t) would be 1. In our case, $x^t$ would be the same as $y^t$ shifted to the left where $x^1 = \vec{0}$; however, starting from $t = 2$, $x^{t + 1} = y^{t}$. For example, if we use &amp;quot;imad&amp;quot; as the input, then $y = [3, 4, 1, 2, 0]$ while $x = [\vec{0}, 3, 4, 1, 2]$. Notice that $x^1 = \vec{0}$ and not the index 0. Moreover, we&#39;re using &amp;quot;\n&amp;quot; as EOS (end of sentence/name) for each name so that the RNN learns &amp;quot;\n&amp;quot; as any other character. This will help the network learn when to to stop generating characters. Therefore, the last target character for all names will be &amp;quot;\n&amp;quot; that represents the end of the name.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Compute the hidden state using the following formula:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(h^t = tanh(W_{hh}h^{t - 1} + W_{xh}x^t + b)\tag{1}\)&lt;/span&gt;
Notice that we use hyperbolic tangent $(\frac{e^x - e&lt;sup&gt;{-x}}{e&lt;/sup&gt;x + e^{-x}})$ as the non-linear function. One of the main advantages of the hyperbolic tangent function is that it resembles the identity function.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Compute the output layer using the following formula:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[o^t = W_{hy}h^{t} + c \tag{2}\]&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pass the output through softmax layer to normalize the output that allows us to express it as a probability, i.e. all output will be between 0 and 1 and sum up to 1. Below is the softmax formula:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(\widehat{y^t} = \frac{e^{o^t}}{\sum_ie^{o_i^t}}\tag{3}\)&lt;/span&gt;
The softmax layer has the same dimension as the output layer which is vocab_size x 1. As a result, $y^t[i]$ is the probability of index $i$ being the next character at time step (t).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;As mentioned before, the objective of a character-level language model is to minimize the negative log-likelihood of the training sequence. Therefore, the loss function at time step (t) and the total loss across all time steps are:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\mathcal{L}^t = -\sum_{i = 1}^{T_y}y^tlog\widehat{y^t}\tag{4}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(\mathcal{L} = \sum_{t = 1}^{T_y}\mathcal{L}^t(\widehat{y^t}, y^t)\tag{5}\)&lt;/span&gt;
Since we&#39;ll be using SGD, the loss will be noisy and have many oscillations, so it&#39;s a good practice to smooth out the loss using exponential weighted average.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pass the target character $y^t$ as the next input $x^{t + 1}$ until we finish the sequence.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load packages
import os

import numpy as np

os.chdir(&amp;quot;../scripts/&amp;quot;)
from character_level_language_model import (initialize_parameters,
                                            initialize_rmsprop,
                                            softmax,
                                            smooth_loss,
                                            update_parameters_with_rmsprop)


def rnn_forward(x, y, h_prev, parameters):
    &amp;quot;&amp;quot;&amp;quot;Implement one Forward pass on one name.&amp;quot;&amp;quot;&amp;quot;
    # Retrieve parameters
    Wxh, Whh, b = parameters[&amp;quot;Wxh&amp;quot;], parameters[&amp;quot;Whh&amp;quot;], parameters[&amp;quot;b&amp;quot;]
    Why, c = parameters[&amp;quot;Why&amp;quot;], parameters[&amp;quot;c&amp;quot;]

    # Initialize inputs, hidden state, output, and probabilities dictionaries
    xs, hs, os, probs = {}, {}, {}, {}

    # Initialize x0 to zero vector
    xs[0] = np.zeros((vocab_size, 1))

    # Initialize loss and assigns h_prev to last hidden state in hs
    loss = 0
    hs[-1] = np.copy(h_prev)

    # Forward pass: loop over all characters of the name
    for t in range(len(x)):
        # Convert to one-hot vector
        if t &amp;gt; 0:
            xs[t] = np.zeros((vocab_size, 1))
            xs[t][x[t]] = 1
        # Hidden state
        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + b)
        # Logits
        os[t] = np.dot(Why, hs[t]) + c
        # Probs
        probs[t] = softmax(os[t])
        # Loss
        loss -= np.log(probs[t][y[t], 0])

    cache = (xs, hs, probs)

    return loss, cache
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#39;bckwrd_prop&#39;&gt;&lt;/a&gt;
&lt;h3 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Backpropagation&lt;/h3&gt;&lt;/p&gt;

&lt;p&gt;With RNN based models, the gradient-based technique that will be used is called &lt;strong&gt;Backpropagation Through Time (BPTT)&lt;/strong&gt;. We start at last time step $T$ and backpropagate loss function w.r.t. all parameters across all time steps and sum them up (see figure 4).&lt;/p&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/character-level-language-model/backprop.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/character-level-language-model/backprop.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Backpropagation Through Time (BPTT).&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;In addition, since RNNs are known to have steep cliffs (sudden steep decrease in $\mathcal{L}$), gradients may overshoot the minimum and undo a lot of the work that was done even if we are using adaptive learning methods such as RMSProp. The reason is because gradient is a linear approximation of the loss function and may not capture information further than the point it was evaluated on such as the curvature of loss curve. Therefore, it&#39;s a common practice to clip the gradients to be in the interval . For this exercise, we&#39;ll clip the gradients to be in the interval . That means if the gradient is &amp;gt; 5 or &amp;lt; -5, it would be clipped to 5 and -5 respectively. Below are all the formulas needed to compute the gradients w.r.t. all parameters at all time steps.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(\nabla_{o^t}\mathcal{L} = \widehat{y^t} - y^t\tag{6}\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(\nabla_{W_{hy}}\mathcal{L} = \sum_t \nabla_{o^t}\mathcal{L} . {h^t}^T\tag{7}\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(\nabla_{c}\mathcal{L} = \sum_t \nabla_{o^t}\mathcal{L} \tag{8}\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(\nabla_{h^t}\mathcal{L} = W_{hy}^T . \nabla_{o^t}\mathcal{L} + \underbrace { W_{hh}^T . \nabla_{h^{t + 1}}\mathcal{L} * (1 - tanh(W_{hh}h^{t} + W_{xh}x^{t + 1} + b) ^ 2)}_{dh_{next}} \tag{9}\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(\nabla_{h^{t - 1}}\mathcal{L} = W_{hh}^T . \nabla_{h^t}\mathcal{L} * (1 - tanh(h^t) ^ 2)\tag{10}\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(\nabla_{x^t}\mathcal{L} = W_{xh}^T . \nabla_{h^t}\mathcal{L} * (1 - tanh(W_{hh}h^{t-1} + W_{xh}x^t + b) ^ 2)\tag{11}\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(\nabla_{W_{hh}}\mathcal{L} = \sum_t \nabla_{h^t}\mathcal{L} * (1 - tanh(W_{hh}h^{t-1} + W_{xh}x^t + b) ^ 2) . {h^{t - 1}}^T\tag{12}\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(\nabla_{W_{xh}}\mathcal{L} = \sum_t \nabla_{h^t}\mathcal{L} * (1 - tanh(W_{hh}h^{t-1} + W_{xh}x^t + b) ^ 2) . {x^t}^T\tag{13}\)&lt;/span&gt;
&lt;span  class=&#34;math&#34;&gt;\(\nabla_{b}\mathcal{L} = \sum_t \nabla_{h^t}\mathcal{L} * (1 - tanh(h^t) ^ 2) \tag{14}\)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Note that at last time step $T$, we&#39;ll initialize $dh_{next}$  to zeros since we can&#39;t get values from future. To stabilize the update at each time step since SGD may have so many oscillations, we&#39;ll be using one of the adaptive learning method optimizers. More specifically, we&#39;ll use &lt;a href=&#34;https://nbviewer.jupyter.org/github/ImadDabbura/Deep-Learning/blob/master/notebooks/Optimization-Algorithms.ipynb&#34;&gt;Root Mean Squared Propagation (RMSProp)&lt;/a&gt; which tends to have acceptable performance.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def clip_gradients(gradients, max_value):
    &amp;quot;&amp;quot;&amp;quot;
    Implements gradient clipping element-wise on gradients to be between the
    interval [-max_value, max_value].
    &amp;quot;&amp;quot;&amp;quot;
    for grad in gradients.keys():
        np.clip(gradients[grad], -max_value, max_value, out=gradients[grad])

    return gradients


def rnn_backward(y, parameters, cache):
    &amp;quot;&amp;quot;&amp;quot;
    Implements Backpropagation on one name.
    &amp;quot;&amp;quot;&amp;quot;
    # Retrieve xs, hs, and probs
    xs, hs, probs = cache

    # Initialize all gradients to zero
    dh_next = np.zeros_like(hs[0])

    parameters_names = [&amp;quot;Whh&amp;quot;, &amp;quot;Wxh&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;Why&amp;quot;, &amp;quot;c&amp;quot;]
    grads = {}
    for param_name in parameters_names:
        grads[&amp;quot;d&amp;quot; + param_name] = np.zeros_like(parameters[param_name])

    # Iterate over all time steps in reverse order starting from Tx
    for t in reversed(range(len(xs))):
        dy = np.copy(probs[t])
        dy[y[t]] -= 1
        grads[&amp;quot;dWhy&amp;quot;] += np.dot(dy, hs[t].T)
        grads[&amp;quot;dc&amp;quot;] += dy
        dh = np.dot(parameters[&amp;quot;Why&amp;quot;].T, dy) + dh_next
        dhraw = (1 - hs[t] ** 2) * dh
        grads[&amp;quot;dWhh&amp;quot;] += np.dot(dhraw, hs[t - 1].T)
        grads[&amp;quot;dWxh&amp;quot;] += np.dot(dhraw, xs[t].T)
        grads[&amp;quot;db&amp;quot;] += dhraw
        dh_next = np.dot(parameters[&amp;quot;Whh&amp;quot;].T, dhraw)
        # Clip the gradients using [-5, 5] as the interval
        grads = clip_gradients(grads, 5)
    
    # Get the last hidden state
    h_prev = hs[len(xs) - 1]

    return grads, h_prev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#39;sampling&#39;&gt;&lt;/a&gt;
&lt;h3 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Sampling&lt;/h3&gt;&lt;br&gt;
Sampling is what makes the text generated by the RNN at each time step an interesting/creative text. On each time step (t), the RNN output the conditional probability distribution of the next character given all the previous characters, i.e. $P(c_t/c_1, c_2, ..., c_{t-1})$. Let&#39;s assume that we are at time step $t = 3$ and we&#39;re trying to predict the third character, the conditional probability distribution is: $P(c_3/c_1, c_2) = (0.2, 0.3, 0.4, 0.1)$. We&#39;ll have two extremes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Maximum entropy: the character will be picked randomly using uniform probability distribution; which means that all characters in the vocabulary dictionary are equally likely. Therefore, we&#39;ll end up with maximum randomness in picking the next character and the generated text will not be either meaningful or sound real.&lt;/li&gt;
&lt;li&gt;Minimum entropy: the character with the highest conditional probability will be picked on each time step. That means next character will be what the model estimates to be the right one based on the training text and learned parameters. As a result, the names generated will be both meaningful and sound real. However, it will also be repetitive and not as interesting since all the parameters were optimized to learn joint probability distribution in predicting the next character.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As we increase randomness, text will lose local structure; however, as we decrease randomness, the generated text will sound more real and start to preserve its local structure. For this exercise, we will sample from the distribution that&#39;s generated by the model which can be seen as an intermediate level of randomness between maximum and minimum entropy (see figure 5). Using this sampling strategy on the above distribution, the index 0 has $20$% probability of being picked, while index 2 has $40$% probability to be picked.&lt;/p&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/character-level-language-model/sampling.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/character-level-language-model/sampling.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Sampling: An example of predicting next character using character-level language model.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Therefore, sampling will be used at test time to generate names character by character.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sample(parameters, idx_to_chars, chars_to_idx, n):
    &amp;quot;&amp;quot;&amp;quot;
    Implements sampling of a squence of n characters characters length. The
    sampling will be based on the probability distribution output of RNN.
    &amp;quot;&amp;quot;&amp;quot;
    # Retrienve parameters, shapes, and vocab size
    Whh, Wxh, b = parameters[&amp;quot;Whh&amp;quot;], parameters[&amp;quot;Wxh&amp;quot;], parameters[&amp;quot;b&amp;quot;]
    Why, c = parameters[&amp;quot;Why&amp;quot;], parameters[&amp;quot;c&amp;quot;]
    n_h, n_x = Wxh.shape
    vocab_size = c.shape[0]

    # Initialize a0 and x1 to zero vectors
    h_prev = np.zeros((n_h, 1))
    x = np.zeros((n_x, 1))

    # Initialize empty sequence
    indices = []
    idx = -1
    counter = 0
    while (counter &amp;lt;= n and idx != chars_to_idx[&amp;quot;\n&amp;quot;]):
        # Fwd propagation
        h = np.tanh(np.dot(Whh, h_prev) + np.dot(Wxh, x) + b)
        o = np.dot(Why, h) + c
        probs = softmax(o)

        # Sample the index of the character using generated probs distribution
        idx = np.random.choice(vocab_size, p=probs.ravel())

        # Get the character of the sampled index
        char = idx_to_chars[idx]

        # Add the char to the sequence
        indices.append(idx)

        # Update a_prev and x
        h_prev = np.copy(h)
        x = np.zeros((n_x, 1))
        x[idx] = 1

        counter += 1
    sequence = &amp;quot;&amp;quot;.join([idx_to_chars[idx] for idx in indices if idx != 0])

    return sequence
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a id=&#39;train&#39;&gt;&lt;/a&gt;
&lt;h3 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Fitting the model&lt;/h3&gt;&lt;br&gt;
After covering all the concepts/intuitions behind character-level language model, now we&#39;re ready to fit the model. We&#39;ll use the default settings for RMSProp&#39;s hyperparameters and run the model for 100 iterations. On each iteration, we&#39;ll print out one sampled name and smoothed loss to see how the names generated start to get more interesting with more iterations as well as the loss will start decreasing. When done with fitting the model, we&#39;ll plot the loss function and generate some names.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def model(
        file_path, chars_to_idx, idx_to_chars, hidden_layer_size, vocab_size,
        num_epochs=10, learning_rate=0.01):
    &amp;quot;&amp;quot;&amp;quot;Implements RNN to generate characters.&amp;quot;&amp;quot;&amp;quot;
    # Get the data
    with open(file_path) as f:
        data = f.readlines()
    examples = [x.lower().strip() for x in data]

    # Initialize parameters
    parameters = initialize_parameters(vocab_size, hidden_layer_size)

    # Initialize Adam parameters
    s = initialize_rmsprop(parameters)

    # Initialize loss
    smoothed_loss = -np.log(1 / vocab_size) * 7

    # Initialize hidden state h0 and overall loss
    h_prev = np.zeros((hidden_layer_size, 1))
    overall_loss = []

    # Iterate over number of epochs
    for epoch in range(num_epochs):
        print(f&amp;quot;\033[1m\033[94mEpoch {epoch}&amp;quot;)
        print(f&amp;quot;\033[1m\033[92m=======&amp;quot;)

        # Sample one name
        print(f&amp;quot;&amp;quot;&amp;quot;Sampled name: {sample(parameters, idx_to_chars, chars_to_idx,
            10).capitalize()}&amp;quot;&amp;quot;&amp;quot;)
        print(f&amp;quot;Smoothed loss: {smoothed_loss:.4f}\n&amp;quot;)

        # Shuffle examples
        np.random.shuffle(examples)

        # Iterate over all examples (SGD)
        for example in examples:
            x = [None] + [chars_to_idx[char] for char in example]
            y = x[1:] + [chars_to_idx[&amp;quot;\n&amp;quot;]]
            # Fwd pass
            loss, cache = rnn_forward(x, y, h_prev, parameters)
            # Compute smooth loss
            smoothed_loss = smooth_loss(smoothed_loss, loss)
            # Bwd pass
            grads, h_prev = rnn_backward(y, parameters, cache)
            # Update parameters
            parameters, s = update_parameters_with_rmsprop(
                parameters, grads, s)

        overall_loss.append(smoothed_loss)

    return parameters, overall_loss
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load names
data = open(&amp;quot;../data/names.txt&amp;quot;, &amp;quot;r&amp;quot;).read()

# Convert characters to lower case
data = data.lower()

# Construct vocabulary using unique characters, sort it in ascending order,
# then construct two dictionaries that maps character to index and index to
# characters.
chars = list(sorted(set(data)))
chars_to_idx = {ch:i for i, ch in enumerate(chars)}
idx_to_chars = {i:ch for ch, i in chars_to_idx.items()}

# Get the size of the data and vocab size
data_size = len(data)
vocab_size = len(chars_to_idx)
print(f&amp;quot;There are {data_size} characters and {vocab_size} unique characters.&amp;quot;)

# Fitting the model
parameters, loss = model(&amp;quot;../data/names.txt&amp;quot;, chars_to_idx, idx_to_chars, 100, vocab_size, 100, 0.01)

# Plotting the loss
plt.plot(range(len(loss)), loss)
plt.xlabel(&amp;quot;Epochs&amp;quot;)
plt.ylabel(&amp;quot;Smoothed loss&amp;quot;);

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;There are 36121 characters and 27 unique characters.

Epoch 0
=======
Sampled name: Nijqikkgzst
Smoothed loss: 23.0709

Epoch 10
=======
Sampled name: Milton
Smoothed loss: 14.7446

Epoch 30
=======
Sampled name: Dangelyn
Smoothed loss: 13.8179

Epoch 70
=======
Sampled name: Lacira
Smoothed loss: 13.3782

Epoch 99
=======
Sampled name: Cathranda
Smoothed loss: 13.3380
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/character-level-language-model/loss_plot.png&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/character-level-language-model/loss_plot.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Smoothed loss.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;The names that were generated started to get more interesting after 15 epochs. I didn&#39;t include the results of all epochs to shorten the post; however, you can check the results in the &lt;a href=&#34;https://nbviewer.jupyter.org/github/ImadDabbura/blog-posts/blob/master/notebooks/Character-LeveL-Language-Model.ipynb&#34;&gt;notebook&lt;/a&gt; associated with this post. One of the interesting names is &amp;quot;Yasira&amp;quot; which is an Arabic name :).&lt;/p&gt;

&lt;p&gt;&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Conclusion&lt;/h2&gt;&lt;br&gt;
Statistical language models are very crucial in Natural Language Processing (NLP) such as speech recognition and machine translation. We demonstrated in this post the main concepts behind statistical language models using the character-level language model. The task of this model is to generate names character by character using names obtained from census data that were consisted of 5,163 names. Below are the main key takeaways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If we have more data, a bigger model, and train longer, we may get more interesting results. However, to get very interesting results, we should instead use &lt;strong&gt;Long Short-Term Memory (LSTM)&lt;/strong&gt; model with more than one layer deep. People have used 3 layers deep LSTM model with dropout and were able to generate very interesting results when applied on cookbooks and Shakespeare poems. LSTM models outperform simple RNN due to its ability in capturing longer time dependencies.&lt;/li&gt;
&lt;li&gt;With the sampling technique we&#39;re using, don&#39;t expect the RNN to generate meaningful sequence of characters (names).&lt;/li&gt;
&lt;li&gt;We used in this post each name as its own sequence; however, we may be able to speed up learning and get better results if we increase the batch size; let&#39;s say from one name to a sequence of 50 characters.&lt;/li&gt;
&lt;li&gt;We can control the level of randomness using the sampling strategy. Here, we balanced between what the model thinks it&#39;s the right character and the level of randomness.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Gradient Descent Algorithm and Its Variants</title>
      <link>https://imaddabbura.github.io/post/gradient-descent-algorithm/</link>
      <pubDate>Thu, 21 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://imaddabbura.github.io/post/gradient-descent-algorithm/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Optimization&lt;/strong&gt; refers to the task of minimizing/maximizing an objective function &lt;em&gt;f(x)&lt;/em&gt; parameterized by &lt;em&gt;x&lt;/em&gt;. In machine/deep learning terminology, it&#39;s the task of minimizing the cost/loss function &lt;em&gt;J(w)&lt;/em&gt; parameterized by the model&#39;s parameters $w \in \mathbb{R}^d$. Optimization algorithms (in case of minimization) have one of the following goals:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum.&lt;/li&gt;
&lt;li&gt;Find the lowest possible value of the objective function within its neighborhood. That&#39;s usually the case if the objective function is not convex as the case in most deep learning problems.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are three kinds of optimization algorithms:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Optimization algorithm that is not iterative and simply solves for one point.&lt;/li&gt;
&lt;li&gt;Optimization algorithm that is iterative in nature and converges to acceptable solution regardless of the parameters initialization such as gradient descent applied to logistic regression.&lt;/li&gt;
&lt;li&gt;Optimization algorithm that is iterative in nature and applied to a set of problems that have non-convex cost functions such as neural networks. Therefore, parameters&#39; initialization plays a critical role in speeding up convergence and achieving lower error rates.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Gradient Descent&lt;/strong&gt; is the most common optimization algorithm in &lt;em&gt;machine learning&lt;/em&gt; and &lt;em&gt;deep learning&lt;/em&gt;. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function &lt;em&gt;J(w)&lt;/em&gt; w.r.t the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate Î±. Therefore, we follow the direction of the slope downhill until we reach a local minimum.&lt;/p&gt;

&lt;p&gt;In this post, we&#39;ll cover gradient descent algorithm and its variants: &lt;em&gt;Batch Gradient Descent, Mini-batch Gradient Descent, and Stochastic Gradient Descent&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Let&#39;s first see how gradient descent and its associated steps works on logistic regression before going into the details of its variants. For the sake of simplicity, let&#39;s assume that the logistic regression model has only two parameters: weight &lt;em&gt;w&lt;/em&gt; and bias &lt;em&gt;b&lt;/em&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Initialize weight &lt;em&gt;w&lt;/em&gt; and bias &lt;em&gt;b&lt;/em&gt; to any random numbers.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pick a value for the learning rate Î±. The learning rate determines how big the step would be on each iteration.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If Î± is very small, it would take long time to converge and become computationally expensive.&lt;/li&gt;
&lt;li&gt;IF Î± is large, it may fail to converge and overshoot the minimum.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, plot the cost function against different values of Î± and pick the value of Î± that is right before the first value that didn&#39;t converge so that we would have a very fast learning algorithm that converges (see figure 1).
   


  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/learning_rate.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/learning_rate.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Gradient descent with different learning rates.&lt;/h4&gt;
  &lt;p&gt;
    
    &lt;a href=&#34;http://cs231n.github.io/neural-networks-3/&#34;&gt; 
    Source
    &lt;/a&gt; 
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;
&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The most commonly used rates are : &lt;em&gt;0.001, 0.003, 0.01, 0.03, 0.1, 0.3&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Make sure to scale the data if it&#39;s on very different scales. If we don&#39;t scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge (see figure 2).



  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/normalized-vs-unnormalized.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/normalized-vs-unnormalized.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Gradient descent: normalized versus unnormalized level curves.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Scale the data to have Î¼ = 0 and Ï = 1. Below is the formula for scaling each example:
&lt;span  class=&#34;math&#34;&gt;\(\frac {x_i - \mu}{\sigma}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;On each iteration, take the partial derivative of the cost function &lt;em&gt;J(w)&lt;/em&gt; w.r.t each parameter (gradient):&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\partial}{\partial w}J(w, b) = \nabla_wJ\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\partial}{\partial b}J(w, b) = \nabla_bJ\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The update equations are:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[w = w - \alpha \nabla_w J\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[b = b - \alpha \nabla_b J\]&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For the sake of illustration, letâs assume we donât have bias. If the slope of the current value of &lt;em&gt;w &amp;gt; 0&lt;/em&gt;, this means that we are to the right of optimal &lt;em&gt;w&lt;/em&gt;*. Therefore, the update will be negative, and will start getting close to the optimal values of &lt;em&gt;w&lt;/em&gt;*. However, if itâs negative, the update will be positive and will increase the current values of &lt;em&gt;w&lt;/em&gt; to converge to the optimal values of &lt;em&gt;w&lt;/em&gt;*(see figure 3):



  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/gradients.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/gradients.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Gradient descent. An illustration of how gradient descent algorithm uses the first derivative of the loss function to follow downhill it&amp;rsquo;s minimum.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;ul&gt;
&lt;li&gt;Continue the process until the cost function converges. That is, until the error curve becomes flat and doesn&#39;t change.&lt;/li&gt;
&lt;li&gt;In addition, on each iteration, the step would be in the direction that gives the &lt;em&gt;maximum&lt;/em&gt; change since it&#39;s perpendicular to level curves at each step.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now let&#39;s discuss the three variants of gradient descent algorithm. The main difference between them is the amount of data we use when computing the gradients for each learning step. The trade-off between them is the accuracy of the gradient versus the time complexity to perform each parameter&#39;s update (learning step).&lt;/p&gt;

&lt;h3 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Batch Gradient Descent
&lt;/h3&gt;

&lt;p&gt;Batch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples:
&lt;span  class=&#34;math&#34;&gt;\(w = w - \alpha \nabla_w J\)&lt;/span&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(num_epochs):
  grad = compute_gradient(data, params)
  params = params - learning_rate * grad
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The main advantages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We can use fixed learning rate during training without worrying about learning rate decay.&lt;/li&gt;
&lt;li&gt;It has straight trajectory towards the minimum and it is guaranteed to converge in theory to the global minimum if the loss function is convex and to a local minimum if the loss function is not convex.&lt;/li&gt;
&lt;li&gt;It has unbiased estimate of gradients. The more the examples, the lower the standard error.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The main disadvantages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Even though we can use vectorized implementation, it may still be slow to go over all examples especially when we have large datasets.&lt;/li&gt;
&lt;li&gt;Each step of learning happens after going over all examples where some examples may be redundant and don&#39;t contribute much to the update.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Mini-Batch Gradient Descent
&lt;/h3&gt;

&lt;p&gt;Instead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on batch size. Therefore, learning happens on each mini-batch of &lt;em&gt;b&lt;/em&gt; examples:
&lt;span  class=&#34;math&#34;&gt;\(w = w - \alpha \nabla_w J(x^{\{i:i + b\}}, y^{\{i: i + b\}}; w, b)\)&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Shuffle the training dataset to avoid pre-existing order of examples.&lt;/li&gt;
&lt;li&gt;Partition the training dataset into &lt;em&gt;b&lt;/em&gt; mini-batches based on the batch size. If the training set size is not divisible by batch size, the remaining will be its own batch.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(num_epochs):
    np.random.shuffle(data)
    for batch in radom_minibatches(data, batch_size=32):
        grad = compute_gradient(batch, params)
        params = params - learning_rate * grad
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The batch size is something we can tune. It is usually chosen as power of 2 such as 32, 64, 128, 256, 512, etc. The reason behind it is because some hardware such as GPUs achieve better runtime with common batch sizes such as power of 2.&lt;/p&gt;

&lt;p&gt;The main advantages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Faster than Batch version because it goes through a lot less examples than Batch (all examples).&lt;/li&gt;
&lt;li&gt;Randomly selecting examples will help avoid redundant examples or examples that are very similar that don&#39;t contribute much to the learning.&lt;/li&gt;
&lt;li&gt;With batch size &amp;lt; size of training set, it adds noise to the learning process that helps improving generalization error.&lt;/li&gt;
&lt;li&gt;Even though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The main disadvantages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It won&#39;t converge. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges.&lt;/li&gt;
&lt;li&gt;Due to the noise, the learning steps have more oscillations (see figure 4) and requires adding learning-decay to decrease the learning rate as we become closer to the minimum.



  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/batch-vs-minibatch.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/batch-vs-minibatch.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Gradient descent: batch versus mini-batch loss function.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

With large training datasets, we don&#39;t usually need more than 2-10 passes over all training examples (epochs). Note: with batch size &lt;em&gt;b = m&lt;/em&gt;, we get the Batch Gradient Descent.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 style=&#34;font-family: Georgia; font-size:1.5em;color:purple; font-style:bold&#34;&gt;
Stochastic Gradient Descent
&lt;/h3&gt;

&lt;p&gt;Instead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example $(x^i, y^i)$. Therefore, learning happens on every example:
&lt;span  class=&#34;math&#34;&gt;\(w = w - \alpha \nabla_w J(x^i, y^i; w, b)\)&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Shuffle the training dataset to avoid pre-existing order of examples.&lt;/li&gt;
&lt;li&gt;Partition the training dataset into &lt;em&gt;m&lt;/em&gt; examples.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(num_epochs):
    np.random.shuffle(data)
    for example in data:
        grad = compute_gradient(example, params)
        params = params - learning_rate * grad
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It shares most of the advantages and the disadvantages with mini-batch version. Below are the ones that are specific to SGD:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It adds even more noise to the learning process than mini-batch that helps improving generalization error. However, this would increase the run time.&lt;/li&gt;
&lt;li&gt;We can&#39;t utilize vectorization over 1 example and becomes very slow. Also, the variance becomes large since we only use 1 example for each learning step.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below is a graph that shows the gradient descent&#39;s variants and their direction towards the minimum:



  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/batch-vs-minibatch-vs-stochastic.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/batch-vs-minibatch-vs-stochastic.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Gradient descent variants&amp;rsquo; trajectory towards minimum&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

As the figure above shows, SGD direction is very noisy compared to mini-batch.&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size:2em;color:purple; font-style:bold&#34;&gt;
Challenges
&lt;/h2&gt;

&lt;p&gt;Below are some challenges regarding gradient descent algorithm in general as well as its variants - mainly batch and mini-batch:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Gradient descent is a first-order optimization algorithm, which means it doesn&#39;t take into account the second derivatives of the cost function. However, the curvature of the function affects the size of each learning step. The gradient measures the steepness of the curve but the second derivative measures the curvature of the curve. Therefore, if:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Second derivative = 0 --&amp;gt; the curvature is linear. Therefore, the step size = the learning rate Î±.&lt;/li&gt;
&lt;li&gt;Second derivative &amp;gt; 0 --&amp;gt; the curvature is going upward. Therefore, the step size &amp;lt; the learning rate Î± and may lead to divergence.&lt;/li&gt;
&lt;li&gt;Second derivative &amp;lt; 0 --&amp;gt; the curvature is going downward. Therefore, the step size &amp;gt; the learning rate Î±.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As a result, the direction that looks promising to the gradient may not be so and may lead to slow the learning process or even diverge.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If Hessian matrix has poor conditioning number, i.e. the direction of the most curvature has much more curvature than the direction of the lowest curvature. This will lead the cost function to be very sensitive in some directions and insensitive in other directions. As a result, it will make it harder on the gradient because the direction that looks promising for the gradient may not lead to big changes in the cost function (see figure 6).



  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/curvature.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/curvature.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Gradient descent fails to exploit the curvature information contained in the Hessian matrix.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;
&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The norm of the gradient $g^Tg$ is supposed to decrease slowly with each learning step because the curve is getting flatter and steepness of the curve will decrease. However, we see that the norm of the gradient is increasing, because of the curvature of the curve. Nonetheless, even though the gradients&#39; norm is increasing, we&#39;re able to achieve a very low error rates  (see figure 7).



  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/gradient_norm.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/gradient_norm.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Gradient norm.&lt;/h4&gt;
  &lt;p&gt;
    
    &lt;a href=&#34;http://www.deeplearningbook.org/contents/numerical.html&#34;&gt; 
    Source
    &lt;/a&gt; 
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;
&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In small dimensions, local minimum is common; however, in large dimensions, saddle points are more common. Saddle point is when the function curves up in some directions and curves down in other directions. In other words, saddle point looks a minimum from one direction and a maximum from other direction (see figure 8). This happens when at least one eigenvalue of the hessian matrix is negative and the rest of eigenvalues are positive.



  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/saddle.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/gradient-descent-algorithms/saddle.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Saddle point.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;
&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;As discussed previously, choosing a proper learning rate is hard. Also, for mini-batch gradient descent, we have to adjust the learning rate during the training process to make sure it converges to the local minimum and not wander around it. Figuring out the decay rate of the learning rate is also hard and changes with different datasets.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;All parameter updates have the same learning rate; however, we may want to perform larger updates to some parameters that have their directional derivatives more inline with the trajectory towards the minimum than other parameters.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Predicting Employee Turnover</title>
      <link>https://imaddabbura.github.io/post/pred-employee-turnover/</link>
      <pubDate>Mon, 11 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://imaddabbura.github.io/post/pred-employee-turnover/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Employee turnover&lt;/strong&gt; refers to the percentage of workers who leave an organization and are replaced by new employees. It is very costly for organizations, where costs include but not limited to: separation, vacancy, recruitment, training and replacement. On average, organizations invest between four weeks and three months training new employees. This investment would be a loss for the company if the new employee decided to leave the first year. Furthermore, organizations such as consulting firms would suffer from deterioration in customer satisfaction due to regular changes in &lt;em&gt;Account Reps&lt;/em&gt; and/or &lt;em&gt;Consultants&lt;/em&gt; that would lead to loss of businesses with clients.&lt;/p&gt;

&lt;p&gt;In this post, weâll work on simulated HR data from &lt;a href=&#34;https://www.kaggle.com/ludobenistant/hr-analytics-1&#34;&gt;kaggle&lt;/a&gt; to build a classifier that helps us predict what kind of employees will be more likely to leave given some attributes. Such classifier would help an organization predict employee turnover and be pro-active in helping to solve such costly matter. Weâll restrict ourselves to use the most common classifiers: Random Forest, Gradient Boosting Trees, K-Nearest Neighbors, Logistic Regression and Support Vector Machine.&lt;/p&gt;

&lt;p&gt;The data has 14,999 examples (samples). Below are the features and the definitions of each one:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;satisfaction_level: Level of satisfaction {0â1}.&lt;/li&gt;
&lt;li&gt;last_evaluationTime: Time since last performance evaluation (in years).&lt;/li&gt;
&lt;li&gt;number_project: Number of projects completed while at work.&lt;/li&gt;
&lt;li&gt;average_montly_hours: Average monthly hours at workplace.&lt;/li&gt;
&lt;li&gt;time_spend_company: Number of years spent in the company.&lt;/li&gt;
&lt;li&gt;Work_accident: Whether the employee had a workplace accident.&lt;/li&gt;
&lt;li&gt;left: Whether the employee left the workplace or not {0, 1}.&lt;/li&gt;
&lt;li&gt;promotion_last_5years: Whether the employee was promoted in the last five years.&lt;/li&gt;
&lt;li&gt;sales: Department the employee works for.&lt;/li&gt;
&lt;li&gt;salary: Relative level of salary {low, medium, high}.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Source code that created this post can be found &lt;a href=&#34;https://nbviewer.jupyter.org/github/ImadDabbura/blog-posts/blob/master/notebooks/Employee-Turnover.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size: 2em; color:purple; font-style:bold&#34;&gt;
Data Preprocessing
&lt;/h2&gt;

&lt;p&gt;Letâs take a look at the data (check if there are missing values and the data type of each features):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load the data
df = pd.read_csv(âdata/HR_comma_sep.csvâ)
# Check both the datatypes and if there is missing values
print(â\033[1mâ + â\033[94mâ + âData types:\nâ + 11 * â-â)
print(â\033[30mâ + â{}\nâ.format(df.dtypes))
print(â\033[1mâ + â\033[94mâ + âSum of null values in each column:\nâ + 35 * â-â)
print(â\033[30mâ + â{}â.format(df.isnull().sum()))
df.head()
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/employee-turnover/data_types.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/employee-turnover/data_types.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Data overview.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Since there are no missing values, we do not have to do any imputation. However, there are some data preprocessing needed:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Change &lt;strong&gt;sales&lt;/strong&gt; feature name to &lt;strong&gt;department&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Convert &lt;strong&gt;salary&lt;/strong&gt; into ordinal categorical feature since there is intrinsic order between: low, medium and high.&lt;/li&gt;
&lt;li&gt;Create dummy features from &lt;strong&gt;department&lt;/strong&gt; feature and drop the first one to avoid linear dependency where some learning algorithms may struggle.&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Rename sales feature into department
df = df.rename(columns={&amp;quot;sales&amp;quot;: &amp;quot;department&amp;quot;})
# Map salary into integers
salary_map = {&amp;quot;low&amp;quot;: 0, &amp;quot;medium&amp;quot;: 1, &amp;quot;high&amp;quot;: 2}
df[&amp;quot;salary&amp;quot;] = df[&amp;quot;salary&amp;quot;].map(salary_map)
# Create dummy variables for department feature
df = pd.get_dummies(df, columns=[&amp;quot;department&amp;quot;], drop_first=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The data is now ready to be used for modeling. The final number of features are now 17.&lt;/p&gt;

&lt;h2 style=&#34;font-family: Georgia; font-size: 2em; color:purple; font-style:bold&#34;&gt;
Modeling
&lt;/h2&gt;

&lt;p&gt;Letâs first take a look at the proportion of each class to see if weâre dealing with balanced or imbalanced data, since each one has its own set of tools to be used when fitting classifiers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get number of positve and negative examples
pos = df[df[&amp;quot;left&amp;quot;] == 1].shape[0]
neg = df[df[&amp;quot;left&amp;quot;] == 0].shape[0]
print(&amp;quot;Positive examples = {}&amp;quot;.format(pos))
print(&amp;quot;Negative examples = {}&amp;quot;.format(neg))
print(&amp;quot;Proportion of positive to negative examples = {:.2f}%&amp;quot;.format((pos / neg) * 100))
sns.countplot(df[&amp;quot;left&amp;quot;])
plt.xticks((0, 1), [&amp;quot;Didn&#39;t leave&amp;quot;, &amp;quot;Left&amp;quot;])
plt.xlabel(&amp;quot;Left&amp;quot;)
plt.ylabel(&amp;quot;Count&amp;quot;)
plt.title(&amp;quot;Class counts&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/employee-turnover/class_counts.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/employee-turnover/class_counts.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Class counts.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;As the graph shows, we have an imbalanced dataset. As a result, when we fit classifiers on such datasets, we should use metrics other than accuracy when comparing models such as &lt;em&gt;f1-score&lt;/em&gt; or &lt;em&gt;AUC&lt;/em&gt; (area under ROC curve). Moreover, class imbalance influences a learning algorithm during training by making the decision rule biased towards the majority class by implicitly learns a model that optimizes the predictions based on the majority class in the dataset. There are three ways to deal with this issue:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Assign a larger penalty to wrong predictions from the minority class.&lt;/li&gt;
&lt;li&gt;Upsampling the minority class or downsampling the majority class.&lt;/li&gt;
&lt;li&gt;Generate synthetic training examples.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Nonetheless, there is no definitive guide or best practices to deal with such situations. Therefore, we have to try them all and see which one works best for the problem on hand. Weâll restrict ourselves to use the first two, i.e, assign larger penalty to wrong predictions from the minority class using &lt;code&gt;class_weight&lt;/code&gt; in classifiers that allows us do that and evaluate upsampling/downsampling on the training data to see which gives higher performance.
First, split the data into training and test sets using 80/20 split; 80% of the data will be used to train the models and 20% to test the performance of the models. Second, Upsample the minority class and downsample the majority class. For this data set, positive class is the minority class and negative class is the majority class.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Convert dataframe into numpy objects and split them into
# train and test sets: 80/20
X = df.loc[:, df.columns != &amp;quot;left&amp;quot;].values
y = df.loc[:, df.columns == &amp;quot;left&amp;quot;].values.flatten()
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=1)
# Upsample minority class
X_train_u, y_train_u = resample(X_train[y_train == 1],
                                y_train[y_train == 1],
                                replace=True,
                                n_samples=X_train[y_train == 0].shape[0],
                                random_state=1)
X_train_u = np.concatenate((X_train[y_train == 0], X_train_u))
y_train_u = np.concatenate((y_train[y_train == 0], y_train_u))
# Downsample majority class
X_train_d, y_train_d = resample(X_train[y_train == 0],
                                y_train[y_train == 0],
                                replace=True,
                                n_samples=X_train[y_train == 1].shape[0],
                                random_state=1)
X_train_d = np.concatenate((X_train[y_train == 1], X_train_d))
y_train_d = np.concatenate((y_train[y_train == 1], y_train_d))
print(&amp;quot;Original shape:&amp;quot;, X_train.shape, y_train.shape)
print(&amp;quot;Upsampled shape:&amp;quot;, X_train_u.shape, y_train_u.shape)
print(&amp;quot;Downsampled shape:&amp;quot;, X_train_d.shape, y_train_d.shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Original shape: (11999, 17) (11999,)&lt;/li&gt;
&lt;li&gt;Upsampled shape: (18284, 17) (18284,)&lt;/li&gt;
&lt;li&gt;Downsampled shape: (5714, 17) (5714,)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I donât think we need to apply dimensionality reduction such as PCA because: 1) We want to know the importance of each feature in determining who will leave versus who will not (inference). 2) Dimension of the data set is decent (17 features). However, itâs good to see how many principal components needed to explain 90%, 95% and 99% of the variation in the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Build PCA using standarized trained data
pca = PCA(n_components=None, svd_solver=&amp;quot;full&amp;quot;)
pca.fit(StandardScaler().fit_transform(X_train))
cum_var_exp = np.cumsum(pca.explained_variance_ratio_)
plt.figure(figsize=(12, 6))
plt.bar(range(1, 18), pca.explained_variance_ratio_, align=&amp;quot;center&amp;quot;,
        color=&#39;red&#39;, label=&amp;quot;Individual explained variance&amp;quot;)
plt.step(range(1, 18), cum_var_exp, where=&amp;quot;mid&amp;quot;, label=&amp;quot;Cumulative explained variance&amp;quot;)
plt.xticks(range(1, 18))
plt.legend(loc=&amp;quot;best&amp;quot;)
plt.xlabel(&amp;quot;Principal component index&amp;quot;, {&amp;quot;fontsize&amp;quot;: 14})
plt.ylabel(&amp;quot;Explained variance ratio&amp;quot;, {&amp;quot;fontsize&amp;quot;: 14})
plt.title(&amp;quot;PCA on training data&amp;quot;, {&amp;quot;fontsize&amp;quot;: 16});
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/employee-turnover/PCA.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/employee-turnover/PCA.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;PCA.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Looks like it needs 14, 15 and 16 principal components to capture 90%, 95% and 99% of the variation in the data respectively. In other words, this means that the data is already in a good space since eigenvalues are very close to each other and gives further evidence that we donât need to compress the data.
The methodology that weâll follow when building the classifiers goes as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Build a pipeline that handles all the steps when fitting the classifier using scikit-learnâs &lt;code&gt;make_pipeline&lt;/code&gt; which will have two steps:
I. Standardizing the data to speed up convergence and make all features on the same scale.
II. The classifier (&lt;code&gt;estimator&lt;/code&gt;) we want to use to fit the model.&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;GridSearchCV&lt;/code&gt; to tune hyperparameters using 10-folds cross validation. We can use &lt;code&gt;RandomizedSearchCV&lt;/code&gt; which is faster and may outperform &lt;code&gt;GridSearchCV&lt;/code&gt; especially if we have more than two hyperparameters and the range for each one is very big; however, &lt;code&gt;GridSearchCV&lt;/code&gt; will work just fine since we have only two hyperparameters and descent range.&lt;/li&gt;
&lt;li&gt;Fit the model using training data.&lt;/li&gt;
&lt;li&gt;Plot both confusion matrix and ROC curve for the best estimator using test data.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Repeat the above steps for &lt;em&gt;Random Forest, Gradient Boosting Trees, K-Nearest Neighbors, Logistic Regression and Support Vector Machine&lt;/em&gt;. Next, pick the classifier that has the highest cross validation f1 score. Note: some of the hyperparameter ranges will be guided by the paper &lt;a href=&#34;https://arxiv.org/pdf/1708.05070.pdf&#34;&gt;Data-driven Advice for Applying Machine Learning to Bioinformatics Problems&lt;/a&gt;.&lt;/p&gt;

&lt;h3 style=&#34;font-family: Georgia; font-size: 1.5em; color:purple; font-style:bold&#34;&gt;
Random Forest
&lt;/h3&gt;

&lt;p&gt;First, we will start by fitting a Random Forest classifier using &lt;em&gt;unsampled, upsampled and downsampled&lt;/em&gt; data. Second, we will evaluate each method using cross validation (CV) f1-score and pick the one with the highest CV f1-score. Finally, we will use that method to fit the rest of the classifiers.
The only hyperparameters weâll tune are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;max_feature&lt;/code&gt;: how many features to consider randomly on each split. This will help avoid having few strong features to be picked on each split and let other features have the chance to contribute. Therefore, predictions will be less correlated and the variance of each tree will decrease.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;min_samples_leaf&lt;/code&gt;: how many examples to have for each split to be a final leaf node.
Random Forest is an ensemble model that has multiple trees (&lt;code&gt;n_estimators&lt;/code&gt;). The final prediction would be a weighting average (regression) or mode (classification) of the predictions from all estimators. Note: a high number of trees doesn&#39;t cause overfitting.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Build random forest classifier
methods_data = {&amp;quot;Original&amp;quot;: (X_train, y_train),
                &amp;quot;Upsampled&amp;quot;: (X_train_u, y_train_u),
                &amp;quot;Downsampled&amp;quot;: (X_train_d, y_train_d)}
for method in methods_data.keys():
    pip_rf = make_pipeline(StandardScaler(),
                           RandomForestClassifier(n_estimators=500,
                                                  class_weight=&amp;quot;balanced&amp;quot;,
                                                  random_state=123))
    
    hyperparam_grid = {
        &amp;quot;randomforestclassifier__n_estimators&amp;quot;: [10, 50, 100, 500],
        &amp;quot;randomforestclassifier__max_features&amp;quot;: [&amp;quot;sqrt&amp;quot;, &amp;quot;log2&amp;quot;, 0.4, 0.5],
        &amp;quot;randomforestclassifier__min_samples_leaf&amp;quot;: [1, 3, 5],
        &amp;quot;randomforestclassifier__criterion&amp;quot;: [&amp;quot;gini&amp;quot;, &amp;quot;entropy&amp;quot;]}
    
    gs_rf = GridSearchCV(pip_rf,
                         hyperparam_grid,
                         scoring=&amp;quot;f1&amp;quot;,
                         cv=10,
                         n_jobs=-1)
    
    gs_rf.fit(methods_data[method][0], methods_data[method][1])
    
    print(&amp;quot;\033[1m&amp;quot; + &amp;quot;\033[0m&amp;quot; + &amp;quot;The best hyperparameters for {} data:&amp;quot;.format(method))
    for hyperparam in gs_rf.best_params_.keys():
        print(hyperparam[hyperparam.find(&amp;quot;__&amp;quot;) + 2:], &amp;quot;: &amp;quot;, gs_rf.best_params_[hyperparam])
        
    print(&amp;quot;\033[1m&amp;quot; + &amp;quot;\033[94m&amp;quot; + &amp;quot;Best 10-folds CV f1-score: {:.2f}%.&amp;quot;.format((gs_rf.best_score_) * 100))
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/employee-turnover/rf_hyperparam.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/employee-turnover/rf_hyperparam.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Random Forest hyperparameter tuning results.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Upsampling yielded the highest CV f1-score with 99.8%. Therefore, weâll be using the upsampled data to fit the rest of the classifiers. The new data now has 18,284 examples: 50% belonging to the positive class, and 50% belonging to the negative class.&lt;/p&gt;

&lt;p&gt;Letâs refit the Random Forest with Upsampled data using best hyperparameters tuned above and plot confusion matrix and ROC curve using test data.&lt;/p&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/employee-turnover/rf_roc.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/employee-turnover/rf_roc.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Random Forest.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;h3 style=&#34;font-family: Georgia; font-size: 1.5em; color:purple; font-style:bold&#34;&gt;
Gradient Boosting Trees
&lt;/h3&gt;

&lt;p&gt;Gradient Boosting trees are the same as Random Forest except for:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It starts with small tree and start learning from grown trees by taking into account the residual of grown trees.&lt;/li&gt;
&lt;li&gt;More trees can lead to overfitting; opposite to Random Forest.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, we can think of each tree as a weak learner. The two other hyperparameters than &lt;code&gt;max_features&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt; that we&#39;re going to tune are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;learning_rate&lt;/code&gt;: rate the tree learns, the slower the better.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max_depth&lt;/code&gt;: number of split each time a tree is growing which limits the number of nodes in each tree.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Letâs fit GB classifier and plot confusion matrix and ROC curve using test data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Build Gradient Boosting classifier
pip_gb = make_pipeline(StandardScaler(),
                       GradientBoostingClassifier(loss=&amp;quot;deviance&amp;quot;,
                                                  random_state=123))
hyperparam_grid = {&amp;quot;gradientboostingclassifier__max_features&amp;quot;: [&amp;quot;log2&amp;quot;, 0.5],
                   &amp;quot;gradientboostingclassifier__n_estimators&amp;quot;: [100, 300, 500],
                   &amp;quot;gradientboostingclassifier__learning_rate&amp;quot;: [0.001, 0.01, 0.1],
                   &amp;quot;gradientboostingclassifier__max_depth&amp;quot;: [1, 2, 3]}
gs_gb = GridSearchCV(pip_gb,
                      param_grid=hyperparam_grid,
                      scoring=&amp;quot;f1&amp;quot;,
                      cv=10,
                      n_jobs=-1)
gs_gb.fit(X_train, y_train)
print(&amp;quot;\033[1m&amp;quot; + &amp;quot;\033[0m&amp;quot; + &amp;quot;The best hyperparameters:&amp;quot;)
print(&amp;quot;-&amp;quot; * 25)
for hyperparam in gs_gb.best_params_.keys():
    print(hyperparam[hyperparam.find(&amp;quot;__&amp;quot;) + 2:], &amp;quot;: &amp;quot;, gs_gb.best_params_[hyperparam])
print(&amp;quot;\033[1m&amp;quot; + &amp;quot;\033[94m&amp;quot; + &amp;quot;Best 10-folds CV f1-score: {:.2f}%.&amp;quot;.format((gs_gb.best_score_) * 100))
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/employee-turnover/gb_hyperparam.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/employee-turnover/gb_hyperparam.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Gradient Boosting Trees hyperparameter tuning results.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;





  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/employee-turnover/gb_roc.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/employee-turnover/gb_roc.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Gradient Boosting Trees.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;h3 style=&#34;font-family: Georgia; font-size: 1.5em; color:purple; font-style:bold&#34;&gt;
K-Nearest Neighbors
&lt;/h3&gt;

&lt;p&gt;KNN is called a lazy learning algorithm because it doesnât learn or fit any parameter. It takes &lt;code&gt;n_neighbors&lt;/code&gt; points from the training data closest to the point we&#39;re interested to predict it&#39;s class and take the mode (majority vote) of the classes for the neighboring point as its class. The two hyperparameters we&#39;re going to tune are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;n_neighbors&lt;/code&gt;: number of neighbors to use in prediction.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weights&lt;/code&gt;: how much weight to assign neighbors based on:&lt;/li&gt;
&lt;li&gt;âuniformâ: all neighboring points have the same weight.&lt;/li&gt;
&lt;li&gt;âdistanceâ: use the inverse of euclidean distance of each neighboring point used in prediction.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Letâs fit KNN classifier and plot confusion matrix and ROC curve.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Build KNN classifier
pip_knn = make_pipeline(StandardScaler(), KNeighborsClassifier())
hyperparam_range = range(1, 20)
gs_knn = GridSearchCV(pip_knn,
                      param_grid={&amp;quot;kneighborsclassifier__n_neighbors&amp;quot;: hyperparam_range,
                                  &amp;quot;kneighborsclassifier__weights&amp;quot;: [&amp;quot;uniform&amp;quot;, &amp;quot;distance&amp;quot;]},
                      scoring=&amp;quot;f1&amp;quot;,
                      cv=10,
                      n_jobs=-1)
gs_knn.fit(X_train, y_train)
print(&amp;quot;\033[1m&amp;quot; + &amp;quot;\033[0m&amp;quot; + &amp;quot;The best hyperparameters:&amp;quot;)
print(&amp;quot;-&amp;quot; * 25)
for hyperparam in gs_knn.best_params_.keys():
    print(hyperparam[hyperparam.find(&amp;quot;__&amp;quot;) + 2:], &amp;quot;: &amp;quot;, gs_knn.best_params_[hyperparam])
print(&amp;quot;\033[1m&amp;quot; + &amp;quot;\033[94m&amp;quot; + &amp;quot;Best 10-folds CV f1-score: {:.2f}%.&amp;quot;.format((gs_knn.best_score_) * 100))
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/employee-turnover/knn_hyperparam.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/employee-turnover/knn_hyperparam.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;K-Nearest Neighbors hyperparameter tuning results.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;





  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/employee-turnover/knn_roc.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/employee-turnover/knn_roc.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;K-Nearest Neighbors.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;h3 style=&#34;font-family: Georgia; font-size: 1.5em; color:purple; font-style:bold&#34;&gt;
Logistic Regression
&lt;/h3&gt;

&lt;p&gt;For logistic regression, weâll tune three hyperparameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;penalty&lt;/code&gt;: type of regularization, L2 or L1 regularization.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;C&lt;/code&gt;: the opposite of regularization of parameter Î»Î». The higher C the less regularization. We&#39;ll use values that cover the full range between unregularized to fully regularized where model is the mode of the examples&#39; label.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit_intercept&lt;/code&gt;: whether to include intercept or not.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We wonât use any non-linearities such as polynomial features.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Build logistic model classifier
pip_logmod = make_pipeline(StandardScaler(),
                           LogisticRegression(class_weight=&amp;quot;balanced&amp;quot;))
hyperparam_range = np.arange(0.5, 20.1, 0.5)
hyperparam_grid = {&amp;quot;logisticregression__penalty&amp;quot;: [&amp;quot;l1&amp;quot;, &amp;quot;l2&amp;quot;],
                   &amp;quot;logisticregression__C&amp;quot;:  hyperparam_range,
                   &amp;quot;logisticregression__fit_intercept&amp;quot;: [True, False]
                  }
gs_logmodel = GridSearchCV(pip_logmod,
                           hyperparam_grid,
                           scoring=&amp;quot;accuracy&amp;quot;,
                           cv=2,
                           n_jobs=-1)
gs_logmodel.fit(X_train, y_train)
print(&amp;quot;\033[1m&amp;quot; + &amp;quot;\033[0m&amp;quot; + &amp;quot;The best hyperparameters:&amp;quot;)
print(&amp;quot;-&amp;quot; * 25)
for hyperparam in gs_logmodel.best_params_.keys():
    print(hyperparam[hyperparam.find(&amp;quot;__&amp;quot;) + 2:], &amp;quot;: &amp;quot;, gs_logmodel.best_params_[hyperparam])
print(&amp;quot;\033[1m&amp;quot; + &amp;quot;\033[94m&amp;quot; + &amp;quot;Best 10-folds CV f1-score: {:.2f}%.&amp;quot;.format((gs_logmodel.best_score_) * 100))
&lt;/code&gt;&lt;/pre&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/employee-turnover/logmodel_hyperparam.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/employee-turnover/logmodel_hyperparam.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Logistic Regression hyperparameter tuning results.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;





  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/employee-turnover/logmodel_roc.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/employee-turnover/logmodel_roc.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Logistic Regression.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;h3 style=&#34;font-family: Georgia; font-size: 1.5em; color:purple; font-style:bold&#34;&gt;
Support Vector Machine
&lt;/h3&gt;

&lt;p&gt;SVM is comutationally very expensive to tune itâs hyperparameters for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;With big datasets, it becomes very slow.&lt;/li&gt;
&lt;li&gt;It has good number of hyperparameters to tune that takes very long time to tune on a CPU.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Therefore, weâll use recommended hyperparametersâ values from the paper we mentioned before that showed to yield the best performane on Penn Machine Learning Benchmark 165 datasets. The hyperparameters that we usually look to tune are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;C, gamma, kernel, degree&lt;/code&gt; and &lt;code&gt;coef0&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Build SVM classifier
clf_svc = make_pipeline(StandardScaler(),
                        SVC(C=0.01,
                            gamma=0.1,
                            kernel=&amp;quot;poly&amp;quot;,
                            degree=5,
                            coef0=10,
                            probability=True))
clf_svc.fit(X_train, y_train)
svc_cv_scores = cross_val_score(clf_svc,
                                X=X_train,
                                y=y_train,
                                scoring=&amp;quot;f1&amp;quot;,
                                cv=10,
                                n_jobs=-1)
# Print CV
print(&amp;quot;\033[1m&amp;quot; + &amp;quot;\033[94m&amp;quot; + &amp;quot;The 10-folds CV f1-score is: {:.2f}%&amp;quot;.format(
       np.mean(svc_cv_scores) * 100))
The 10-folds CV f1-score is: 96.38%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;The 10-folds CV f1-score is: 96.38%&lt;/strong&gt;&lt;/p&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/employee-turnover/svc_roc.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/employee-turnover/svc_roc.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Support Vector Machine.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;h2 style=&#34;font-family: Georgia; font-size: 2em; color:purple; font-style:bold&#34;&gt;
Conclusion
&lt;/h2&gt;

&lt;p&gt;Letâs conclude by printing out the test accuracy rates for all classifiers weâve trained so far and plot ROC curves. Then we will pick the classifier that has the highest area under ROC curve.&lt;/p&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/employee-turnover/overall.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/employee-turnover/overall.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Comparing ROC curves for all classifiers.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Even though Random Forest and Gradient Boosting Trees have almost equal AUC, Random Forest has higher accuracy rate and an f1-score with 99.27% and 99.44% respectively. Therefore, we safely say Random Forest outperforms the rest of the classifiers. Letâs have a look of feature importances from Random Forest classifier.&lt;/p&gt;




  




&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;https://imaddabbura.github.io/img/employee-turnover/feature_importance.PNG&#34; &gt;

&lt;img src=&#34;https://imaddabbura.github.io/img/employee-turnover/feature_importance.PNG&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  &lt;h4&gt;Random Forest feature importance.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;p&gt;Looks like the five most important features are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;satisfaction_level&lt;/li&gt;
&lt;li&gt;time_spend_company&lt;/li&gt;
&lt;li&gt;average_montly_hours&lt;/li&gt;
&lt;li&gt;number_project&lt;/li&gt;
&lt;li&gt;lats_evaluation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The take home message is the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;When dealing with imbalanced classes, accuracy is not a good method for model evaluation. AUC and f1-score are examples of metrics we can use.&lt;/li&gt;
&lt;li&gt;Upsampling/downsampling, data synthetic and using balanced class weights are good strategies to try to improve the accuracy of a classifier for imbalanced classes datasets.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GridSearchCV&lt;/code&gt; helps tune hyperparameters for each learning algorithm. &lt;code&gt;RandomizedSearchCV&lt;/code&gt; is faster and may outperform &lt;code&gt;GridSearchCV&lt;/code&gt; especially when we have more than two hyperparameters to tune.&lt;/li&gt;
&lt;li&gt;Principal Component Analysis (PCA) isnât always recommended especially if the data is in a good feature space and their eigen values are very close to each other.&lt;/li&gt;
&lt;li&gt;As expected, ensemble models outperforms other learning algorithms in most cases.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
